{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3a5f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "from model import Dominant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ad1eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    #co ordinate matrix \n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def load_anomaly_detection_dataset(dataset, datadir='data'):\n",
    "    \n",
    "    data_mat = sio.loadmat(f'{datadir}/{dataset}.mat')\n",
    "    adj = data_mat['Network']\n",
    "    feat = data_mat['Attributes']\n",
    "    truth = data_mat['Label']\n",
    "    truth = truth.flatten()\n",
    "\n",
    "    adj_norm = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    adj_norm = adj_norm.toarray()\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    adj = adj.toarray()\n",
    "    feat = feat.toarray()\n",
    "    return adj_norm, feat, truth, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "487f3b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153e9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nhid)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, adj))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Attribute_Decoder(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, dropout):\n",
    "        super(Attribute_Decoder, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nhid, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nfeat)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, adj))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Structure_Decoder(nn.Module):\n",
    "    def __init__(self, nhid, dropout):\n",
    "        super(Structure_Decoder, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nhid, nhid)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = x @ x.T        #this is matrix multiplication\n",
    "\n",
    "        return x\n",
    "\n",
    "class Dominant(nn.Module):\n",
    "    def __init__(self, feat_size, hidden_size, dropout):\n",
    "        super(Dominant, self).__init__()\n",
    "        \n",
    "        self.shared_encoder = Encoder(feat_size, hidden_size, dropout)\n",
    "        self.attr_decoder = Attribute_Decoder(feat_size, hidden_size, dropout)\n",
    "        self.struct_decoder = Structure_Decoder(hidden_size, dropout)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        # encode\n",
    "        x = self.shared_encoder(x, adj)\n",
    "        # decode feature matrix\n",
    "        x_hat = self.attr_decoder(x, adj)\n",
    "        # decode adjacency matrix\n",
    "        struct_reconstructed = self.struct_decoder(x, adj)\n",
    "        # return reconstructed matrices\n",
    "        return struct_reconstructed, x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f65441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(adj, A_hat, attrs, X_hat, alpha):\n",
    "    # Attribute reconstruction loss\n",
    "    diff_attribute = torch.pow(X_hat - attrs, 2)\n",
    "    attribute_reconstruction_errors = torch.sqrt(torch.sum(diff_attribute, 1))\n",
    "    attribute_cost = torch.mean(attribute_reconstruction_errors)\n",
    "\n",
    "    # structure reconstruction loss\n",
    "    diff_structure = torch.pow(A_hat - adj, 2)\n",
    "    structure_reconstruction_errors = torch.sqrt(torch.sum(diff_structure, 1))\n",
    "    structure_cost = torch.mean(structure_reconstruction_errors)\n",
    "\n",
    "\n",
    "    cost =  alpha * attribute_reconstruction_errors + (1-alpha) * structure_reconstruction_errors\n",
    "\n",
    "    return cost, structure_cost, attribute_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37d2c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dominant(dataset=\"BlogCatalog\", hidden_dim=64, max_epoch=100, lr=5e-3, dropout=0.3, alpha=0.8, device=\"cpu\"):\n",
    "    adj, attrs, label, adj_label = load_anomaly_detection_dataset(dataset)\n",
    "    adj = torch.FloatTensor(adj)\n",
    "    adj_label = torch.FloatTensor(adj_label)\n",
    "    attrs = torch.FloatTensor(attrs)\n",
    "    \n",
    "    model = Dominant(feat_size = attrs.size(1), hidden_size = hidden_dim, dropout = dropout)\n",
    "\n",
    "\n",
    "    if device == 'cuda':\n",
    "        device = torch.device(device)\n",
    "        adj = adj.to(device)\n",
    "        adj_label = adj_label.to(device)\n",
    "        attrs = attrs.to(device)\n",
    "        model = model.cuda()\n",
    "        \n",
    "    \n",
    "    optimizer =  torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # returns the reconstructed matrices\n",
    "        A_hat, X_hat = model(attrs, adj)\n",
    "        loss, struct_loss, feat_loss = loss_func(adj_label, A_hat, attrs, X_hat, alpha)\n",
    "        l = torch.mean(loss)\n",
    "        l.backward()\n",
    "        optimizer.step()        \n",
    "        print(\"Epoch:\", '%04d' % (epoch), \n",
    "              \"train_loss=\", \"{:.5f}\".format(l.item()), \n",
    "              \"train/struct_loss=\", \"{:.5f}\".format(struct_loss.item()),\n",
    "              \"train/feat_loss=\", \"{:.5f}\".format(feat_loss.item()))\n",
    "\n",
    "        if epoch%10 == 0 or epoch == max_epoch - 1:\n",
    "            model.eval()\n",
    "#             A_hat, X_hat = model(attrs, adj)\n",
    "#             loss, struct_loss, feat_loss = loss_func(adj_label, A_hat, attrs, X_hat, alpha)\n",
    "            score = loss.detach().cpu().numpy()\n",
    "            precision, recall, _ = precision_recall_curve(label, score)\n",
    "            print(\"Epoch:\", '%04d' % (epoch), 'Auc', roc_auc_score(label, score), \"precision-recall auc\", auc(recall, precision))\n",
    "    return label, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d829c509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train_loss= 4.25870 train/struct_loss= 16.22077 train/feat_loss= 1.26819\n",
      "Score:  [6.3678603 6.209527  6.358977  ... 3.533097  3.9823842 3.8342009] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0000 Auc 0.7833754908865693 precision-recall auc 0.3567988809204118\n",
      "Epoch: 0001 train_loss= 3.10141 train/struct_loss= 10.67251 train/feat_loss= 1.20864\n",
      "Epoch: 0002 train_loss= 2.62402 train/struct_loss= 8.41616 train/feat_loss= 1.17599\n",
      "Epoch: 0003 train_loss= 2.49402 train/struct_loss= 7.77015 train/feat_loss= 1.17499\n",
      "Epoch: 0004 train_loss= 2.47342 train/struct_loss= 7.66758 train/feat_loss= 1.17488\n",
      "Epoch: 0005 train_loss= 2.47075 train/struct_loss= 7.65462 train/feat_loss= 1.17478\n",
      "Epoch: 0006 train_loss= 2.47573 train/struct_loss= 7.67976 train/feat_loss= 1.17472\n",
      "Epoch: 0007 train_loss= 2.46882 train/struct_loss= 7.64534 train/feat_loss= 1.17469\n",
      "Epoch: 0008 train_loss= 2.47011 train/struct_loss= 7.65186 train/feat_loss= 1.17468\n",
      "Epoch: 0009 train_loss= 2.46979 train/struct_loss= 7.65029 train/feat_loss= 1.17467\n",
      "Epoch: 0010 train_loss= 2.46696 train/struct_loss= 7.63615 train/feat_loss= 1.17466\n",
      "Score:  [5.5009465 5.335097  5.3332577 ... 1.6832461 1.7089663 1.7898827] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0010 Auc 0.814037232016355 precision-recall auc 0.3435781529781781\n",
      "Epoch: 0011 train_loss= 2.47741 train/struct_loss= 7.68847 train/feat_loss= 1.17465\n",
      "Epoch: 0012 train_loss= 2.46738 train/struct_loss= 7.63830 train/feat_loss= 1.17465\n",
      "Epoch: 0013 train_loss= 2.47192 train/struct_loss= 7.66099 train/feat_loss= 1.17465\n",
      "Epoch: 0014 train_loss= 2.47380 train/struct_loss= 7.67039 train/feat_loss= 1.17465\n",
      "Epoch: 0015 train_loss= 2.47202 train/struct_loss= 7.66149 train/feat_loss= 1.17465\n",
      "Epoch: 0016 train_loss= 2.46756 train/struct_loss= 7.63921 train/feat_loss= 1.17465\n",
      "Epoch: 0017 train_loss= 2.47381 train/struct_loss= 7.67048 train/feat_loss= 1.17465\n",
      "Epoch: 0018 train_loss= 2.46795 train/struct_loss= 7.64117 train/feat_loss= 1.17465\n",
      "Epoch: 0019 train_loss= 2.46859 train/struct_loss= 7.64436 train/feat_loss= 1.17464\n",
      "Epoch: 0020 train_loss= 2.46948 train/struct_loss= 7.64882 train/feat_loss= 1.17465\n",
      "Score:  [5.5845065 5.377441  5.3534517 ... 1.683812  1.7094247 1.7906199] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0020 Auc 0.8139694054003689 precision-recall auc 0.34203604922078834\n",
      "Epoch: 0021 train_loss= 2.46856 train/struct_loss= 7.64422 train/feat_loss= 1.17465\n",
      "Epoch: 0022 train_loss= 2.46789 train/struct_loss= 7.64086 train/feat_loss= 1.17465\n",
      "Epoch: 0023 train_loss= 2.46933 train/struct_loss= 7.64807 train/feat_loss= 1.17465\n",
      "Epoch: 0024 train_loss= 2.46749 train/struct_loss= 7.63888 train/feat_loss= 1.17464\n",
      "Epoch: 0025 train_loss= 2.46769 train/struct_loss= 7.63986 train/feat_loss= 1.17464\n",
      "Epoch: 0026 train_loss= 2.46742 train/struct_loss= 7.63852 train/feat_loss= 1.17464\n",
      "Epoch: 0027 train_loss= 2.46749 train/struct_loss= 7.63885 train/feat_loss= 1.17464\n",
      "Epoch: 0028 train_loss= 2.46760 train/struct_loss= 7.63944 train/feat_loss= 1.17464\n",
      "Epoch: 0029 train_loss= 2.46709 train/struct_loss= 7.63690 train/feat_loss= 1.17464\n",
      "Epoch: 0030 train_loss= 2.46763 train/struct_loss= 7.63957 train/feat_loss= 1.17464\n",
      "Score:  [5.736972  5.419307  5.365425  ... 1.6840475 1.7089045 1.7913   ] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0030 Auc 0.814120816331005 precision-recall auc 0.3422753880141333\n",
      "Epoch: 0031 train_loss= 2.46732 train/struct_loss= 7.63801 train/feat_loss= 1.17464\n",
      "Epoch: 0032 train_loss= 2.46707 train/struct_loss= 7.63678 train/feat_loss= 1.17464\n",
      "Epoch: 0033 train_loss= 2.46672 train/struct_loss= 7.63500 train/feat_loss= 1.17464\n",
      "Epoch: 0034 train_loss= 2.46685 train/struct_loss= 7.63569 train/feat_loss= 1.17464\n",
      "Epoch: 0035 train_loss= 2.46671 train/struct_loss= 7.63496 train/feat_loss= 1.17464\n",
      "Epoch: 0036 train_loss= 2.46675 train/struct_loss= 7.63516 train/feat_loss= 1.17464\n",
      "Epoch: 0037 train_loss= 2.46659 train/struct_loss= 7.63438 train/feat_loss= 1.17464\n",
      "Epoch: 0038 train_loss= 2.46652 train/struct_loss= 7.63405 train/feat_loss= 1.17464\n",
      "Epoch: 0039 train_loss= 2.46664 train/struct_loss= 7.63464 train/feat_loss= 1.17464\n",
      "Epoch: 0040 train_loss= 2.46702 train/struct_loss= 7.63652 train/feat_loss= 1.17464\n",
      "Score:  [5.578943  5.336928  5.3355494 ... 1.6838949 1.7087616 1.7899389] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0040 Auc 0.8139824226296994 precision-recall auc 0.34222069296186214\n",
      "Epoch: 0041 train_loss= 2.46639 train/struct_loss= 7.63339 train/feat_loss= 1.17464\n",
      "Epoch: 0042 train_loss= 2.46730 train/struct_loss= 7.63792 train/feat_loss= 1.17464\n",
      "Epoch: 0043 train_loss= 2.46665 train/struct_loss= 7.63465 train/feat_loss= 1.17464\n",
      "Epoch: 0044 train_loss= 2.46682 train/struct_loss= 7.63553 train/feat_loss= 1.17464\n",
      "Epoch: 0045 train_loss= 2.46625 train/struct_loss= 7.63270 train/feat_loss= 1.17464\n",
      "Epoch: 0046 train_loss= 2.46629 train/struct_loss= 7.63289 train/feat_loss= 1.17464\n",
      "Epoch: 0047 train_loss= 2.46670 train/struct_loss= 7.63493 train/feat_loss= 1.17464\n",
      "Epoch: 0048 train_loss= 2.46626 train/struct_loss= 7.63272 train/feat_loss= 1.17464\n",
      "Epoch: 0049 train_loss= 2.46660 train/struct_loss= 7.63441 train/feat_loss= 1.17464\n",
      "Epoch: 0050 train_loss= 2.46625 train/struct_loss= 7.63266 train/feat_loss= 1.17464\n",
      "Score:  [5.4695415 5.387392  5.3198853 ... 1.6835549 1.7085482 1.7897342] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0050 Auc 0.8141358889123351 precision-recall auc 0.3433334156156842\n",
      "Epoch: 0051 train_loss= 2.46673 train/struct_loss= 7.63506 train/feat_loss= 1.17464\n",
      "Epoch: 0052 train_loss= 2.46640 train/struct_loss= 7.63343 train/feat_loss= 1.17464\n",
      "Epoch: 0053 train_loss= 2.46605 train/struct_loss= 7.63169 train/feat_loss= 1.17464\n",
      "Epoch: 0054 train_loss= 2.46624 train/struct_loss= 7.63262 train/feat_loss= 1.17464\n",
      "Epoch: 0055 train_loss= 2.46614 train/struct_loss= 7.63210 train/feat_loss= 1.17464\n",
      "Epoch: 0056 train_loss= 2.46636 train/struct_loss= 7.63320 train/feat_loss= 1.17464\n",
      "Epoch: 0057 train_loss= 2.46580 train/struct_loss= 7.63043 train/feat_loss= 1.17464\n",
      "Epoch: 0058 train_loss= 2.46666 train/struct_loss= 7.63472 train/feat_loss= 1.17464\n",
      "Epoch: 0059 train_loss= 2.46599 train/struct_loss= 7.63136 train/feat_loss= 1.17464\n",
      "Epoch: 0060 train_loss= 2.46609 train/struct_loss= 7.63187 train/feat_loss= 1.17464\n",
      "Score:  [5.4277034 5.354476  5.327999  ... 1.6836958 1.7089018 1.7899005] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0060 Auc 0.8142016601763218 precision-recall auc 0.34373989176401876\n",
      "Epoch: 0061 train_loss= 2.46601 train/struct_loss= 7.63145 train/feat_loss= 1.17464\n",
      "Epoch: 0062 train_loss= 2.46652 train/struct_loss= 7.63401 train/feat_loss= 1.17464\n",
      "Epoch: 0063 train_loss= 2.46602 train/struct_loss= 7.63152 train/feat_loss= 1.17464\n",
      "Epoch: 0064 train_loss= 2.46614 train/struct_loss= 7.63212 train/feat_loss= 1.17464\n",
      "Epoch: 0065 train_loss= 2.46617 train/struct_loss= 7.63225 train/feat_loss= 1.17464\n",
      "Epoch: 0066 train_loss= 2.46614 train/struct_loss= 7.63211 train/feat_loss= 1.17464\n",
      "Epoch: 0067 train_loss= 2.46609 train/struct_loss= 7.63188 train/feat_loss= 1.17464\n",
      "Epoch: 0068 train_loss= 2.46602 train/struct_loss= 7.63150 train/feat_loss= 1.17464\n",
      "Epoch: 0069 train_loss= 2.46600 train/struct_loss= 7.63144 train/feat_loss= 1.17464\n",
      "Epoch: 0070 train_loss= 2.46592 train/struct_loss= 7.63103 train/feat_loss= 1.17464\n",
      "Score:  [5.4519935 5.3605957 5.3191805 ... 1.68368   1.7076085 1.7902114] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0070 Auc 0.8140769688216803 precision-recall auc 0.3429587243729895\n",
      "Epoch: 0071 train_loss= 2.46586 train/struct_loss= 7.63074 train/feat_loss= 1.17464\n",
      "Epoch: 0072 train_loss= 2.46604 train/struct_loss= 7.63161 train/feat_loss= 1.17464\n",
      "Epoch: 0073 train_loss= 2.46585 train/struct_loss= 7.63065 train/feat_loss= 1.17464\n",
      "Epoch: 0074 train_loss= 2.46596 train/struct_loss= 7.63122 train/feat_loss= 1.17464\n",
      "Epoch: 0075 train_loss= 2.46590 train/struct_loss= 7.63094 train/feat_loss= 1.17464\n",
      "Epoch: 0076 train_loss= 2.46576 train/struct_loss= 7.63021 train/feat_loss= 1.17464\n",
      "Epoch: 0077 train_loss= 2.46591 train/struct_loss= 7.63098 train/feat_loss= 1.17464\n",
      "Epoch: 0078 train_loss= 2.46580 train/struct_loss= 7.63042 train/feat_loss= 1.17464\n",
      "Epoch: 0079 train_loss= 2.46608 train/struct_loss= 7.63183 train/feat_loss= 1.17464\n",
      "Epoch: 0080 train_loss= 2.46579 train/struct_loss= 7.63038 train/feat_loss= 1.17464\n",
      "Score:  [5.4399977 5.3704424 5.3434668 ... 1.6840096 1.7081571 1.789427 ] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0080 Auc 0.8140728581176813 precision-recall auc 0.34339983958737913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0081 train_loss= 2.46578 train/struct_loss= 7.63034 train/feat_loss= 1.17464\n",
      "Epoch: 0082 train_loss= 2.46572 train/struct_loss= 7.63001 train/feat_loss= 1.17464\n",
      "Epoch: 0083 train_loss= 2.46609 train/struct_loss= 7.63189 train/feat_loss= 1.17464\n",
      "Epoch: 0084 train_loss= 2.46559 train/struct_loss= 7.62939 train/feat_loss= 1.17464\n",
      "Epoch: 0085 train_loss= 2.46591 train/struct_loss= 7.63098 train/feat_loss= 1.17464\n",
      "Epoch: 0086 train_loss= 2.46590 train/struct_loss= 7.63095 train/feat_loss= 1.17464\n",
      "Epoch: 0087 train_loss= 2.46624 train/struct_loss= 7.63260 train/feat_loss= 1.17464\n",
      "Epoch: 0088 train_loss= 2.46562 train/struct_loss= 7.62953 train/feat_loss= 1.17464\n",
      "Epoch: 0089 train_loss= 2.46612 train/struct_loss= 7.63202 train/feat_loss= 1.17464\n",
      "Epoch: 0090 train_loss= 2.46570 train/struct_loss= 7.62994 train/feat_loss= 1.17464\n",
      "Score:  [5.5521545 5.417408  5.3302364 ... 1.6833731 1.7083457 1.7896022] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0090 Auc 0.8140975223416762 precision-recall auc 0.34333048068709465\n",
      "Epoch: 0091 train_loss= 2.46599 train/struct_loss= 7.63139 train/feat_loss= 1.17464\n",
      "Epoch: 0092 train_loss= 2.46567 train/struct_loss= 7.62979 train/feat_loss= 1.17464\n",
      "Epoch: 0093 train_loss= 2.46592 train/struct_loss= 7.63104 train/feat_loss= 1.17464\n",
      "Epoch: 0094 train_loss= 2.46548 train/struct_loss= 7.62884 train/feat_loss= 1.17464\n",
      "Epoch: 0095 train_loss= 2.46577 train/struct_loss= 7.63028 train/feat_loss= 1.17464\n",
      "Epoch: 0096 train_loss= 2.46591 train/struct_loss= 7.63099 train/feat_loss= 1.17464\n",
      "Epoch: 0097 train_loss= 2.46549 train/struct_loss= 7.62889 train/feat_loss= 1.17464\n",
      "Epoch: 0098 train_loss= 2.46581 train/struct_loss= 7.63045 train/feat_loss= 1.17464\n",
      "Epoch: 0099 train_loss= 2.46543 train/struct_loss= 7.62859 train/feat_loss= 1.17464\n",
      "Score:  [5.421526  5.344923  5.3165503 ... 1.6832588 1.707464  1.7896421] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0099 Auc 0.8143105938322998 precision-recall auc 0.3435406655909773\n"
     ]
    }
   ],
   "source": [
    "label, score = train_dominant(dataset=\"BlogCatalog\", hidden_dim=64, max_epoch=100, lr=5e-3, dropout=0.3, alpha=0.8, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efe79597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.5083923,\n",
       " 5.3390894,\n",
       " 5.401518,\n",
       " 4.978561,\n",
       " 4.751317,\n",
       " 4.6615467,\n",
       " 5.009154,\n",
       " 4.487972,\n",
       " 4.7825966,\n",
       " 4.875497,\n",
       " 4.834939,\n",
       " 4.929274,\n",
       " 4.510497,\n",
       " 4.3228054,\n",
       " 4.247916,\n",
       " 4.4402304,\n",
       " 4.454445,\n",
       " 4.228711,\n",
       " 5.003453,\n",
       " 4.132231,\n",
       " 4.538274,\n",
       " 4.2743835,\n",
       " 4.371066,\n",
       " 4.0517354,\n",
       " 6.188692,\n",
       " 4.4547753,\n",
       " 4.4146852,\n",
       " 4.682736,\n",
       " 4.4123025,\n",
       " 4.315397,\n",
       " 4.4512877,\n",
       " 4.3731318,\n",
       " 5.294868,\n",
       " 4.1845593,\n",
       " 4.371368,\n",
       " 5.271095,\n",
       " 4.2814727,\n",
       " 3.7128096,\n",
       " 4.012249,\n",
       " 3.53054,\n",
       " 5.045007,\n",
       " 4.11461,\n",
       " 5.2713585,\n",
       " 4.618676,\n",
       " 4.0157094,\n",
       " 3.6318839,\n",
       " 6.2413416,\n",
       " 4.7002754,\n",
       " 4.093712,\n",
       " 4.3240733,\n",
       " 4.067833,\n",
       " 4.42612,\n",
       " 3.7962375,\n",
       " 3.6151035,\n",
       " 3.57635,\n",
       " 4.4881506,\n",
       " 3.9737942,\n",
       " 3.815003,\n",
       " 3.5591214,\n",
       " 4.123475,\n",
       " 3.9164438,\n",
       " 4.545974,\n",
       " 4.2230473,\n",
       " 3.9341009,\n",
       " 4.001337,\n",
       " 4.4376163,\n",
       " 4.0163918,\n",
       " 4.3073363,\n",
       " 4.1484995,\n",
       " 4.236006,\n",
       " 3.3839872,\n",
       " 3.671132,\n",
       " 3.9031463,\n",
       " 4.2549033,\n",
       " 4.3110447,\n",
       " 4.5184484,\n",
       " 4.10927,\n",
       " 3.3064208,\n",
       " 4.0383573,\n",
       " 3.7575161,\n",
       " 3.6116457,\n",
       " 4.315479,\n",
       " 3.975282,\n",
       " 4.025603,\n",
       " 5.2845592,\n",
       " 4.172268,\n",
       " 3.5385966,\n",
       " 3.4340763,\n",
       " 3.4618437,\n",
       " 3.465866,\n",
       " 4.279786,\n",
       " 5.208722,\n",
       " 4.2968163,\n",
       " 3.7082977,\n",
       " 3.7947729,\n",
       " 3.7727816,\n",
       " 4.455398,\n",
       " 3.8686483,\n",
       " 4.0799637,\n",
       " 3.5088983,\n",
       " 3.71575,\n",
       " 3.5295866,\n",
       " 6.8645115,\n",
       " 3.8986506,\n",
       " 3.7230473,\n",
       " 3.407277,\n",
       " 3.6276278,\n",
       " 4.2443247,\n",
       " 4.208603,\n",
       " 3.8717594,\n",
       " 3.6425974,\n",
       " 3.8488705,\n",
       " 3.8340101,\n",
       " 3.7116904,\n",
       " 3.9331827,\n",
       " 3.6045647,\n",
       " 6.8132343,\n",
       " 3.4242349,\n",
       " 3.5429783,\n",
       " 3.3396947,\n",
       " 3.5978446,\n",
       " 3.3890266,\n",
       " 3.4903026,\n",
       " 3.4383836,\n",
       " 4.084529,\n",
       " 3.667219,\n",
       " 3.4288468,\n",
       " 3.7756584,\n",
       " 3.3522847,\n",
       " 3.0038357,\n",
       " 3.6810665,\n",
       " 3.2474275,\n",
       " 3.5718582,\n",
       " 4.591763,\n",
       " 4.3693953,\n",
       " 3.851512,\n",
       " 4.0790386,\n",
       " 3.5939999,\n",
       " 3.7476282,\n",
       " 3.097462,\n",
       " 3.487149,\n",
       " 3.204779,\n",
       " 3.5115066,\n",
       " 3.6311038,\n",
       " 4.08762,\n",
       " 4.3248754,\n",
       " 3.9495993,\n",
       " 3.374999,\n",
       " 3.9121904,\n",
       " 3.6369112,\n",
       " 3.8568268,\n",
       " 3.483146,\n",
       " 3.890853,\n",
       " 3.2046764,\n",
       " 3.733492,\n",
       " 3.511112,\n",
       " 3.659359,\n",
       " 4.352804,\n",
       " 3.2131288,\n",
       " 3.5565171,\n",
       " 3.696321,\n",
       " 4.03468,\n",
       " 3.1590533,\n",
       " 3.6941895,\n",
       " 3.5011191,\n",
       " 3.5556655,\n",
       " 5.1632376,\n",
       " 3.4193618,\n",
       " 3.4491475,\n",
       " 3.7174222,\n",
       " 3.4263206,\n",
       " 3.9897394,\n",
       " 3.5602474,\n",
       " 3.585853,\n",
       " 6.7388277,\n",
       " 3.439204,\n",
       " 3.4531655,\n",
       " 3.4185448,\n",
       " 3.1706145,\n",
       " 3.145086,\n",
       " 3.7340708,\n",
       " 3.535484,\n",
       " 3.0753598,\n",
       " 4.1567984,\n",
       " 4.4676733,\n",
       " 3.2870343,\n",
       " 2.9246824,\n",
       " 5.997162,\n",
       " 3.508339,\n",
       " 3.7623525,\n",
       " 3.7943506,\n",
       " 3.5590746,\n",
       " 3.1498911,\n",
       " 3.6776109,\n",
       " 6.0463643,\n",
       " 3.91809,\n",
       " 3.8622565,\n",
       " 3.1789522,\n",
       " 3.9497705,\n",
       " 3.6137385,\n",
       " 3.6967905,\n",
       " 3.0224433,\n",
       " 3.3639178,\n",
       " 3.4364953,\n",
       " 6.1480417,\n",
       " 3.1247022,\n",
       " 3.3169367,\n",
       " 3.5052834,\n",
       " 3.8273501,\n",
       " 3.2890959,\n",
       " 3.1942172,\n",
       " 3.2337646,\n",
       " 3.5192482,\n",
       " 3.1717923,\n",
       " 3.4774895,\n",
       " 3.5919213,\n",
       " 3.3141143,\n",
       " 3.1989622,\n",
       " 3.772902,\n",
       " 3.393893,\n",
       " 3.026541,\n",
       " 3.190143,\n",
       " 3.4563913,\n",
       " 3.1512065,\n",
       " 3.2277172,\n",
       " 4.3963037,\n",
       " 3.5563228,\n",
       " 3.2082603,\n",
       " 3.4119294,\n",
       " 3.487492,\n",
       " 3.7139473,\n",
       " 5.735906,\n",
       " 2.9844391,\n",
       " 3.9588718,\n",
       " 2.9296117,\n",
       " 3.0033174,\n",
       " 7.7337294,\n",
       " 3.4099956,\n",
       " 3.6380668,\n",
       " 3.1477509,\n",
       " 3.1404343,\n",
       " 3.0661838,\n",
       " 4.098915,\n",
       " 3.2980368,\n",
       " 3.488666,\n",
       " 3.3983672,\n",
       " 3.5091627,\n",
       " 4.833721,\n",
       " 3.7235951,\n",
       " 3.4331927,\n",
       " 2.8811152,\n",
       " 3.3283195,\n",
       " 3.6311364,\n",
       " 6.015979,\n",
       " 3.2544334,\n",
       " 4.6589727,\n",
       " 3.0330133,\n",
       " 3.480241,\n",
       " 3.46907,\n",
       " 3.5409055,\n",
       " 3.4685419,\n",
       " 3.3034046,\n",
       " 3.5819764,\n",
       " 3.5001302,\n",
       " 2.9530292,\n",
       " 3.4442697,\n",
       " 3.2465045,\n",
       " 2.7678664,\n",
       " 3.1098886,\n",
       " 3.0523984,\n",
       " 3.8784976,\n",
       " 3.2382133,\n",
       " 4.3098507,\n",
       " 4.065533,\n",
       " 2.96974,\n",
       " 3.7692714,\n",
       " 3.7106395,\n",
       " 3.2947626,\n",
       " 3.1720576,\n",
       " 3.55939,\n",
       " 3.237188,\n",
       " 3.0296187,\n",
       " 3.229447,\n",
       " 3.1427274,\n",
       " 4.230546,\n",
       " 3.4618382,\n",
       " 4.52516,\n",
       " 3.480901,\n",
       " 2.6948297,\n",
       " 2.7970219,\n",
       " 3.3674254,\n",
       " 3.2167149,\n",
       " 5.6497946,\n",
       " 2.5911608,\n",
       " 3.431281,\n",
       " 3.5902686,\n",
       " 3.22051,\n",
       " 3.464597,\n",
       " 3.4224064,\n",
       " 2.7987313,\n",
       " 3.1481123,\n",
       " 3.356501,\n",
       " 3.2323627,\n",
       " 3.5157843,\n",
       " 3.2326593,\n",
       " 3.1519454,\n",
       " 2.9032958,\n",
       " 3.3344715,\n",
       " 4.064274,\n",
       " 3.6351275,\n",
       " 3.3849578,\n",
       " 3.6469789,\n",
       " 3.2380168,\n",
       " 3.0378604,\n",
       " 3.5904899,\n",
       " 5.031912,\n",
       " 4.273237,\n",
       " 3.0303638,\n",
       " 3.2247794,\n",
       " 7.5521917,\n",
       " 3.4835029,\n",
       " 3.2909486,\n",
       " 2.9102407,\n",
       " 3.2698953,\n",
       " 3.534783,\n",
       " 3.4472775,\n",
       " 3.295005,\n",
       " 3.4033382,\n",
       " 2.9885325,\n",
       " 3.03447,\n",
       " 3.184641,\n",
       " 3.4482446,\n",
       " 2.989338,\n",
       " 3.3341343,\n",
       " 3.3414342,\n",
       " 3.3667164,\n",
       " 2.9839141,\n",
       " 7.8759794,\n",
       " 3.0926514,\n",
       " 4.4948764,\n",
       " 2.7532685,\n",
       " 3.0425107,\n",
       " 3.2110343,\n",
       " 2.8513842,\n",
       " 2.7949977,\n",
       " 3.4489374,\n",
       " 3.1923509,\n",
       " 3.5056324,\n",
       " 3.138341,\n",
       " 3.0558019,\n",
       " 3.10373,\n",
       " 2.9906414,\n",
       " 3.05853,\n",
       " 3.0252542,\n",
       " 3.7275746,\n",
       " 3.1153286,\n",
       " 3.425314,\n",
       " 3.2564757,\n",
       " 3.0569735,\n",
       " 4.209947,\n",
       " 4.723506,\n",
       " 3.28859,\n",
       " 3.8741374,\n",
       " 6.4067316,\n",
       " 4.077447,\n",
       " 2.9429755,\n",
       " 2.8880436,\n",
       " 2.7640657,\n",
       " 3.879213,\n",
       " 3.4138331,\n",
       " 3.5790563,\n",
       " 2.95943,\n",
       " 5.374502,\n",
       " 3.061388,\n",
       " 3.020308,\n",
       " 2.8549523,\n",
       " 3.4034066,\n",
       " 3.2680745,\n",
       " 3.354702,\n",
       " 3.9026523,\n",
       " 2.9197824,\n",
       " 3.7932124,\n",
       " 6.4068117,\n",
       " 6.556536,\n",
       " 3.0925424,\n",
       " 3.0972857,\n",
       " 3.6090107,\n",
       " 6.112913,\n",
       " 2.798002,\n",
       " 3.658721,\n",
       " 3.2487774,\n",
       " 2.9997694,\n",
       " 3.4616222,\n",
       " 3.570417,\n",
       " 3.9925303,\n",
       " 4.4515495,\n",
       " 2.8967855,\n",
       " 2.995316,\n",
       " 4.0639095,\n",
       " 3.3914042,\n",
       " 3.044874,\n",
       " 3.7184224,\n",
       " 3.0938537,\n",
       " 2.9152284,\n",
       " 3.198292,\n",
       " 3.2292542,\n",
       " 2.9102383,\n",
       " 3.3565874,\n",
       " 2.8431554,\n",
       " 3.165618,\n",
       " 3.804728,\n",
       " 3.0820823,\n",
       " 2.706751,\n",
       " 2.8732748,\n",
       " 2.8730567,\n",
       " 3.3895662,\n",
       " 3.7499857,\n",
       " 3.673131,\n",
       " 4.2316856,\n",
       " 2.822785,\n",
       " 3.6073446,\n",
       " 3.1961367,\n",
       " 3.565277,\n",
       " 3.322039,\n",
       " 3.6376987,\n",
       " 2.8775535,\n",
       " 3.348546,\n",
       " 2.78989,\n",
       " 3.1555915,\n",
       " 2.8676379,\n",
       " 5.028302,\n",
       " 2.9104247,\n",
       " 3.1213136,\n",
       " 3.5801497,\n",
       " 2.9650416,\n",
       " 3.3112652,\n",
       " 3.2714775,\n",
       " 3.0765438,\n",
       " 3.0719552,\n",
       " 2.9318233,\n",
       " 3.1742418,\n",
       " 3.357122,\n",
       " 2.8460155,\n",
       " 3.0170045,\n",
       " 4.7506523,\n",
       " 3.3183057,\n",
       " 2.8368828,\n",
       " 2.9015033,\n",
       " 3.3141925,\n",
       " 2.7646608,\n",
       " 3.5098493,\n",
       " 3.128608,\n",
       " 2.7812107,\n",
       " 3.049434,\n",
       " 3.1394231,\n",
       " 3.2775786,\n",
       " 2.8441799,\n",
       " 2.884476,\n",
       " 2.8629045,\n",
       " 3.0172524,\n",
       " 3.8619852,\n",
       " 3.3165274,\n",
       " 3.050753,\n",
       " 3.3206997,\n",
       " 2.8759093,\n",
       " 3.203744,\n",
       " 3.8070438,\n",
       " 3.5223973,\n",
       " 2.6676707,\n",
       " 3.041232,\n",
       " 3.4110382,\n",
       " 3.1563585,\n",
       " 2.7936502,\n",
       " 3.3314447,\n",
       " 3.0033774,\n",
       " 3.0031433,\n",
       " 3.1781049,\n",
       " 3.6298256,\n",
       " 3.6991801,\n",
       " 3.123512,\n",
       " 2.9997995,\n",
       " 3.217791,\n",
       " 2.827368,\n",
       " 2.8763335,\n",
       " 2.4671624,\n",
       " 3.2167773,\n",
       " 3.0789094,\n",
       " 2.5732665,\n",
       " 2.9358118,\n",
       " 2.5646276,\n",
       " 2.9076498,\n",
       " 3.2493954,\n",
       " 2.7144277,\n",
       " 3.037816,\n",
       " 2.7077515,\n",
       " 2.9093046,\n",
       " 3.2595744,\n",
       " 11.379204,\n",
       " 3.4012299,\n",
       " 3.1385121,\n",
       " 2.923606,\n",
       " 4.108941,\n",
       " 2.9969416,\n",
       " 2.995154,\n",
       " 3.0305214,\n",
       " 2.9916105,\n",
       " 2.9674501,\n",
       " 2.6964893,\n",
       " 3.1048458,\n",
       " 2.7793968,\n",
       " 3.2353575,\n",
       " 2.7769718,\n",
       " 3.160417,\n",
       " 3.1001315,\n",
       " 2.8514762,\n",
       " 2.874534,\n",
       " 3.4740522,\n",
       " 3.1150415,\n",
       " 4.3318725,\n",
       " 3.071101,\n",
       " 2.6699233,\n",
       " 3.1790268,\n",
       " 3.0289469,\n",
       " 3.0648232,\n",
       " 2.7558792,\n",
       " 3.2531557,\n",
       " 2.894295,\n",
       " 3.008543,\n",
       " 3.9643674,\n",
       " 3.0876904,\n",
       " 3.0565102,\n",
       " 2.8676023,\n",
       " 4.41626,\n",
       " 3.2783663,\n",
       " 3.0254831,\n",
       " 2.8146577,\n",
       " 3.2454972,\n",
       " 3.284548,\n",
       " 2.9126291,\n",
       " 3.2189934,\n",
       " 3.2837696,\n",
       " 2.7094417,\n",
       " 3.4204965,\n",
       " 2.982854,\n",
       " 2.7057729,\n",
       " 3.6986938,\n",
       " 3.6552544,\n",
       " 2.6660676,\n",
       " 2.805605,\n",
       " 2.8117626,\n",
       " 3.049108,\n",
       " 4.243908,\n",
       " 2.5957167,\n",
       " 2.9615812,\n",
       " 2.9258666,\n",
       " 3.1010222,\n",
       " 6.520259,\n",
       " 2.950263,\n",
       " 2.8807385,\n",
       " 3.1734996,\n",
       " 3.2723277,\n",
       " 2.6998916,\n",
       " 3.3435318,\n",
       " 3.092561,\n",
       " 2.7373247,\n",
       " 2.7109058,\n",
       " 3.1585822,\n",
       " 2.8669448,\n",
       " 3.278594,\n",
       " 3.1672668,\n",
       " 3.2407851,\n",
       " 2.9625542,\n",
       " 3.8741114,\n",
       " 3.3089561,\n",
       " 2.823944,\n",
       " 2.864149,\n",
       " 2.8082755,\n",
       " 3.2017703,\n",
       " 3.0317187,\n",
       " 3.021832,\n",
       " 3.3783267,\n",
       " 2.5792396,\n",
       " 3.136785,\n",
       " 2.7457256,\n",
       " 2.707907,\n",
       " 3.2284987,\n",
       " 2.9141264,\n",
       " 3.265562,\n",
       " 3.2054462,\n",
       " 2.742143,\n",
       " 3.2153893,\n",
       " 2.8866267,\n",
       " 2.885882,\n",
       " 2.9750676,\n",
       " 3.3589716,\n",
       " 2.820306,\n",
       " 3.8356686,\n",
       " 2.7063856,\n",
       " 3.3388333,\n",
       " 2.6929724,\n",
       " 3.5040758,\n",
       " 3.3084364,\n",
       " 2.8547843,\n",
       " 3.4053473,\n",
       " 2.6808496,\n",
       " 4.663391,\n",
       " 3.4322019,\n",
       " 3.1350534,\n",
       " 3.2744067,\n",
       " 3.1235805,\n",
       " 3.3918734,\n",
       " 3.2200208,\n",
       " 2.6722198,\n",
       " 3.324448,\n",
       " 2.9837985,\n",
       " 2.8877447,\n",
       " 6.5112934,\n",
       " 3.0641923,\n",
       " 3.025155,\n",
       " 2.7732644,\n",
       " 3.071217,\n",
       " 2.6658678,\n",
       " 2.823413,\n",
       " 3.3090706,\n",
       " 3.3677573,\n",
       " 3.043376,\n",
       " 3.126044,\n",
       " 2.8400316,\n",
       " 2.6881704,\n",
       " 3.1123736,\n",
       " 3.0935516,\n",
       " 3.2586715,\n",
       " 2.8187408,\n",
       " 2.8823037,\n",
       " 2.742009,\n",
       " 2.7569294,\n",
       " 3.278502,\n",
       " 3.0311508,\n",
       " 3.003631,\n",
       " 3.3379693,\n",
       " 2.9436402,\n",
       " 3.223949,\n",
       " 3.1739354,\n",
       " 3.224997,\n",
       " 2.4970756,\n",
       " 2.578528,\n",
       " 3.5896194,\n",
       " 3.0607202,\n",
       " 3.7142196,\n",
       " 3.0965285,\n",
       " 2.9078684,\n",
       " 3.3715854,\n",
       " 2.8026428,\n",
       " 2.9150636,\n",
       " 2.8539286,\n",
       " 2.5000448,\n",
       " 3.0527875,\n",
       " 3.1009405,\n",
       " 2.7274425,\n",
       " 3.176323,\n",
       " 3.1235113,\n",
       " 2.6227412,\n",
       " 3.6401343,\n",
       " 4.9254847,\n",
       " 2.4532533,\n",
       " 3.7192183,\n",
       " 2.852559,\n",
       " 3.111087,\n",
       " 2.5214775,\n",
       " 3.0202987,\n",
       " 3.1722317,\n",
       " 2.7835615,\n",
       " 2.999752,\n",
       " 2.9797702,\n",
       " 3.3087025,\n",
       " 3.539557,\n",
       " 3.133235,\n",
       " 3.2273927,\n",
       " 2.701881,\n",
       " 2.964356,\n",
       " 3.0441773,\n",
       " 3.4712324,\n",
       " 3.6128929,\n",
       " 2.5906568,\n",
       " 3.0634742,\n",
       " 2.8964343,\n",
       " 2.9260917,\n",
       " 3.4318004,\n",
       " 3.7606287,\n",
       " 3.629097,\n",
       " 2.5638444,\n",
       " 5.558712,\n",
       " 3.0051825,\n",
       " 3.4406962,\n",
       " 2.627651,\n",
       " 2.5876427,\n",
       " 2.5302072,\n",
       " 3.6458015,\n",
       " 3.0508118,\n",
       " 3.1389248,\n",
       " 2.7699082,\n",
       " 2.4409702,\n",
       " 3.4060006,\n",
       " 3.0146198,\n",
       " 2.9475865,\n",
       " 3.1826856,\n",
       " 3.2557824,\n",
       " 2.9776223,\n",
       " 3.051229,\n",
       " 2.6526728,\n",
       " 2.5470848,\n",
       " 2.881845,\n",
       " 2.832263,\n",
       " 3.5042648,\n",
       " 3.1041436,\n",
       " 2.7451916,\n",
       " 3.1270487,\n",
       " 2.800589,\n",
       " 3.8408823,\n",
       " 4.0061197,\n",
       " 3.368999,\n",
       " 2.9353688,\n",
       " 3.1033468,\n",
       " 3.1464775,\n",
       " 2.9154727,\n",
       " 2.5271976,\n",
       " 3.2821422,\n",
       " 3.2928147,\n",
       " 2.7015595,\n",
       " 3.097379,\n",
       " 2.5659184,\n",
       " 2.7247477,\n",
       " 2.8342772,\n",
       " 3.02972,\n",
       " 3.084732,\n",
       " 3.277792,\n",
       " 3.4182196,\n",
       " 2.797758,\n",
       " 2.900656,\n",
       " 2.7847314,\n",
       " 3.217491,\n",
       " 3.3012452,\n",
       " 2.6110475,\n",
       " 3.0953484,\n",
       " 2.9005308,\n",
       " 2.98638,\n",
       " 3.120432,\n",
       " 2.6283007,\n",
       " 3.1540363,\n",
       " 3.2903805,\n",
       " 2.5360897,\n",
       " 2.9224222,\n",
       " 2.5000746,\n",
       " 5.486532,\n",
       " 2.685328,\n",
       " 3.3674903,\n",
       " 2.404522,\n",
       " 3.3037765,\n",
       " 4.862851,\n",
       " 2.676072,\n",
       " 2.7357476,\n",
       " 2.7703836,\n",
       " 3.1753376,\n",
       " 2.9958925,\n",
       " 2.8094316,\n",
       " 3.3271828,\n",
       " 2.7350345,\n",
       " 2.8261807,\n",
       " 2.8539906,\n",
       " 2.7566655,\n",
       " 2.8685935,\n",
       " 3.435371,\n",
       " 2.6168742,\n",
       " 2.7152393,\n",
       " 2.849103,\n",
       " 2.902774,\n",
       " 3.4084103,\n",
       " 3.1396165,\n",
       " 2.8250444,\n",
       " 3.1167192,\n",
       " 2.9274611,\n",
       " 2.614513,\n",
       " 2.6974878,\n",
       " 2.7785635,\n",
       " 3.0688848,\n",
       " 2.9355135,\n",
       " 3.124219,\n",
       " 3.3172874,\n",
       " 5.61625,\n",
       " 4.214986,\n",
       " 2.9890594,\n",
       " 2.5693116,\n",
       " 3.595169,\n",
       " 2.6291966,\n",
       " 2.5562239,\n",
       " 2.8664348,\n",
       " 2.8977497,\n",
       " 2.7643309,\n",
       " 2.9671712,\n",
       " 3.252658,\n",
       " 2.8313198,\n",
       " 2.7251647,\n",
       " 3.4637513,\n",
       " 2.619484,\n",
       " 3.3416486,\n",
       " 3.021906,\n",
       " 3.79932,\n",
       " 3.126214,\n",
       " 2.8151832,\n",
       " 4.1795764,\n",
       " 2.648592,\n",
       " 3.3132694,\n",
       " 5.4710655,\n",
       " 2.8841934,\n",
       " 3.2669454,\n",
       " 5.5252867,\n",
       " 2.98552,\n",
       " 3.1791158,\n",
       " 2.866561,\n",
       " 2.6139586,\n",
       " 2.6320076,\n",
       " 3.5537987,\n",
       " 3.5171232,\n",
       " 2.7557812,\n",
       " 2.8687518,\n",
       " 2.6126654,\n",
       " 2.4400063,\n",
       " 2.8809376,\n",
       " 2.6559958,\n",
       " 3.07301,\n",
       " 2.7058153,\n",
       " 2.8635728,\n",
       " 3.290971,\n",
       " 2.3834965,\n",
       " 2.7604237,\n",
       " 2.7605238,\n",
       " 3.0461805,\n",
       " 3.9752736,\n",
       " 3.4426813,\n",
       " 2.8344855,\n",
       " 2.604429,\n",
       " 3.0270083,\n",
       " 2.7089195,\n",
       " 2.6888509,\n",
       " 2.535673,\n",
       " 2.7260325,\n",
       " 2.6892974,\n",
       " 3.1634912,\n",
       " 2.9055226,\n",
       " 2.5936668,\n",
       " 6.0529737,\n",
       " 2.7786353,\n",
       " 3.654988,\n",
       " 4.2103643,\n",
       " 3.3253496,\n",
       " 2.8765998,\n",
       " 2.7724745,\n",
       " 2.811653,\n",
       " 2.6042435,\n",
       " 2.524324,\n",
       " 2.7466743,\n",
       " 2.3570778,\n",
       " 2.7187393,\n",
       " 2.6846557,\n",
       " 2.9197779,\n",
       " 2.8522089,\n",
       " 5.485634,\n",
       " 2.9322438,\n",
       " 3.053283,\n",
       " 2.497956,\n",
       " 4.247471,\n",
       " 2.9571643,\n",
       " 3.6290455,\n",
       " 2.6032226,\n",
       " 2.8368077,\n",
       " 3.3660524,\n",
       " 2.7247791,\n",
       " 2.6665914,\n",
       " 2.5863752,\n",
       " 2.9233057,\n",
       " 2.7329497,\n",
       " 3.204188,\n",
       " 2.749251,\n",
       " 2.5155096,\n",
       " 2.869206,\n",
       " 2.6339922,\n",
       " 2.7989318,\n",
       " 2.4452884,\n",
       " 3.438868,\n",
       " 2.7164764,\n",
       " 2.8897047,\n",
       " 3.0036092,\n",
       " 3.1483307,\n",
       " 2.569448,\n",
       " 2.6195152,\n",
       " 3.1097236,\n",
       " 3.192727,\n",
       " 2.609943,\n",
       " 2.93445,\n",
       " 2.7823513,\n",
       " 2.7231016,\n",
       " 4.6687117,\n",
       " 2.3815176,\n",
       " 2.6582255,\n",
       " 3.2032557,\n",
       " 4.658328,\n",
       " 2.4981158,\n",
       " 2.6337032,\n",
       " 3.2828684,\n",
       " 2.4818683,\n",
       " 2.6306448,\n",
       " 2.6383958,\n",
       " 2.6949885,\n",
       " 2.7648566,\n",
       " 2.78401,\n",
       " 2.7959435,\n",
       " 3.008154,\n",
       " 3.0053976,\n",
       " 3.4103477,\n",
       " 2.5491,\n",
       " 2.9359665,\n",
       " 3.3160896,\n",
       " 2.762964,\n",
       " 2.914894,\n",
       " 3.0687943,\n",
       " 2.9851537,\n",
       " 2.75872,\n",
       " 2.496388,\n",
       " 3.073998,\n",
       " 2.4940715,\n",
       " 3.3869748,\n",
       " 2.4219677,\n",
       " 3.0711896,\n",
       " 2.3435967,\n",
       " 2.8673306,\n",
       " 2.8743849,\n",
       " 2.722941,\n",
       " 2.9913344,\n",
       " 3.4412818,\n",
       " 2.9262946,\n",
       " 2.6259565,\n",
       " 2.9149911,\n",
       " 2.894344,\n",
       " 2.9730225,\n",
       " 2.9392135,\n",
       " 2.280719,\n",
       " 2.3415625,\n",
       " 2.571972,\n",
       " 2.2970748,\n",
       " 3.8096333,\n",
       " 2.8124924,\n",
       " 3.098466,\n",
       " 2.8050065,\n",
       " 2.5050526,\n",
       " 2.716499,\n",
       " 3.005087,\n",
       " 2.4161723,\n",
       " 2.8968265,\n",
       " 3.7385597,\n",
       " 2.2776814,\n",
       " 2.5347226,\n",
       " 3.0644522,\n",
       " 2.6039975,\n",
       " 2.943779,\n",
       " 4.4643426,\n",
       " 2.9113107,\n",
       " 2.5939589,\n",
       " 2.8021617,\n",
       " 2.7836199,\n",
       " 2.93301,\n",
       " 2.6326888,\n",
       " 2.6816192,\n",
       " 3.4641352,\n",
       " 2.6465042,\n",
       " 2.8571966,\n",
       " 2.9521663,\n",
       " 3.008152,\n",
       " 3.5215764,\n",
       " 2.98534,\n",
       " 2.8334155,\n",
       " 2.9526343,\n",
       " 3.1312625,\n",
       " 4.741256,\n",
       " 3.2191784,\n",
       " 2.712707,\n",
       " 2.525954,\n",
       " 3.1017723,\n",
       " 3.9088144,\n",
       " 2.6503954,\n",
       " 3.3180168,\n",
       " 2.61766,\n",
       " 2.7287226,\n",
       " 3.3018928,\n",
       " 2.652215,\n",
       " 2.6205978,\n",
       " 3.1242077,\n",
       " 2.5169287,\n",
       " 3.4782305,\n",
       " 2.7864187,\n",
       " 2.4934714,\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f9af7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fbb613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train_loss= 4.36503 train/struct_loss= 16.78088 train/feat_loss= 1.26106\n",
      "Score:  [6.2628446 6.2847447 5.7857976 ... 3.938805  3.806305  3.9313893] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0000 Auc 0.7778966075730129\n",
      "Epoch: 0001 train_loss= 3.38588 train/struct_loss= 12.09742 train/feat_loss= 1.20800\n",
      "Epoch: 0002 train_loss= 2.85177 train/struct_loss= 9.55445 train/feat_loss= 1.17610\n",
      "Epoch: 0003 train_loss= 2.59396 train/struct_loss= 8.27062 train/feat_loss= 1.17479\n",
      "Epoch: 0004 train_loss= 2.49939 train/struct_loss= 7.79748 train/feat_loss= 1.17486\n",
      "Epoch: 0005 train_loss= 2.47424 train/struct_loss= 7.67200 train/feat_loss= 1.17480\n",
      "Epoch: 0006 train_loss= 2.47841 train/struct_loss= 7.69300 train/feat_loss= 1.17477\n",
      "Epoch: 0007 train_loss= 2.47970 train/struct_loss= 7.69960 train/feat_loss= 1.17473\n",
      "Epoch: 0008 train_loss= 2.47001 train/struct_loss= 7.65116 train/feat_loss= 1.17473\n",
      "Epoch: 0009 train_loss= 2.47002 train/struct_loss= 7.65128 train/feat_loss= 1.17471\n",
      "Epoch: 0010 train_loss= 2.47112 train/struct_loss= 7.65674 train/feat_loss= 1.17471\n",
      "Score:  [5.824448  5.4656487 5.443757  ... 1.6840088 1.7097261 1.7914035] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0010 Auc 0.813751538088413\n",
      "Epoch: 0011 train_loss= 2.47006 train/struct_loss= 7.65149 train/feat_loss= 1.17470\n",
      "Epoch: 0012 train_loss= 2.47394 train/struct_loss= 7.67089 train/feat_loss= 1.17470\n",
      "Epoch: 0013 train_loss= 2.47005 train/struct_loss= 7.65142 train/feat_loss= 1.17470\n",
      "Epoch: 0014 train_loss= 2.46786 train/struct_loss= 7.64052 train/feat_loss= 1.17470\n",
      "Epoch: 0015 train_loss= 2.46745 train/struct_loss= 7.63851 train/feat_loss= 1.17469\n",
      "Epoch: 0016 train_loss= 2.46775 train/struct_loss= 7.64000 train/feat_loss= 1.17469\n",
      "Epoch: 0017 train_loss= 2.46788 train/struct_loss= 7.64065 train/feat_loss= 1.17469\n",
      "Epoch: 0018 train_loss= 2.46782 train/struct_loss= 7.64036 train/feat_loss= 1.17469\n",
      "Epoch: 0019 train_loss= 2.46878 train/struct_loss= 7.64518 train/feat_loss= 1.17468\n",
      "Epoch: 0020 train_loss= 2.46756 train/struct_loss= 7.63906 train/feat_loss= 1.17468\n",
      "Score:  [5.518549  5.3804917 5.4268956 ... 1.6835856 1.7088964 1.7907125] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0020 Auc 0.8140399724856878\n",
      "Epoch: 0021 train_loss= 2.46817 train/struct_loss= 7.64211 train/feat_loss= 1.17469\n",
      "Epoch: 0022 train_loss= 2.46726 train/struct_loss= 7.63757 train/feat_loss= 1.17468\n",
      "Epoch: 0023 train_loss= 2.46830 train/struct_loss= 7.64275 train/feat_loss= 1.17468\n",
      "Epoch: 0024 train_loss= 2.46749 train/struct_loss= 7.63872 train/feat_loss= 1.17468\n",
      "Epoch: 0025 train_loss= 2.46759 train/struct_loss= 7.63924 train/feat_loss= 1.17468\n",
      "Epoch: 0026 train_loss= 2.46755 train/struct_loss= 7.63901 train/feat_loss= 1.17468\n",
      "Epoch: 0027 train_loss= 2.46760 train/struct_loss= 7.63927 train/feat_loss= 1.17468\n",
      "Epoch: 0028 train_loss= 2.46744 train/struct_loss= 7.63846 train/feat_loss= 1.17468\n",
      "Epoch: 0029 train_loss= 2.46752 train/struct_loss= 7.63888 train/feat_loss= 1.17468\n",
      "Epoch: 0030 train_loss= 2.46746 train/struct_loss= 7.63859 train/feat_loss= 1.17468\n",
      "Score:  [5.6222644 5.339603  5.359831  ... 1.6837628 1.7072413 1.7907273] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0030 Auc 0.8140714878830149\n",
      "Epoch: 0031 train_loss= 2.46749 train/struct_loss= 7.63872 train/feat_loss= 1.17468\n",
      "Epoch: 0032 train_loss= 2.46741 train/struct_loss= 7.63832 train/feat_loss= 1.17468\n",
      "Epoch: 0033 train_loss= 2.46676 train/struct_loss= 7.63507 train/feat_loss= 1.17468\n",
      "Epoch: 0034 train_loss= 2.46867 train/struct_loss= 7.64460 train/feat_loss= 1.17468\n",
      "Epoch: 0035 train_loss= 2.46714 train/struct_loss= 7.63696 train/feat_loss= 1.17468\n",
      "Epoch: 0036 train_loss= 2.46859 train/struct_loss= 7.64420 train/feat_loss= 1.17468\n",
      "Epoch: 0037 train_loss= 2.46671 train/struct_loss= 7.63481 train/feat_loss= 1.17468\n",
      "Epoch: 0038 train_loss= 2.46938 train/struct_loss= 7.64817 train/feat_loss= 1.17468\n",
      "Epoch: 0039 train_loss= 2.46740 train/struct_loss= 7.63826 train/feat_loss= 1.17468\n",
      "Epoch: 0040 train_loss= 2.46946 train/struct_loss= 7.64857 train/feat_loss= 1.17468\n",
      "Score:  [5.6472325 5.404663  5.500385  ... 1.6838057 1.7088761 1.7901734] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0040 Auc 0.8139618691097037\n",
      "Epoch: 0041 train_loss= 2.46676 train/struct_loss= 7.63506 train/feat_loss= 1.17468\n",
      "Epoch: 0042 train_loss= 2.46768 train/struct_loss= 7.63966 train/feat_loss= 1.17468\n",
      "Epoch: 0043 train_loss= 2.46689 train/struct_loss= 7.63570 train/feat_loss= 1.17468\n",
      "Epoch: 0044 train_loss= 2.46671 train/struct_loss= 7.63482 train/feat_loss= 1.17468\n",
      "Epoch: 0045 train_loss= 2.46726 train/struct_loss= 7.63758 train/feat_loss= 1.17468\n",
      "Epoch: 0046 train_loss= 2.46635 train/struct_loss= 7.63301 train/feat_loss= 1.17468\n",
      "Epoch: 0047 train_loss= 2.46755 train/struct_loss= 7.63902 train/feat_loss= 1.17468\n",
      "Epoch: 0048 train_loss= 2.46669 train/struct_loss= 7.63472 train/feat_loss= 1.17468\n",
      "Epoch: 0049 train_loss= 2.46647 train/struct_loss= 7.63363 train/feat_loss= 1.17468\n",
      "Epoch: 0050 train_loss= 2.46633 train/struct_loss= 7.63293 train/feat_loss= 1.17468\n",
      "Score:  [5.5056973 5.332724  5.3692966 ... 1.6835496 1.7080638 1.7904389] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0050 Auc 0.8140070868536946\n",
      "Epoch: 0051 train_loss= 2.46669 train/struct_loss= 7.63474 train/feat_loss= 1.17468\n",
      "Epoch: 0052 train_loss= 2.46665 train/struct_loss= 7.63454 train/feat_loss= 1.17468\n",
      "Epoch: 0053 train_loss= 2.46646 train/struct_loss= 7.63360 train/feat_loss= 1.17468\n",
      "Epoch: 0054 train_loss= 2.46653 train/struct_loss= 7.63393 train/feat_loss= 1.17468\n",
      "Epoch: 0055 train_loss= 2.46620 train/struct_loss= 7.63228 train/feat_loss= 1.17468\n",
      "Epoch: 0056 train_loss= 2.46690 train/struct_loss= 7.63578 train/feat_loss= 1.17468\n",
      "Epoch: 0057 train_loss= 2.46617 train/struct_loss= 7.63213 train/feat_loss= 1.17468\n",
      "Epoch: 0058 train_loss= 2.46670 train/struct_loss= 7.63476 train/feat_loss= 1.17468\n",
      "Epoch: 0059 train_loss= 2.46659 train/struct_loss= 7.63425 train/feat_loss= 1.17468\n",
      "Epoch: 0060 train_loss= 2.46675 train/struct_loss= 7.63500 train/feat_loss= 1.17468\n",
      "Score:  [5.4199533 5.3101254 5.329043  ... 1.6841884 1.7077224 1.7889341] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0060 Auc 0.8140934116376771\n",
      "Epoch: 0061 train_loss= 2.46623 train/struct_loss= 7.63241 train/feat_loss= 1.17468\n",
      "Epoch: 0062 train_loss= 2.46640 train/struct_loss= 7.63327 train/feat_loss= 1.17468\n",
      "Epoch: 0063 train_loss= 2.46649 train/struct_loss= 7.63371 train/feat_loss= 1.17468\n",
      "Epoch: 0064 train_loss= 2.46643 train/struct_loss= 7.63340 train/feat_loss= 1.17468\n",
      "Epoch: 0065 train_loss= 2.46636 train/struct_loss= 7.63306 train/feat_loss= 1.17468\n",
      "Epoch: 0066 train_loss= 2.46615 train/struct_loss= 7.63204 train/feat_loss= 1.17468\n",
      "Epoch: 0067 train_loss= 2.46633 train/struct_loss= 7.63294 train/feat_loss= 1.17468\n",
      "Epoch: 0068 train_loss= 2.46618 train/struct_loss= 7.63218 train/feat_loss= 1.17468\n",
      "Epoch: 0069 train_loss= 2.46640 train/struct_loss= 7.63326 train/feat_loss= 1.17468\n",
      "Epoch: 0070 train_loss= 2.46608 train/struct_loss= 7.63168 train/feat_loss= 1.17468\n",
      "Score:  [5.578356  5.318471  5.2866807 ... 1.6835532 1.7078795 1.7895131] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0070 Auc 0.8141934387683235\n",
      "Epoch: 0071 train_loss= 2.46642 train/struct_loss= 7.63339 train/feat_loss= 1.17468\n",
      "Epoch: 0072 train_loss= 2.46614 train/struct_loss= 7.63196 train/feat_loss= 1.17468\n",
      "Epoch: 0073 train_loss= 2.46641 train/struct_loss= 7.63334 train/feat_loss= 1.17468\n",
      "Epoch: 0074 train_loss= 2.46609 train/struct_loss= 7.63174 train/feat_loss= 1.17468\n",
      "Epoch: 0075 train_loss= 2.46602 train/struct_loss= 7.63136 train/feat_loss= 1.17468\n",
      "Epoch: 0076 train_loss= 2.46598 train/struct_loss= 7.63118 train/feat_loss= 1.17468\n",
      "Epoch: 0077 train_loss= 2.46622 train/struct_loss= 7.63235 train/feat_loss= 1.17468\n",
      "Epoch: 0078 train_loss= 2.46602 train/struct_loss= 7.63139 train/feat_loss= 1.17468\n",
      "Epoch: 0079 train_loss= 2.46606 train/struct_loss= 7.63156 train/feat_loss= 1.17468\n",
      "Epoch: 0080 train_loss= 2.46603 train/struct_loss= 7.63145 train/feat_loss= 1.17468\n",
      "Score:  [5.4895024 5.3471165 5.3230505 ... 1.6833924 1.7083151 1.7893493] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0080 Auc 0.8140930690790105\n",
      "Epoch: 0081 train_loss= 2.46612 train/struct_loss= 7.63188 train/feat_loss= 1.17468\n",
      "Epoch: 0082 train_loss= 2.46576 train/struct_loss= 7.63006 train/feat_loss= 1.17468\n",
      "Epoch: 0083 train_loss= 2.46604 train/struct_loss= 7.63147 train/feat_loss= 1.17468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0084 train_loss= 2.46596 train/struct_loss= 7.63108 train/feat_loss= 1.17468\n",
      "Epoch: 0085 train_loss= 2.46594 train/struct_loss= 7.63099 train/feat_loss= 1.17468\n",
      "Epoch: 0086 train_loss= 2.46609 train/struct_loss= 7.63175 train/feat_loss= 1.17468\n",
      "Epoch: 0087 train_loss= 2.46621 train/struct_loss= 7.63232 train/feat_loss= 1.17468\n",
      "Epoch: 0088 train_loss= 2.46564 train/struct_loss= 7.62945 train/feat_loss= 1.17468\n",
      "Epoch: 0089 train_loss= 2.46662 train/struct_loss= 7.63437 train/feat_loss= 1.17468\n",
      "Epoch: 0090 train_loss= 2.46655 train/struct_loss= 7.63402 train/feat_loss= 1.17468\n",
      "Score:  [5.5730104 5.391494  5.4070306 ... 1.683486  1.7075173 1.7904813] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0090 Auc 0.8140457959830201\n",
      "Epoch: 0091 train_loss= 2.46686 train/struct_loss= 7.63557 train/feat_loss= 1.17468\n",
      "Epoch: 0092 train_loss= 2.46633 train/struct_loss= 7.63295 train/feat_loss= 1.17468\n",
      "Epoch: 0093 train_loss= 2.46608 train/struct_loss= 7.63169 train/feat_loss= 1.17468\n",
      "Epoch: 0094 train_loss= 2.46599 train/struct_loss= 7.63123 train/feat_loss= 1.17468\n",
      "Epoch: 0095 train_loss= 2.46533 train/struct_loss= 7.62793 train/feat_loss= 1.17468\n",
      "Epoch: 0096 train_loss= 2.46637 train/struct_loss= 7.63313 train/feat_loss= 1.17468\n",
      "Epoch: 0097 train_loss= 2.46530 train/struct_loss= 7.62778 train/feat_loss= 1.17468\n",
      "Epoch: 0098 train_loss= 2.46567 train/struct_loss= 7.62965 train/feat_loss= 1.17468\n",
      "Epoch: 0099 train_loss= 2.46583 train/struct_loss= 7.63042 train/feat_loss= 1.17468\n",
      "Score:  [5.572731  5.3120527 5.2907825 ... 1.6835673 1.7080913 1.7888675] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0099 Auc 0.814311278949633\n"
     ]
    }
   ],
   "source": [
    "train_dominant(dataset=\"BlogCatalog\", hidden_dim=64, max_epoch=100, lr=5e-3, dropout=0.3, alpha=0.8, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ce29cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train_loss= 3.80194 train/struct_loss= 13.96719 train/feat_loss= 1.26063\n",
      "Epoch: 0000 Auc 0.8076820836336431\n",
      "Epoch: 0001 train_loss= 3.63766 train/struct_loss= 13.25442 train/feat_loss= 1.23347\n",
      "Epoch: 0002 train_loss= 3.53396 train/struct_loss= 12.81713 train/feat_loss= 1.21317\n",
      "Epoch: 0003 train_loss= 3.41595 train/struct_loss= 12.28494 train/feat_loss= 1.19870\n",
      "Epoch: 0004 train_loss= 3.30913 train/struct_loss= 11.79079 train/feat_loss= 1.18872\n",
      "Epoch: 0005 train_loss= 3.22164 train/struct_loss= 11.37924 train/feat_loss= 1.18224\n",
      "Epoch: 0006 train_loss= 3.14188 train/struct_loss= 10.99608 train/feat_loss= 1.17834\n",
      "Epoch: 0007 train_loss= 3.07212 train/struct_loss= 10.65597 train/feat_loss= 1.17615\n",
      "Epoch: 0008 train_loss= 2.99504 train/struct_loss= 10.27493 train/feat_loss= 1.17507\n",
      "Epoch: 0009 train_loss= 2.93523 train/struct_loss= 9.97784 train/feat_loss= 1.17458\n",
      "Epoch: 0010 train_loss= 2.87493 train/struct_loss= 9.67710 train/feat_loss= 1.17438\n",
      "Epoch: 0010 Auc 0.808315817166848\n",
      "Epoch: 0011 train_loss= 2.81950 train/struct_loss= 9.40025 train/feat_loss= 1.17431\n",
      "Epoch: 0012 train_loss= 2.77723 train/struct_loss= 9.18897 train/feat_loss= 1.17429\n",
      "Epoch: 0013 train_loss= 2.73005 train/struct_loss= 8.95313 train/feat_loss= 1.17428\n",
      "Epoch: 0014 train_loss= 2.69099 train/struct_loss= 8.75789 train/feat_loss= 1.17427\n",
      "Epoch: 0015 train_loss= 2.65765 train/struct_loss= 8.59119 train/feat_loss= 1.17426\n",
      "Epoch: 0016 train_loss= 2.62433 train/struct_loss= 8.42463 train/feat_loss= 1.17426\n",
      "Epoch: 0017 train_loss= 2.60178 train/struct_loss= 8.31192 train/feat_loss= 1.17425\n",
      "Epoch: 0018 train_loss= 2.58050 train/struct_loss= 8.20556 train/feat_loss= 1.17423\n",
      "Epoch: 0019 train_loss= 2.56091 train/struct_loss= 8.10767 train/feat_loss= 1.17422\n",
      "Epoch: 0020 train_loss= 2.54422 train/struct_loss= 8.02425 train/feat_loss= 1.17422\n",
      "Epoch: 0020 Auc 0.8122018026807271\n",
      "Epoch: 0021 train_loss= 2.53058 train/struct_loss= 7.95605 train/feat_loss= 1.17421\n",
      "Epoch: 0022 train_loss= 2.52005 train/struct_loss= 7.90347 train/feat_loss= 1.17420\n",
      "Epoch: 0023 train_loss= 2.51026 train/struct_loss= 7.85449 train/feat_loss= 1.17420\n",
      "Epoch: 0024 train_loss= 2.50265 train/struct_loss= 7.81646 train/feat_loss= 1.17420\n",
      "Epoch: 0025 train_loss= 2.49594 train/struct_loss= 7.78290 train/feat_loss= 1.17419\n",
      "Epoch: 0026 train_loss= 2.49088 train/struct_loss= 7.75765 train/feat_loss= 1.17419\n",
      "Epoch: 0027 train_loss= 2.48585 train/struct_loss= 7.73248 train/feat_loss= 1.17419\n",
      "Epoch: 0028 train_loss= 2.48234 train/struct_loss= 7.71498 train/feat_loss= 1.17418\n",
      "Epoch: 0029 train_loss= 2.47894 train/struct_loss= 7.69799 train/feat_loss= 1.17418\n",
      "Epoch: 0030 train_loss= 2.47648 train/struct_loss= 7.68570 train/feat_loss= 1.17417\n",
      "Epoch: 0030 Auc 0.8138063474750685\n",
      "Epoch: 0031 train_loss= 2.47418 train/struct_loss= 7.67421 train/feat_loss= 1.17417\n",
      "Epoch: 0032 train_loss= 2.47279 train/struct_loss= 7.66729 train/feat_loss= 1.17416\n",
      "Epoch: 0033 train_loss= 2.47103 train/struct_loss= 7.65849 train/feat_loss= 1.17416\n",
      "Epoch: 0034 train_loss= 2.47011 train/struct_loss= 7.65392 train/feat_loss= 1.17416\n",
      "Epoch: 0035 train_loss= 2.46913 train/struct_loss= 7.64903 train/feat_loss= 1.17416\n",
      "Epoch: 0036 train_loss= 2.46817 train/struct_loss= 7.64425 train/feat_loss= 1.17415\n",
      "Epoch: 0037 train_loss= 2.46781 train/struct_loss= 7.64242 train/feat_loss= 1.17415\n",
      "Epoch: 0038 train_loss= 2.46761 train/struct_loss= 7.64146 train/feat_loss= 1.17415\n",
      "Epoch: 0039 train_loss= 2.46714 train/struct_loss= 7.63910 train/feat_loss= 1.17415\n",
      "Epoch: 0040 train_loss= 2.46699 train/struct_loss= 7.63838 train/feat_loss= 1.17414\n",
      "Epoch: 0040 Auc 0.814089300933678\n",
      "Epoch: 0041 train_loss= 2.46655 train/struct_loss= 7.63620 train/feat_loss= 1.17414\n",
      "Epoch: 0042 train_loss= 2.46623 train/struct_loss= 7.63461 train/feat_loss= 1.17414\n",
      "Epoch: 0043 train_loss= 2.46614 train/struct_loss= 7.63414 train/feat_loss= 1.17414\n",
      "Epoch: 0044 train_loss= 2.46603 train/struct_loss= 7.63362 train/feat_loss= 1.17414\n",
      "Epoch: 0045 train_loss= 2.46580 train/struct_loss= 7.63245 train/feat_loss= 1.17413\n",
      "Epoch: 0046 train_loss= 2.46581 train/struct_loss= 7.63255 train/feat_loss= 1.17413\n",
      "Epoch: 0047 train_loss= 2.46581 train/struct_loss= 7.63251 train/feat_loss= 1.17413\n",
      "Epoch: 0048 train_loss= 2.46584 train/struct_loss= 7.63268 train/feat_loss= 1.17413\n",
      "Epoch: 0049 train_loss= 2.46539 train/struct_loss= 7.63046 train/feat_loss= 1.17413\n",
      "Epoch: 0050 train_loss= 2.46563 train/struct_loss= 7.63164 train/feat_loss= 1.17413\n",
      "Epoch: 0050 Auc 0.8141612382536635\n",
      "Epoch: 0051 train_loss= 2.46541 train/struct_loss= 7.63055 train/feat_loss= 1.17413\n",
      "Epoch: 0052 train_loss= 2.46539 train/struct_loss= 7.63047 train/feat_loss= 1.17412\n",
      "Epoch: 0053 train_loss= 2.46546 train/struct_loss= 7.63083 train/feat_loss= 1.17412\n",
      "Epoch: 0054 train_loss= 2.46539 train/struct_loss= 7.63049 train/feat_loss= 1.17412\n",
      "Epoch: 0055 train_loss= 2.46551 train/struct_loss= 7.63109 train/feat_loss= 1.17412\n",
      "Epoch: 0056 train_loss= 2.46526 train/struct_loss= 7.62984 train/feat_loss= 1.17412\n",
      "Epoch: 0057 train_loss= 2.46531 train/struct_loss= 7.63007 train/feat_loss= 1.17412\n",
      "Epoch: 0058 train_loss= 2.46532 train/struct_loss= 7.63012 train/feat_loss= 1.17412\n",
      "Epoch: 0059 train_loss= 2.46538 train/struct_loss= 7.63046 train/feat_loss= 1.17411\n",
      "Epoch: 0060 train_loss= 2.46527 train/struct_loss= 7.62991 train/feat_loss= 1.17411\n",
      "Epoch: 0060 Auc 0.8142297499869828\n",
      "Epoch: 0061 train_loss= 2.46524 train/struct_loss= 7.62973 train/feat_loss= 1.17411\n",
      "Epoch: 0062 train_loss= 2.46522 train/struct_loss= 7.62963 train/feat_loss= 1.17411\n",
      "Epoch: 0063 train_loss= 2.46510 train/struct_loss= 7.62909 train/feat_loss= 1.17411\n",
      "Epoch: 0064 train_loss= 2.46512 train/struct_loss= 7.62922 train/feat_loss= 1.17410\n",
      "Epoch: 0065 train_loss= 2.46500 train/struct_loss= 7.62862 train/feat_loss= 1.17409\n",
      "Epoch: 0066 train_loss= 2.46488 train/struct_loss= 7.62803 train/feat_loss= 1.17409\n",
      "Epoch: 0067 train_loss= 2.46504 train/struct_loss= 7.62888 train/feat_loss= 1.17408\n",
      "Epoch: 0068 train_loss= 2.46486 train/struct_loss= 7.62795 train/feat_loss= 1.17409\n",
      "Epoch: 0069 train_loss= 2.46511 train/struct_loss= 7.62917 train/feat_loss= 1.17409\n",
      "Epoch: 0070 train_loss= 2.46488 train/struct_loss= 7.62804 train/feat_loss= 1.17409\n",
      "Epoch: 0070 Auc 0.8142324904563154\n",
      "Epoch: 0071 train_loss= 2.46481 train/struct_loss= 7.62770 train/feat_loss= 1.17408\n",
      "Epoch: 0072 train_loss= 2.46515 train/struct_loss= 7.62944 train/feat_loss= 1.17408\n",
      "Epoch: 0073 train_loss= 2.46477 train/struct_loss= 7.62757 train/feat_loss= 1.17408\n",
      "Epoch: 0074 train_loss= 2.46500 train/struct_loss= 7.62873 train/feat_loss= 1.17406\n",
      "Epoch: 0075 train_loss= 2.46473 train/struct_loss= 7.62746 train/feat_loss= 1.17405\n",
      "Epoch: 0076 train_loss= 2.46472 train/struct_loss= 7.62748 train/feat_loss= 1.17403\n",
      "Epoch: 0077 train_loss= 2.46472 train/struct_loss= 7.62753 train/feat_loss= 1.17402\n",
      "Epoch: 0078 train_loss= 2.46481 train/struct_loss= 7.62799 train/feat_loss= 1.17401\n",
      "Epoch: 0079 train_loss= 2.46464 train/struct_loss= 7.62715 train/feat_loss= 1.17401\n",
      "Epoch: 0080 train_loss= 2.46462 train/struct_loss= 7.62703 train/feat_loss= 1.17401\n",
      "Epoch: 0080 Auc 0.8142468779203127\n",
      "Epoch: 0081 train_loss= 2.46460 train/struct_loss= 7.62694 train/feat_loss= 1.17402\n",
      "Epoch: 0082 train_loss= 2.46459 train/struct_loss= 7.62685 train/feat_loss= 1.17402\n",
      "Epoch: 0083 train_loss= 2.46459 train/struct_loss= 7.62686 train/feat_loss= 1.17402\n",
      "Epoch: 0084 train_loss= 2.46456 train/struct_loss= 7.62672 train/feat_loss= 1.17402\n",
      "Epoch: 0085 train_loss= 2.46444 train/struct_loss= 7.62614 train/feat_loss= 1.17401\n",
      "Epoch: 0086 train_loss= 2.46456 train/struct_loss= 7.62676 train/feat_loss= 1.17401\n",
      "Epoch: 0087 train_loss= 2.46456 train/struct_loss= 7.62677 train/feat_loss= 1.17400\n",
      "Epoch: 0088 train_loss= 2.46448 train/struct_loss= 7.62638 train/feat_loss= 1.17400\n",
      "Epoch: 0089 train_loss= 2.46446 train/struct_loss= 7.62629 train/feat_loss= 1.17400\n",
      "Epoch: 0090 train_loss= 2.46418 train/struct_loss= 7.62492 train/feat_loss= 1.17400\n",
      "Epoch: 0090 Auc 0.8142276946349831\n",
      "Epoch: 0091 train_loss= 2.46445 train/struct_loss= 7.62623 train/feat_loss= 1.17400\n",
      "Epoch: 0092 train_loss= 2.46432 train/struct_loss= 7.62560 train/feat_loss= 1.17400\n",
      "Epoch: 0093 train_loss= 2.46426 train/struct_loss= 7.62532 train/feat_loss= 1.17400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0094 train_loss= 2.46428 train/struct_loss= 7.62543 train/feat_loss= 1.17400\n",
      "Epoch: 0095 train_loss= 2.46446 train/struct_loss= 7.62631 train/feat_loss= 1.17400\n",
      "Epoch: 0096 train_loss= 2.46435 train/struct_loss= 7.62578 train/feat_loss= 1.17399\n",
      "Epoch: 0097 train_loss= 2.46421 train/struct_loss= 7.62509 train/feat_loss= 1.17399\n",
      "Epoch: 0098 train_loss= 2.46424 train/struct_loss= 7.62525 train/feat_loss= 1.17399\n",
      "Epoch: 0099 train_loss= 2.46420 train/struct_loss= 7.62506 train/feat_loss= 1.17399\n",
      "Epoch: 0100 train_loss= 2.46437 train/struct_loss= 7.62591 train/feat_loss= 1.17399\n",
      "Epoch: 0100 Auc 0.81424310977498\n",
      "Epoch: 0101 train_loss= 2.46432 train/struct_loss= 7.62566 train/feat_loss= 1.17399\n",
      "Epoch: 0102 train_loss= 2.46430 train/struct_loss= 7.62555 train/feat_loss= 1.17399\n",
      "Epoch: 0103 train_loss= 2.46438 train/struct_loss= 7.62595 train/feat_loss= 1.17399\n",
      "Epoch: 0104 train_loss= 2.46415 train/struct_loss= 7.62481 train/feat_loss= 1.17399\n",
      "Epoch: 0105 train_loss= 2.46423 train/struct_loss= 7.62522 train/feat_loss= 1.17398\n",
      "Epoch: 0106 train_loss= 2.46413 train/struct_loss= 7.62470 train/feat_loss= 1.17398\n",
      "Epoch: 0107 train_loss= 2.46418 train/struct_loss= 7.62499 train/feat_loss= 1.17398\n",
      "Epoch: 0108 train_loss= 2.46405 train/struct_loss= 7.62433 train/feat_loss= 1.17398\n",
      "Epoch: 0109 train_loss= 2.46436 train/struct_loss= 7.62590 train/feat_loss= 1.17398\n",
      "Epoch: 0110 train_loss= 2.46421 train/struct_loss= 7.62514 train/feat_loss= 1.17398\n",
      "Epoch: 0110 Auc 0.8142838742563051\n",
      "Epoch: 0111 train_loss= 2.46406 train/struct_loss= 7.62441 train/feat_loss= 1.17398\n",
      "Epoch: 0112 train_loss= 2.46402 train/struct_loss= 7.62421 train/feat_loss= 1.17397\n",
      "Epoch: 0113 train_loss= 2.46405 train/struct_loss= 7.62435 train/feat_loss= 1.17397\n",
      "Epoch: 0114 train_loss= 2.46395 train/struct_loss= 7.62389 train/feat_loss= 1.17397\n",
      "Epoch: 0115 train_loss= 2.46406 train/struct_loss= 7.62441 train/feat_loss= 1.17397\n",
      "Epoch: 0116 train_loss= 2.46422 train/struct_loss= 7.62520 train/feat_loss= 1.17397\n",
      "Epoch: 0117 train_loss= 2.46395 train/struct_loss= 7.62386 train/feat_loss= 1.17397\n",
      "Epoch: 0118 train_loss= 2.46407 train/struct_loss= 7.62450 train/feat_loss= 1.17397\n",
      "Epoch: 0119 train_loss= 2.46397 train/struct_loss= 7.62399 train/feat_loss= 1.17397\n",
      "Epoch: 0120 train_loss= 2.46393 train/struct_loss= 7.62381 train/feat_loss= 1.17396\n",
      "Epoch: 0120 Auc 0.814278050758973\n",
      "Epoch: 0121 train_loss= 2.46393 train/struct_loss= 7.62381 train/feat_loss= 1.17396\n",
      "Epoch: 0122 train_loss= 2.46386 train/struct_loss= 7.62345 train/feat_loss= 1.17397\n",
      "Epoch: 0123 train_loss= 2.46384 train/struct_loss= 7.62336 train/feat_loss= 1.17396\n",
      "Epoch: 0124 train_loss= 2.46393 train/struct_loss= 7.62380 train/feat_loss= 1.17396\n",
      "Epoch: 0125 train_loss= 2.46384 train/struct_loss= 7.62333 train/feat_loss= 1.17396\n",
      "Epoch: 0126 train_loss= 2.46390 train/struct_loss= 7.62365 train/feat_loss= 1.17396\n",
      "Epoch: 0127 train_loss= 2.46388 train/struct_loss= 7.62353 train/feat_loss= 1.17396\n",
      "Epoch: 0128 train_loss= 2.46387 train/struct_loss= 7.62351 train/feat_loss= 1.17396\n",
      "Epoch: 0129 train_loss= 2.46360 train/struct_loss= 7.62216 train/feat_loss= 1.17396\n",
      "Epoch: 0130 train_loss= 2.46390 train/struct_loss= 7.62368 train/feat_loss= 1.17396\n",
      "Epoch: 0130 Auc 0.814313676860299\n",
      "Epoch: 0131 train_loss= 2.46401 train/struct_loss= 7.62420 train/feat_loss= 1.17396\n",
      "Epoch: 0132 train_loss= 2.46406 train/struct_loss= 7.62445 train/feat_loss= 1.17396\n",
      "Epoch: 0133 train_loss= 2.46372 train/struct_loss= 7.62276 train/feat_loss= 1.17396\n",
      "Epoch: 0134 train_loss= 2.46373 train/struct_loss= 7.62283 train/feat_loss= 1.17396\n",
      "Epoch: 0135 train_loss= 2.46380 train/struct_loss= 7.62317 train/feat_loss= 1.17396\n",
      "Epoch: 0136 train_loss= 2.46391 train/struct_loss= 7.62374 train/feat_loss= 1.17396\n",
      "Epoch: 0137 train_loss= 2.46379 train/struct_loss= 7.62312 train/feat_loss= 1.17396\n",
      "Epoch: 0138 train_loss= 2.46387 train/struct_loss= 7.62354 train/feat_loss= 1.17396\n",
      "Epoch: 0139 train_loss= 2.46390 train/struct_loss= 7.62367 train/feat_loss= 1.17396\n",
      "Epoch: 0140 train_loss= 2.46372 train/struct_loss= 7.62276 train/feat_loss= 1.17396\n",
      "Epoch: 0140 Auc 0.8143140194189656\n",
      "Epoch: 0141 train_loss= 2.46363 train/struct_loss= 7.62232 train/feat_loss= 1.17396\n",
      "Epoch: 0142 train_loss= 2.46375 train/struct_loss= 7.62291 train/feat_loss= 1.17395\n",
      "Epoch: 0143 train_loss= 2.46376 train/struct_loss= 7.62295 train/feat_loss= 1.17396\n",
      "Epoch: 0144 train_loss= 2.46367 train/struct_loss= 7.62254 train/feat_loss= 1.17395\n",
      "Epoch: 0145 train_loss= 2.46372 train/struct_loss= 7.62281 train/feat_loss= 1.17395\n",
      "Epoch: 0146 train_loss= 2.46370 train/struct_loss= 7.62270 train/feat_loss= 1.17395\n",
      "Epoch: 0147 train_loss= 2.46375 train/struct_loss= 7.62293 train/feat_loss= 1.17395\n",
      "Epoch: 0148 train_loss= 2.46372 train/struct_loss= 7.62282 train/feat_loss= 1.17395\n",
      "Epoch: 0149 train_loss= 2.46366 train/struct_loss= 7.62251 train/feat_loss= 1.17395\n",
      "Epoch: 0150 train_loss= 2.46371 train/struct_loss= 7.62277 train/feat_loss= 1.17395\n",
      "Epoch: 0150 Auc 0.814304427776301\n",
      "Epoch: 0151 train_loss= 2.46363 train/struct_loss= 7.62235 train/feat_loss= 1.17395\n",
      "Epoch: 0152 train_loss= 2.46369 train/struct_loss= 7.62264 train/feat_loss= 1.17395\n",
      "Epoch: 0153 train_loss= 2.46365 train/struct_loss= 7.62245 train/feat_loss= 1.17395\n",
      "Epoch: 0154 train_loss= 2.46346 train/struct_loss= 7.62151 train/feat_loss= 1.17395\n",
      "Epoch: 0155 train_loss= 2.46362 train/struct_loss= 7.62232 train/feat_loss= 1.17395\n",
      "Epoch: 0156 train_loss= 2.46372 train/struct_loss= 7.62282 train/feat_loss= 1.17395\n",
      "Epoch: 0157 train_loss= 2.46352 train/struct_loss= 7.62180 train/feat_loss= 1.17395\n",
      "Epoch: 0158 train_loss= 2.46353 train/struct_loss= 7.62185 train/feat_loss= 1.17395\n",
      "Epoch: 0159 train_loss= 2.46364 train/struct_loss= 7.62241 train/feat_loss= 1.17395\n",
      "Epoch: 0160 train_loss= 2.46355 train/struct_loss= 7.62196 train/feat_loss= 1.17395\n",
      "Epoch: 0160 Auc 0.8143284068829628\n",
      "Epoch: 0161 train_loss= 2.46347 train/struct_loss= 7.62156 train/feat_loss= 1.17395\n",
      "Epoch: 0162 train_loss= 2.46378 train/struct_loss= 7.62310 train/feat_loss= 1.17395\n",
      "Epoch: 0163 train_loss= 2.46348 train/struct_loss= 7.62163 train/feat_loss= 1.17394\n",
      "Epoch: 0164 train_loss= 2.46358 train/struct_loss= 7.62213 train/feat_loss= 1.17395\n",
      "Epoch: 0165 train_loss= 2.46346 train/struct_loss= 7.62153 train/feat_loss= 1.17394\n",
      "Epoch: 0166 train_loss= 2.46347 train/struct_loss= 7.62158 train/feat_loss= 1.17394\n",
      "Epoch: 0167 train_loss= 2.46351 train/struct_loss= 7.62178 train/feat_loss= 1.17394\n",
      "Epoch: 0168 train_loss= 2.46345 train/struct_loss= 7.62147 train/feat_loss= 1.17394\n",
      "Epoch: 0169 train_loss= 2.46372 train/struct_loss= 7.62283 train/feat_loss= 1.17394\n",
      "Epoch: 0170 train_loss= 2.46347 train/struct_loss= 7.62158 train/feat_loss= 1.17394\n",
      "Epoch: 0170 Auc 0.8143277217656296\n",
      "Epoch: 0171 train_loss= 2.46343 train/struct_loss= 7.62137 train/feat_loss= 1.17394\n",
      "Epoch: 0172 train_loss= 2.46347 train/struct_loss= 7.62158 train/feat_loss= 1.17394\n",
      "Epoch: 0173 train_loss= 2.46332 train/struct_loss= 7.62085 train/feat_loss= 1.17394\n",
      "Epoch: 0174 train_loss= 2.46341 train/struct_loss= 7.62130 train/feat_loss= 1.17394\n",
      "Epoch: 0175 train_loss= 2.46354 train/struct_loss= 7.62196 train/feat_loss= 1.17394\n",
      "Epoch: 0176 train_loss= 2.46350 train/struct_loss= 7.62174 train/feat_loss= 1.17394\n",
      "Epoch: 0177 train_loss= 2.46344 train/struct_loss= 7.62143 train/feat_loss= 1.17394\n",
      "Epoch: 0178 train_loss= 2.46348 train/struct_loss= 7.62167 train/feat_loss= 1.17393\n",
      "Epoch: 0179 train_loss= 2.46350 train/struct_loss= 7.62176 train/feat_loss= 1.17393\n",
      "Epoch: 0180 train_loss= 2.46356 train/struct_loss= 7.62204 train/feat_loss= 1.17394\n",
      "Epoch: 0180 Auc 0.8143897248842836\n",
      "Epoch: 0181 train_loss= 2.46345 train/struct_loss= 7.62150 train/feat_loss= 1.17394\n",
      "Epoch: 0182 train_loss= 2.46347 train/struct_loss= 7.62163 train/feat_loss= 1.17394\n",
      "Epoch: 0183 train_loss= 2.46323 train/struct_loss= 7.62043 train/feat_loss= 1.17393\n",
      "Epoch: 0184 train_loss= 2.46344 train/struct_loss= 7.62150 train/feat_loss= 1.17393\n",
      "Epoch: 0185 train_loss= 2.46342 train/struct_loss= 7.62138 train/feat_loss= 1.17393\n",
      "Epoch: 0186 train_loss= 2.46326 train/struct_loss= 7.62060 train/feat_loss= 1.17393\n",
      "Epoch: 0187 train_loss= 2.46347 train/struct_loss= 7.62164 train/feat_loss= 1.17393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0188 train_loss= 2.46348 train/struct_loss= 7.62169 train/feat_loss= 1.17393\n",
      "Epoch: 0189 train_loss= 2.46333 train/struct_loss= 7.62091 train/feat_loss= 1.17393\n",
      "Epoch: 0190 train_loss= 2.46346 train/struct_loss= 7.62160 train/feat_loss= 1.17393\n",
      "Epoch: 0190 Auc 0.8143475901682923\n",
      "Epoch: 0191 train_loss= 2.46331 train/struct_loss= 7.62085 train/feat_loss= 1.17393\n",
      "Epoch: 0192 train_loss= 2.46333 train/struct_loss= 7.62093 train/feat_loss= 1.17393\n",
      "Epoch: 0193 train_loss= 2.46315 train/struct_loss= 7.62006 train/feat_loss= 1.17393\n",
      "Epoch: 0194 train_loss= 2.46318 train/struct_loss= 7.62020 train/feat_loss= 1.17393\n",
      "Epoch: 0195 train_loss= 2.46322 train/struct_loss= 7.62041 train/feat_loss= 1.17393\n",
      "Epoch: 0196 train_loss= 2.46329 train/struct_loss= 7.62076 train/feat_loss= 1.17393\n",
      "Epoch: 0197 train_loss= 2.46322 train/struct_loss= 7.62041 train/feat_loss= 1.17393\n",
      "Epoch: 0198 train_loss= 2.46355 train/struct_loss= 7.62207 train/feat_loss= 1.17393\n",
      "Epoch: 0199 train_loss= 2.46353 train/struct_loss= 7.62194 train/feat_loss= 1.17392\n",
      "Epoch: 0200 train_loss= 2.46333 train/struct_loss= 7.62092 train/feat_loss= 1.17393\n",
      "Epoch: 0200 Auc 0.8144058251416139\n",
      "Epoch: 0201 train_loss= 2.46332 train/struct_loss= 7.62089 train/feat_loss= 1.17392\n",
      "Epoch: 0202 train_loss= 2.46351 train/struct_loss= 7.62189 train/feat_loss= 1.17392\n",
      "Epoch: 0203 train_loss= 2.46338 train/struct_loss= 7.62122 train/feat_loss= 1.17392\n",
      "Epoch: 0204 train_loss= 2.46321 train/struct_loss= 7.62036 train/feat_loss= 1.17392\n",
      "Epoch: 0205 train_loss= 2.46344 train/struct_loss= 7.62154 train/feat_loss= 1.17392\n",
      "Epoch: 0206 train_loss= 2.46337 train/struct_loss= 7.62114 train/feat_loss= 1.17392\n",
      "Epoch: 0207 train_loss= 2.46307 train/struct_loss= 7.61964 train/feat_loss= 1.17392\n",
      "Epoch: 0208 train_loss= 2.46332 train/struct_loss= 7.62090 train/feat_loss= 1.17392\n",
      "Epoch: 0209 train_loss= 2.46337 train/struct_loss= 7.62118 train/feat_loss= 1.17392\n",
      "Epoch: 0210 train_loss= 2.46324 train/struct_loss= 7.62050 train/feat_loss= 1.17392\n",
      "Epoch: 0210 Auc 0.8144161019016116\n",
      "Epoch: 0211 train_loss= 2.46317 train/struct_loss= 7.62018 train/feat_loss= 1.17392\n",
      "Epoch: 0212 train_loss= 2.46306 train/struct_loss= 7.61963 train/feat_loss= 1.17392\n",
      "Epoch: 0213 train_loss= 2.46328 train/struct_loss= 7.62072 train/feat_loss= 1.17392\n",
      "Epoch: 0214 train_loss= 2.46310 train/struct_loss= 7.61983 train/feat_loss= 1.17392\n",
      "Epoch: 0215 train_loss= 2.46310 train/struct_loss= 7.61981 train/feat_loss= 1.17392\n",
      "Epoch: 0216 train_loss= 2.46311 train/struct_loss= 7.61991 train/feat_loss= 1.17392\n",
      "Epoch: 0217 train_loss= 2.46307 train/struct_loss= 7.61967 train/feat_loss= 1.17392\n",
      "Epoch: 0218 train_loss= 2.46304 train/struct_loss= 7.61956 train/feat_loss= 1.17392\n",
      "Epoch: 0219 train_loss= 2.46321 train/struct_loss= 7.62037 train/feat_loss= 1.17392\n",
      "Epoch: 0220 train_loss= 2.46302 train/struct_loss= 7.61943 train/feat_loss= 1.17391\n",
      "Epoch: 0220 Auc 0.8144051400242805\n",
      "Epoch: 0221 train_loss= 2.46304 train/struct_loss= 7.61955 train/feat_loss= 1.17391\n",
      "Epoch: 0222 train_loss= 2.46309 train/struct_loss= 7.61978 train/feat_loss= 1.17391\n",
      "Epoch: 0223 train_loss= 2.46309 train/struct_loss= 7.61978 train/feat_loss= 1.17391\n",
      "Epoch: 0224 train_loss= 2.46296 train/struct_loss= 7.61917 train/feat_loss= 1.17391\n",
      "Epoch: 0225 train_loss= 2.46301 train/struct_loss= 7.61938 train/feat_loss= 1.17391\n",
      "Epoch: 0226 train_loss= 2.46283 train/struct_loss= 7.61853 train/feat_loss= 1.17391\n",
      "Epoch: 0227 train_loss= 2.46298 train/struct_loss= 7.61928 train/feat_loss= 1.17391\n",
      "Epoch: 0228 train_loss= 2.46295 train/struct_loss= 7.61912 train/feat_loss= 1.17391\n",
      "Epoch: 0229 train_loss= 2.46314 train/struct_loss= 7.62008 train/feat_loss= 1.17391\n",
      "Epoch: 0230 train_loss= 2.46294 train/struct_loss= 7.61906 train/feat_loss= 1.17391\n",
      "Epoch: 0230 Auc 0.8144393958909402\n",
      "Epoch: 0231 train_loss= 2.46276 train/struct_loss= 7.61818 train/feat_loss= 1.17391\n",
      "Epoch: 0232 train_loss= 2.46308 train/struct_loss= 7.61973 train/feat_loss= 1.17391\n",
      "Epoch: 0233 train_loss= 2.46286 train/struct_loss= 7.61868 train/feat_loss= 1.17391\n",
      "Epoch: 0234 train_loss= 2.46286 train/struct_loss= 7.61866 train/feat_loss= 1.17391\n",
      "Epoch: 0235 train_loss= 2.46294 train/struct_loss= 7.61908 train/feat_loss= 1.17391\n",
      "Epoch: 0236 train_loss= 2.46299 train/struct_loss= 7.61933 train/feat_loss= 1.17391\n",
      "Epoch: 0237 train_loss= 2.46293 train/struct_loss= 7.61899 train/feat_loss= 1.17391\n",
      "Epoch: 0238 train_loss= 2.46293 train/struct_loss= 7.61901 train/feat_loss= 1.17391\n",
      "Epoch: 0239 train_loss= 2.46287 train/struct_loss= 7.61871 train/feat_loss= 1.17391\n",
      "Epoch: 0240 train_loss= 2.46280 train/struct_loss= 7.61835 train/feat_loss= 1.17391\n",
      "Epoch: 0240 Auc 0.8144736517575999\n",
      "Epoch: 0241 train_loss= 2.46311 train/struct_loss= 7.61995 train/feat_loss= 1.17391\n",
      "Epoch: 0242 train_loss= 2.46288 train/struct_loss= 7.61878 train/feat_loss= 1.17391\n",
      "Epoch: 0243 train_loss= 2.46272 train/struct_loss= 7.61799 train/feat_loss= 1.17391\n",
      "Epoch: 0244 train_loss= 2.46265 train/struct_loss= 7.61766 train/feat_loss= 1.17390\n",
      "Epoch: 0245 train_loss= 2.46295 train/struct_loss= 7.61915 train/feat_loss= 1.17390\n",
      "Epoch: 0246 train_loss= 2.46281 train/struct_loss= 7.61841 train/feat_loss= 1.17390\n",
      "Epoch: 0247 train_loss= 2.46282 train/struct_loss= 7.61851 train/feat_loss= 1.17390\n",
      "Epoch: 0248 train_loss= 2.46267 train/struct_loss= 7.61775 train/feat_loss= 1.17390\n",
      "Epoch: 0249 train_loss= 2.46278 train/struct_loss= 7.61831 train/feat_loss= 1.17390\n",
      "Epoch: 0250 train_loss= 2.46282 train/struct_loss= 7.61846 train/feat_loss= 1.17390\n",
      "Epoch: 0250 Auc 0.8144681708189345\n",
      "Epoch: 0251 train_loss= 2.46273 train/struct_loss= 7.61802 train/feat_loss= 1.17390\n",
      "Epoch: 0252 train_loss= 2.46274 train/struct_loss= 7.61812 train/feat_loss= 1.17390\n",
      "Epoch: 0253 train_loss= 2.46273 train/struct_loss= 7.61804 train/feat_loss= 1.17390\n",
      "Epoch: 0254 train_loss= 2.46261 train/struct_loss= 7.61746 train/feat_loss= 1.17390\n",
      "Epoch: 0255 train_loss= 2.46261 train/struct_loss= 7.61748 train/feat_loss= 1.17390\n",
      "Epoch: 0256 train_loss= 2.46278 train/struct_loss= 7.61832 train/feat_loss= 1.17390\n",
      "Epoch: 0257 train_loss= 2.46271 train/struct_loss= 7.61799 train/feat_loss= 1.17389\n",
      "Epoch: 0258 train_loss= 2.46276 train/struct_loss= 7.61822 train/feat_loss= 1.17389\n",
      "Epoch: 0259 train_loss= 2.46272 train/struct_loss= 7.61804 train/feat_loss= 1.17389\n",
      "Epoch: 0260 train_loss= 2.46258 train/struct_loss= 7.61732 train/feat_loss= 1.17389\n",
      "Epoch: 0260 Auc 0.8144729666402668\n",
      "Epoch: 0261 train_loss= 2.46273 train/struct_loss= 7.61805 train/feat_loss= 1.17389\n",
      "Epoch: 0262 train_loss= 2.46272 train/struct_loss= 7.61805 train/feat_loss= 1.17389\n",
      "Epoch: 0263 train_loss= 2.46277 train/struct_loss= 7.61830 train/feat_loss= 1.17389\n",
      "Epoch: 0264 train_loss= 2.46270 train/struct_loss= 7.61793 train/feat_loss= 1.17389\n",
      "Epoch: 0265 train_loss= 2.46263 train/struct_loss= 7.61758 train/feat_loss= 1.17389\n",
      "Epoch: 0266 train_loss= 2.46259 train/struct_loss= 7.61742 train/feat_loss= 1.17389\n",
      "Epoch: 0267 train_loss= 2.46267 train/struct_loss= 7.61783 train/feat_loss= 1.17389\n",
      "Epoch: 0268 train_loss= 2.46269 train/struct_loss= 7.61789 train/feat_loss= 1.17388\n",
      "Epoch: 0269 train_loss= 2.46263 train/struct_loss= 7.61759 train/feat_loss= 1.17388\n",
      "Epoch: 0270 train_loss= 2.46261 train/struct_loss= 7.61752 train/feat_loss= 1.17388\n",
      "Epoch: 0270 Auc 0.8144805029309319\n",
      "Epoch: 0271 train_loss= 2.46262 train/struct_loss= 7.61758 train/feat_loss= 1.17388\n",
      "Epoch: 0272 train_loss= 2.46272 train/struct_loss= 7.61806 train/feat_loss= 1.17388\n",
      "Epoch: 0273 train_loss= 2.46277 train/struct_loss= 7.61832 train/feat_loss= 1.17388\n",
      "Epoch: 0274 train_loss= 2.46267 train/struct_loss= 7.61782 train/feat_loss= 1.17388\n",
      "Epoch: 0275 train_loss= 2.46256 train/struct_loss= 7.61728 train/feat_loss= 1.17388\n",
      "Epoch: 0276 train_loss= 2.46246 train/struct_loss= 7.61675 train/feat_loss= 1.17388\n",
      "Epoch: 0277 train_loss= 2.46246 train/struct_loss= 7.61678 train/feat_loss= 1.17388\n",
      "Epoch: 0278 train_loss= 2.46262 train/struct_loss= 7.61757 train/feat_loss= 1.17388\n",
      "Epoch: 0279 train_loss= 2.46245 train/struct_loss= 7.61673 train/feat_loss= 1.17388\n",
      "Epoch: 0280 train_loss= 2.46250 train/struct_loss= 7.61696 train/feat_loss= 1.17388\n",
      "Epoch: 0280 Auc 0.8144969457469287\n",
      "Epoch: 0281 train_loss= 2.46244 train/struct_loss= 7.61668 train/feat_loss= 1.17388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0282 train_loss= 2.46259 train/struct_loss= 7.61745 train/feat_loss= 1.17388\n",
      "Epoch: 0283 train_loss= 2.46259 train/struct_loss= 7.61744 train/feat_loss= 1.17387\n",
      "Epoch: 0284 train_loss= 2.46251 train/struct_loss= 7.61707 train/feat_loss= 1.17387\n",
      "Epoch: 0285 train_loss= 2.46260 train/struct_loss= 7.61750 train/feat_loss= 1.17387\n",
      "Epoch: 0286 train_loss= 2.46252 train/struct_loss= 7.61713 train/feat_loss= 1.17387\n",
      "Epoch: 0287 train_loss= 2.46256 train/struct_loss= 7.61729 train/feat_loss= 1.17387\n",
      "Epoch: 0288 train_loss= 2.46261 train/struct_loss= 7.61758 train/feat_loss= 1.17387\n",
      "Epoch: 0289 train_loss= 2.46255 train/struct_loss= 7.61726 train/feat_loss= 1.17387\n",
      "Epoch: 0290 train_loss= 2.46237 train/struct_loss= 7.61640 train/feat_loss= 1.17387\n",
      "Epoch: 0290 Auc 0.8145113332109257\n",
      "Epoch: 0291 train_loss= 2.46237 train/struct_loss= 7.61639 train/feat_loss= 1.17387\n",
      "Epoch: 0292 train_loss= 2.46248 train/struct_loss= 7.61695 train/feat_loss= 1.17387\n",
      "Epoch: 0293 train_loss= 2.46237 train/struct_loss= 7.61638 train/feat_loss= 1.17387\n",
      "Epoch: 0294 train_loss= 2.46248 train/struct_loss= 7.61692 train/feat_loss= 1.17387\n",
      "Epoch: 0295 train_loss= 2.46237 train/struct_loss= 7.61639 train/feat_loss= 1.17387\n",
      "Epoch: 0296 train_loss= 2.46253 train/struct_loss= 7.61720 train/feat_loss= 1.17386\n",
      "Epoch: 0297 train_loss= 2.46246 train/struct_loss= 7.61682 train/feat_loss= 1.17386\n",
      "Epoch: 0298 train_loss= 2.46250 train/struct_loss= 7.61702 train/feat_loss= 1.17386\n",
      "Epoch: 0299 train_loss= 2.46249 train/struct_loss= 7.61701 train/feat_loss= 1.17386\n",
      "Epoch: 0300 train_loss= 2.46241 train/struct_loss= 7.61661 train/feat_loss= 1.17386\n",
      "Epoch: 0300 Auc 0.8145099629762593\n",
      "Epoch: 0301 train_loss= 2.46250 train/struct_loss= 7.61707 train/feat_loss= 1.17386\n",
      "Epoch: 0302 train_loss= 2.46241 train/struct_loss= 7.61663 train/feat_loss= 1.17386\n",
      "Epoch: 0303 train_loss= 2.46239 train/struct_loss= 7.61650 train/feat_loss= 1.17386\n",
      "Epoch: 0304 train_loss= 2.46250 train/struct_loss= 7.61707 train/feat_loss= 1.17386\n",
      "Epoch: 0305 train_loss= 2.46235 train/struct_loss= 7.61630 train/feat_loss= 1.17386\n",
      "Epoch: 0306 train_loss= 2.46259 train/struct_loss= 7.61753 train/feat_loss= 1.17386\n",
      "Epoch: 0307 train_loss= 2.46228 train/struct_loss= 7.61598 train/feat_loss= 1.17386\n",
      "Epoch: 0308 train_loss= 2.46238 train/struct_loss= 7.61646 train/feat_loss= 1.17386\n",
      "Epoch: 0309 train_loss= 2.46250 train/struct_loss= 7.61706 train/feat_loss= 1.17386\n",
      "Epoch: 0310 train_loss= 2.46238 train/struct_loss= 7.61647 train/feat_loss= 1.17386\n",
      "Epoch: 0310 Auc 0.8145250355575895\n",
      "Epoch: 0311 train_loss= 2.46228 train/struct_loss= 7.61599 train/feat_loss= 1.17386\n",
      "Epoch: 0312 train_loss= 2.46235 train/struct_loss= 7.61635 train/feat_loss= 1.17385\n",
      "Epoch: 0313 train_loss= 2.46245 train/struct_loss= 7.61686 train/feat_loss= 1.17385\n",
      "Epoch: 0314 train_loss= 2.46230 train/struct_loss= 7.61609 train/feat_loss= 1.17385\n",
      "Epoch: 0315 train_loss= 2.46230 train/struct_loss= 7.61610 train/feat_loss= 1.17385\n",
      "Epoch: 0316 train_loss= 2.46238 train/struct_loss= 7.61652 train/feat_loss= 1.17385\n",
      "Epoch: 0317 train_loss= 2.46228 train/struct_loss= 7.61600 train/feat_loss= 1.17385\n",
      "Epoch: 0318 train_loss= 2.46232 train/struct_loss= 7.61623 train/feat_loss= 1.17385\n",
      "Epoch: 0319 train_loss= 2.46224 train/struct_loss= 7.61580 train/feat_loss= 1.17385\n",
      "Epoch: 0320 train_loss= 2.46225 train/struct_loss= 7.61585 train/feat_loss= 1.17384\n",
      "Epoch: 0320 Auc 0.8145568935135832\n",
      "Epoch: 0321 train_loss= 2.46221 train/struct_loss= 7.61565 train/feat_loss= 1.17384\n",
      "Epoch: 0322 train_loss= 2.46234 train/struct_loss= 7.61634 train/feat_loss= 1.17384\n",
      "Epoch: 0323 train_loss= 2.46234 train/struct_loss= 7.61633 train/feat_loss= 1.17385\n",
      "Epoch: 0324 train_loss= 2.46225 train/struct_loss= 7.61587 train/feat_loss= 1.17384\n",
      "Epoch: 0325 train_loss= 2.46244 train/struct_loss= 7.61684 train/feat_loss= 1.17385\n",
      "Epoch: 0326 train_loss= 2.46216 train/struct_loss= 7.61542 train/feat_loss= 1.17384\n",
      "Epoch: 0327 train_loss= 2.46216 train/struct_loss= 7.61545 train/feat_loss= 1.17384\n",
      "Epoch: 0328 train_loss= 2.46236 train/struct_loss= 7.61643 train/feat_loss= 1.17384\n",
      "Epoch: 0329 train_loss= 2.46218 train/struct_loss= 7.61556 train/feat_loss= 1.17384\n",
      "Epoch: 0330 train_loss= 2.46232 train/struct_loss= 7.61622 train/feat_loss= 1.17384\n",
      "Epoch: 0330 Auc 0.8145544956029169\n",
      "Epoch: 0331 train_loss= 2.46226 train/struct_loss= 7.61596 train/feat_loss= 1.17384\n",
      "Epoch: 0332 train_loss= 2.46234 train/struct_loss= 7.61635 train/feat_loss= 1.17384\n",
      "Epoch: 0333 train_loss= 2.46232 train/struct_loss= 7.61628 train/feat_loss= 1.17383\n",
      "Epoch: 0334 train_loss= 2.46218 train/struct_loss= 7.61559 train/feat_loss= 1.17383\n",
      "Epoch: 0335 train_loss= 2.46221 train/struct_loss= 7.61575 train/feat_loss= 1.17383\n",
      "Epoch: 0336 train_loss= 2.46217 train/struct_loss= 7.61556 train/feat_loss= 1.17383\n",
      "Epoch: 0337 train_loss= 2.46224 train/struct_loss= 7.61592 train/feat_loss= 1.17382\n",
      "Epoch: 0338 train_loss= 2.46214 train/struct_loss= 7.61537 train/feat_loss= 1.17383\n",
      "Epoch: 0339 train_loss= 2.46220 train/struct_loss= 7.61570 train/feat_loss= 1.17383\n",
      "Epoch: 0340 train_loss= 2.46208 train/struct_loss= 7.61511 train/feat_loss= 1.17383\n",
      "Epoch: 0340 Auc 0.8145812151789116\n",
      "Epoch: 0341 train_loss= 2.46238 train/struct_loss= 7.61660 train/feat_loss= 1.17382\n",
      "Epoch: 0342 train_loss= 2.46205 train/struct_loss= 7.61494 train/feat_loss= 1.17382\n",
      "Epoch: 0343 train_loss= 2.46218 train/struct_loss= 7.61563 train/feat_loss= 1.17382\n",
      "Epoch: 0344 train_loss= 2.46212 train/struct_loss= 7.61532 train/feat_loss= 1.17382\n",
      "Epoch: 0345 train_loss= 2.46215 train/struct_loss= 7.61548 train/feat_loss= 1.17382\n",
      "Epoch: 0346 train_loss= 2.46207 train/struct_loss= 7.61506 train/feat_loss= 1.17382\n",
      "Epoch: 0347 train_loss= 2.46203 train/struct_loss= 7.61486 train/feat_loss= 1.17382\n",
      "Epoch: 0348 train_loss= 2.46206 train/struct_loss= 7.61502 train/feat_loss= 1.17382\n",
      "Epoch: 0349 train_loss= 2.46224 train/struct_loss= 7.61593 train/feat_loss= 1.17382\n",
      "Epoch: 0350 train_loss= 2.46213 train/struct_loss= 7.61538 train/feat_loss= 1.17382\n",
      "Epoch: 0350 Auc 0.814592862173576\n",
      "Epoch: 0351 train_loss= 2.46229 train/struct_loss= 7.61618 train/feat_loss= 1.17382\n",
      "Epoch: 0352 train_loss= 2.46202 train/struct_loss= 7.61482 train/feat_loss= 1.17381\n",
      "Epoch: 0353 train_loss= 2.46230 train/struct_loss= 7.61625 train/feat_loss= 1.17381\n",
      "Epoch: 0354 train_loss= 2.46218 train/struct_loss= 7.61564 train/feat_loss= 1.17381\n",
      "Epoch: 0355 train_loss= 2.46217 train/struct_loss= 7.61561 train/feat_loss= 1.17381\n",
      "Epoch: 0356 train_loss= 2.46228 train/struct_loss= 7.61617 train/feat_loss= 1.17381\n",
      "Epoch: 0357 train_loss= 2.46222 train/struct_loss= 7.61586 train/feat_loss= 1.17381\n",
      "Epoch: 0358 train_loss= 2.46225 train/struct_loss= 7.61600 train/feat_loss= 1.17381\n",
      "Epoch: 0359 train_loss= 2.46217 train/struct_loss= 7.61559 train/feat_loss= 1.17381\n",
      "Epoch: 0360 train_loss= 2.46203 train/struct_loss= 7.61495 train/feat_loss= 1.17381\n",
      "Epoch: 0360 Auc 0.8145887514695765\n",
      "Epoch: 0361 train_loss= 2.46199 train/struct_loss= 7.61475 train/feat_loss= 1.17380\n",
      "Epoch: 0362 train_loss= 2.46215 train/struct_loss= 7.61556 train/feat_loss= 1.17380\n",
      "Epoch: 0363 train_loss= 2.46208 train/struct_loss= 7.61516 train/feat_loss= 1.17380\n",
      "Epoch: 0364 train_loss= 2.46198 train/struct_loss= 7.61470 train/feat_loss= 1.17380\n",
      "Epoch: 0365 train_loss= 2.46200 train/struct_loss= 7.61478 train/feat_loss= 1.17380\n",
      "Epoch: 0366 train_loss= 2.46209 train/struct_loss= 7.61522 train/feat_loss= 1.17380\n",
      "Epoch: 0367 train_loss= 2.46214 train/struct_loss= 7.61552 train/feat_loss= 1.17380\n",
      "Epoch: 0368 train_loss= 2.46215 train/struct_loss= 7.61557 train/feat_loss= 1.17380\n",
      "Epoch: 0369 train_loss= 2.46201 train/struct_loss= 7.61487 train/feat_loss= 1.17380\n",
      "Epoch: 0370 train_loss= 2.46210 train/struct_loss= 7.61532 train/feat_loss= 1.17379\n",
      "Epoch: 0370 Auc 0.8145914919389095\n",
      "Epoch: 0371 train_loss= 2.46196 train/struct_loss= 7.61461 train/feat_loss= 1.17380\n",
      "Epoch: 0372 train_loss= 2.46196 train/struct_loss= 7.61463 train/feat_loss= 1.17379\n",
      "Epoch: 0373 train_loss= 2.46204 train/struct_loss= 7.61506 train/feat_loss= 1.17379\n",
      "Epoch: 0374 train_loss= 2.46206 train/struct_loss= 7.61513 train/feat_loss= 1.17379\n",
      "Epoch: 0375 train_loss= 2.46203 train/struct_loss= 7.61500 train/feat_loss= 1.17379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0376 train_loss= 2.46198 train/struct_loss= 7.61477 train/feat_loss= 1.17379\n",
      "Epoch: 0377 train_loss= 2.46193 train/struct_loss= 7.61450 train/feat_loss= 1.17379\n",
      "Epoch: 0378 train_loss= 2.46205 train/struct_loss= 7.61510 train/feat_loss= 1.17379\n",
      "Epoch: 0379 train_loss= 2.46192 train/struct_loss= 7.61446 train/feat_loss= 1.17378\n",
      "Epoch: 0380 train_loss= 2.46209 train/struct_loss= 7.61534 train/feat_loss= 1.17378\n",
      "Epoch: 0380 Auc 0.8146168412802377\n",
      "Epoch: 0381 train_loss= 2.46189 train/struct_loss= 7.61433 train/feat_loss= 1.17378\n",
      "Epoch: 0382 train_loss= 2.46211 train/struct_loss= 7.61544 train/feat_loss= 1.17378\n",
      "Epoch: 0383 train_loss= 2.46219 train/struct_loss= 7.61583 train/feat_loss= 1.17378\n",
      "Epoch: 0384 train_loss= 2.46219 train/struct_loss= 7.61582 train/feat_loss= 1.17378\n",
      "Epoch: 0385 train_loss= 2.46207 train/struct_loss= 7.61522 train/feat_loss= 1.17378\n",
      "Epoch: 0386 train_loss= 2.46199 train/struct_loss= 7.61483 train/feat_loss= 1.17378\n",
      "Epoch: 0387 train_loss= 2.46194 train/struct_loss= 7.61461 train/feat_loss= 1.17378\n",
      "Epoch: 0388 train_loss= 2.46203 train/struct_loss= 7.61507 train/feat_loss= 1.17377\n",
      "Epoch: 0389 train_loss= 2.46192 train/struct_loss= 7.61450 train/feat_loss= 1.17377\n",
      "Epoch: 0390 train_loss= 2.46192 train/struct_loss= 7.61452 train/feat_loss= 1.17378\n",
      "Epoch: 0390 Auc 0.8145956026429085\n",
      "Epoch: 0391 train_loss= 2.46218 train/struct_loss= 7.61579 train/feat_loss= 1.17377\n",
      "Epoch: 0392 train_loss= 2.46189 train/struct_loss= 7.61438 train/feat_loss= 1.17377\n",
      "Epoch: 0393 train_loss= 2.46201 train/struct_loss= 7.61495 train/feat_loss= 1.17378\n",
      "Epoch: 0394 train_loss= 2.46209 train/struct_loss= 7.61535 train/feat_loss= 1.17377\n",
      "Epoch: 0395 train_loss= 2.46202 train/struct_loss= 7.61501 train/feat_loss= 1.17377\n",
      "Epoch: 0396 train_loss= 2.46207 train/struct_loss= 7.61524 train/feat_loss= 1.17377\n",
      "Epoch: 0397 train_loss= 2.46212 train/struct_loss= 7.61551 train/feat_loss= 1.17377\n",
      "Epoch: 0398 train_loss= 2.46174 train/struct_loss= 7.61362 train/feat_loss= 1.17377\n",
      "Epoch: 0399 train_loss= 2.46204 train/struct_loss= 7.61512 train/feat_loss= 1.17377\n",
      "Epoch: 0400 train_loss= 2.46221 train/struct_loss= 7.61600 train/feat_loss= 1.17377\n",
      "Epoch: 0400 Auc 0.8146223222189031\n",
      "Epoch: 0401 train_loss= 2.46205 train/struct_loss= 7.61520 train/feat_loss= 1.17377\n",
      "Epoch: 0402 train_loss= 2.46204 train/struct_loss= 7.61511 train/feat_loss= 1.17377\n",
      "Epoch: 0403 train_loss= 2.46214 train/struct_loss= 7.61565 train/feat_loss= 1.17376\n",
      "Epoch: 0404 train_loss= 2.46181 train/struct_loss= 7.61401 train/feat_loss= 1.17376\n",
      "Epoch: 0405 train_loss= 2.46183 train/struct_loss= 7.61410 train/feat_loss= 1.17377\n",
      "Epoch: 0406 train_loss= 2.46202 train/struct_loss= 7.61507 train/feat_loss= 1.17376\n",
      "Epoch: 0407 train_loss= 2.46197 train/struct_loss= 7.61480 train/feat_loss= 1.17376\n",
      "Epoch: 0408 train_loss= 2.46184 train/struct_loss= 7.61418 train/feat_loss= 1.17376\n",
      "Epoch: 0409 train_loss= 2.46188 train/struct_loss= 7.61437 train/feat_loss= 1.17375\n",
      "Epoch: 0410 train_loss= 2.46204 train/struct_loss= 7.61516 train/feat_loss= 1.17376\n",
      "Epoch: 0410 Auc 0.8146490417948978\n",
      "Epoch: 0411 train_loss= 2.46194 train/struct_loss= 7.61469 train/feat_loss= 1.17376\n",
      "Epoch: 0412 train_loss= 2.46197 train/struct_loss= 7.61484 train/feat_loss= 1.17376\n",
      "Epoch: 0413 train_loss= 2.46180 train/struct_loss= 7.61397 train/feat_loss= 1.17375\n",
      "Epoch: 0414 train_loss= 2.46196 train/struct_loss= 7.61478 train/feat_loss= 1.17375\n",
      "Epoch: 0415 train_loss= 2.46186 train/struct_loss= 7.61429 train/feat_loss= 1.17375\n",
      "Epoch: 0416 train_loss= 2.46187 train/struct_loss= 7.61437 train/feat_loss= 1.17375\n",
      "Epoch: 0417 train_loss= 2.46179 train/struct_loss= 7.61398 train/feat_loss= 1.17375\n",
      "Epoch: 0418 train_loss= 2.46209 train/struct_loss= 7.61545 train/feat_loss= 1.17375\n",
      "Epoch: 0419 train_loss= 2.46175 train/struct_loss= 7.61378 train/feat_loss= 1.17375\n",
      "Epoch: 0420 train_loss= 2.46185 train/struct_loss= 7.61425 train/feat_loss= 1.17375\n",
      "Epoch: 0420 Auc 0.8145962877602417\n",
      "Epoch: 0421 train_loss= 2.46204 train/struct_loss= 7.61520 train/feat_loss= 1.17375\n",
      "Epoch: 0422 train_loss= 2.46174 train/struct_loss= 7.61373 train/feat_loss= 1.17374\n",
      "Epoch: 0423 train_loss= 2.46200 train/struct_loss= 7.61503 train/feat_loss= 1.17374\n",
      "Epoch: 0424 train_loss= 2.46183 train/struct_loss= 7.61418 train/feat_loss= 1.17374\n",
      "Epoch: 0425 train_loss= 2.46173 train/struct_loss= 7.61368 train/feat_loss= 1.17374\n",
      "Epoch: 0426 train_loss= 2.46168 train/struct_loss= 7.61342 train/feat_loss= 1.17374\n",
      "Epoch: 0427 train_loss= 2.46187 train/struct_loss= 7.61440 train/feat_loss= 1.17374\n",
      "Epoch: 0428 train_loss= 2.46185 train/struct_loss= 7.61431 train/feat_loss= 1.17374\n",
      "Epoch: 0429 train_loss= 2.46182 train/struct_loss= 7.61414 train/feat_loss= 1.17374\n",
      "Epoch: 0430 train_loss= 2.46172 train/struct_loss= 7.61369 train/feat_loss= 1.17373\n",
      "Epoch: 0430 Auc 0.8146558929682297\n",
      "Epoch: 0431 train_loss= 2.46177 train/struct_loss= 7.61394 train/feat_loss= 1.17373\n",
      "Epoch: 0432 train_loss= 2.46203 train/struct_loss= 7.61522 train/feat_loss= 1.17373\n",
      "Epoch: 0433 train_loss= 2.46181 train/struct_loss= 7.61413 train/feat_loss= 1.17372\n",
      "Epoch: 0434 train_loss= 2.46165 train/struct_loss= 7.61336 train/feat_loss= 1.17372\n",
      "Epoch: 0435 train_loss= 2.46175 train/struct_loss= 7.61388 train/feat_loss= 1.17372\n",
      "Epoch: 0436 train_loss= 2.46175 train/struct_loss= 7.61388 train/feat_loss= 1.17372\n",
      "Epoch: 0437 train_loss= 2.46168 train/struct_loss= 7.61352 train/feat_loss= 1.17372\n",
      "Epoch: 0438 train_loss= 2.46184 train/struct_loss= 7.61434 train/feat_loss= 1.17372\n",
      "Epoch: 0439 train_loss= 2.46161 train/struct_loss= 7.61315 train/feat_loss= 1.17372\n",
      "Epoch: 0440 train_loss= 2.46169 train/struct_loss= 7.61358 train/feat_loss= 1.17372\n",
      "Epoch: 0440 Auc 0.8146668548455608\n",
      "Epoch: 0441 train_loss= 2.46169 train/struct_loss= 7.61360 train/feat_loss= 1.17371\n",
      "Epoch: 0442 train_loss= 2.46179 train/struct_loss= 7.61410 train/feat_loss= 1.17371\n",
      "Epoch: 0443 train_loss= 2.46179 train/struct_loss= 7.61409 train/feat_loss= 1.17371\n",
      "Epoch: 0444 train_loss= 2.46165 train/struct_loss= 7.61337 train/feat_loss= 1.17371\n",
      "Epoch: 0445 train_loss= 2.46173 train/struct_loss= 7.61380 train/feat_loss= 1.17371\n",
      "Epoch: 0446 train_loss= 2.46176 train/struct_loss= 7.61394 train/feat_loss= 1.17371\n",
      "Epoch: 0447 train_loss= 2.46175 train/struct_loss= 7.61390 train/feat_loss= 1.17371\n",
      "Epoch: 0448 train_loss= 2.46162 train/struct_loss= 7.61327 train/feat_loss= 1.17371\n",
      "Epoch: 0449 train_loss= 2.46167 train/struct_loss= 7.61353 train/feat_loss= 1.17371\n",
      "Epoch: 0450 train_loss= 2.46173 train/struct_loss= 7.61385 train/feat_loss= 1.17371\n",
      "Epoch: 0450 Auc 0.8146497269122309\n",
      "Epoch: 0451 train_loss= 2.46156 train/struct_loss= 7.61301 train/feat_loss= 1.17370\n",
      "Epoch: 0452 train_loss= 2.46174 train/struct_loss= 7.61387 train/feat_loss= 1.17370\n",
      "Epoch: 0453 train_loss= 2.46169 train/struct_loss= 7.61365 train/feat_loss= 1.17370\n",
      "Epoch: 0454 train_loss= 2.46164 train/struct_loss= 7.61340 train/feat_loss= 1.17370\n",
      "Epoch: 0455 train_loss= 2.46166 train/struct_loss= 7.61349 train/feat_loss= 1.17370\n",
      "Epoch: 0456 train_loss= 2.46172 train/struct_loss= 7.61379 train/feat_loss= 1.17370\n",
      "Epoch: 0457 train_loss= 2.46178 train/struct_loss= 7.61409 train/feat_loss= 1.17370\n",
      "Epoch: 0458 train_loss= 2.46175 train/struct_loss= 7.61395 train/feat_loss= 1.17370\n",
      "Epoch: 0459 train_loss= 2.46155 train/struct_loss= 7.61297 train/feat_loss= 1.17370\n",
      "Epoch: 0460 train_loss= 2.46171 train/struct_loss= 7.61377 train/feat_loss= 1.17370\n",
      "Epoch: 0460 Auc 0.8146709655495601\n",
      "Epoch: 0461 train_loss= 2.46166 train/struct_loss= 7.61353 train/feat_loss= 1.17369\n",
      "Epoch: 0462 train_loss= 2.46142 train/struct_loss= 7.61232 train/feat_loss= 1.17369\n",
      "Epoch: 0463 train_loss= 2.46167 train/struct_loss= 7.61360 train/feat_loss= 1.17369\n",
      "Epoch: 0464 train_loss= 2.46144 train/struct_loss= 7.61245 train/feat_loss= 1.17368\n",
      "Epoch: 0465 train_loss= 2.46157 train/struct_loss= 7.61309 train/feat_loss= 1.17369\n",
      "Epoch: 0466 train_loss= 2.46149 train/struct_loss= 7.61273 train/feat_loss= 1.17369\n",
      "Epoch: 0467 train_loss= 2.46160 train/struct_loss= 7.61328 train/feat_loss= 1.17368\n",
      "Epoch: 0468 train_loss= 2.46158 train/struct_loss= 7.61314 train/feat_loss= 1.17369\n",
      "Epoch: 0469 train_loss= 2.46153 train/struct_loss= 7.61294 train/feat_loss= 1.17368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0470 train_loss= 2.46149 train/struct_loss= 7.61271 train/feat_loss= 1.17368\n",
      "Epoch: 0470 Auc 0.8146600036722289\n",
      "Epoch: 0471 train_loss= 2.46166 train/struct_loss= 7.61359 train/feat_loss= 1.17368\n",
      "Epoch: 0472 train_loss= 2.46147 train/struct_loss= 7.61262 train/feat_loss= 1.17368\n",
      "Epoch: 0473 train_loss= 2.46149 train/struct_loss= 7.61276 train/feat_loss= 1.17368\n",
      "Epoch: 0474 train_loss= 2.46156 train/struct_loss= 7.61307 train/feat_loss= 1.17368\n",
      "Epoch: 0475 train_loss= 2.46140 train/struct_loss= 7.61232 train/feat_loss= 1.17368\n",
      "Epoch: 0476 train_loss= 2.46159 train/struct_loss= 7.61321 train/feat_loss= 1.17368\n",
      "Epoch: 0477 train_loss= 2.46152 train/struct_loss= 7.61292 train/feat_loss= 1.17368\n",
      "Epoch: 0478 train_loss= 2.46150 train/struct_loss= 7.61282 train/feat_loss= 1.17367\n",
      "Epoch: 0479 train_loss= 2.46142 train/struct_loss= 7.61237 train/feat_loss= 1.17368\n",
      "Epoch: 0480 train_loss= 2.46165 train/struct_loss= 7.61354 train/feat_loss= 1.17368\n",
      "Epoch: 0480 Auc 0.8146702804322268\n",
      "Epoch: 0481 train_loss= 2.46142 train/struct_loss= 7.61237 train/feat_loss= 1.17368\n",
      "Epoch: 0482 train_loss= 2.46135 train/struct_loss= 7.61204 train/feat_loss= 1.17367\n",
      "Epoch: 0483 train_loss= 2.46142 train/struct_loss= 7.61241 train/feat_loss= 1.17367\n",
      "Epoch: 0484 train_loss= 2.46156 train/struct_loss= 7.61312 train/feat_loss= 1.17367\n",
      "Epoch: 0485 train_loss= 2.46144 train/struct_loss= 7.61253 train/feat_loss= 1.17367\n",
      "Epoch: 0486 train_loss= 2.46160 train/struct_loss= 7.61332 train/feat_loss= 1.17367\n",
      "Epoch: 0487 train_loss= 2.46136 train/struct_loss= 7.61212 train/feat_loss= 1.17367\n",
      "Epoch: 0488 train_loss= 2.46147 train/struct_loss= 7.61269 train/feat_loss= 1.17367\n",
      "Epoch: 0489 train_loss= 2.46154 train/struct_loss= 7.61302 train/feat_loss= 1.17367\n",
      "Epoch: 0490 train_loss= 2.46142 train/struct_loss= 7.61246 train/feat_loss= 1.17366\n",
      "Epoch: 0490 Auc 0.8146908339522226\n",
      "Epoch: 0491 train_loss= 2.46128 train/struct_loss= 7.61176 train/feat_loss= 1.17366\n",
      "Epoch: 0492 train_loss= 2.46158 train/struct_loss= 7.61323 train/feat_loss= 1.17366\n",
      "Epoch: 0493 train_loss= 2.46133 train/struct_loss= 7.61200 train/feat_loss= 1.17366\n",
      "Epoch: 0494 train_loss= 2.46136 train/struct_loss= 7.61216 train/feat_loss= 1.17366\n",
      "Epoch: 0495 train_loss= 2.46141 train/struct_loss= 7.61245 train/feat_loss= 1.17365\n",
      "Epoch: 0496 train_loss= 2.46132 train/struct_loss= 7.61196 train/feat_loss= 1.17366\n",
      "Epoch: 0497 train_loss= 2.46134 train/struct_loss= 7.61209 train/feat_loss= 1.17365\n",
      "Epoch: 0498 train_loss= 2.46126 train/struct_loss= 7.61167 train/feat_loss= 1.17365\n",
      "Epoch: 0499 train_loss= 2.46126 train/struct_loss= 7.61168 train/feat_loss= 1.17366\n",
      "Epoch: 0499 Auc 0.81468809348289\n"
     ]
    }
   ],
   "source": [
    "train_dominant(dataset=\"BlogCatalog\", hidden_dim=64, max_epoch=500, lr=5e-4, dropout=0.3, alpha=0.8, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "782bc7c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train_loss= 4.34323 train/struct_loss= 16.68764 train/feat_loss= 1.25712\n",
      "Epoch: 0000 Auc 0.6479319048180192\n",
      "Epoch: 0001 train_loss= 14.08281 train/struct_loss= 52.20726 train/feat_loss= 4.55171\n",
      "Epoch: 0002 train_loss= 2.67803 train/struct_loss= 7.71949 train/feat_loss= 1.41767\n",
      "Epoch: 0003 train_loss= 2.50623 train/struct_loss= 7.68096 train/feat_loss= 1.21255\n",
      "Epoch: 0004 train_loss= 2.47950 train/struct_loss= 7.67807 train/feat_loss= 1.17986\n",
      "Epoch: 0005 train_loss= 2.51940 train/struct_loss= 7.88895 train/feat_loss= 1.17702\n",
      "Epoch: 0006 train_loss= 2.47858 train/struct_loss= 7.68228 train/feat_loss= 1.17765\n",
      "Epoch: 0007 train_loss= 2.48236 train/struct_loss= 7.71183 train/feat_loss= 1.17500\n",
      "Epoch: 0008 train_loss= 2.47954 train/struct_loss= 7.69773 train/feat_loss= 1.17500\n",
      "Epoch: 0009 train_loss= 2.47969 train/struct_loss= 7.69844 train/feat_loss= 1.17500\n",
      "Epoch: 0010 train_loss= 2.47787 train/struct_loss= 7.68937 train/feat_loss= 1.17500\n",
      "Epoch: 0010 Auc 0.8133027862351705\n",
      "Epoch: 0011 train_loss= 2.47739 train/struct_loss= 7.68696 train/feat_loss= 1.17500\n",
      "Epoch: 0012 train_loss= 2.47684 train/struct_loss= 7.68419 train/feat_loss= 1.17500\n",
      "Epoch: 0013 train_loss= 2.48229 train/struct_loss= 7.71146 train/feat_loss= 1.17500\n",
      "Epoch: 0014 train_loss= 2.47642 train/struct_loss= 7.68213 train/feat_loss= 1.17500\n",
      "Epoch: 0015 train_loss= 2.47766 train/struct_loss= 7.68831 train/feat_loss= 1.17500\n",
      "Epoch: 0016 train_loss= 2.47698 train/struct_loss= 7.68492 train/feat_loss= 1.17500\n",
      "Epoch: 0017 train_loss= 2.47655 train/struct_loss= 7.68277 train/feat_loss= 1.17500\n",
      "Epoch: 0018 train_loss= 2.47813 train/struct_loss= 7.69068 train/feat_loss= 1.17500\n",
      "Epoch: 0019 train_loss= 2.47551 train/struct_loss= 7.67755 train/feat_loss= 1.17500\n",
      "Epoch: 0020 train_loss= 2.47676 train/struct_loss= 7.68380 train/feat_loss= 1.17500\n",
      "Epoch: 0020 Auc 0.8134086368631492\n",
      "Epoch: 0021 train_loss= 2.47697 train/struct_loss= 7.68487 train/feat_loss= 1.17500\n",
      "Epoch: 0022 train_loss= 2.47371 train/struct_loss= 7.66854 train/feat_loss= 1.17500\n",
      "Epoch: 0023 train_loss= 2.47697 train/struct_loss= 7.68488 train/feat_loss= 1.17500\n",
      "Epoch: 0024 train_loss= 2.47312 train/struct_loss= 7.66560 train/feat_loss= 1.17500\n",
      "Epoch: 0025 train_loss= 2.47496 train/struct_loss= 7.67481 train/feat_loss= 1.17500\n",
      "Epoch: 0026 train_loss= 2.47519 train/struct_loss= 7.67596 train/feat_loss= 1.17500\n",
      "Epoch: 0027 train_loss= 2.47392 train/struct_loss= 7.66962 train/feat_loss= 1.17500\n",
      "Epoch: 0028 train_loss= 2.47360 train/struct_loss= 7.66802 train/feat_loss= 1.17500\n",
      "Epoch: 0029 train_loss= 2.47382 train/struct_loss= 7.66909 train/feat_loss= 1.17500\n",
      "Epoch: 0030 train_loss= 2.47177 train/struct_loss= 7.65884 train/feat_loss= 1.17500\n",
      "Epoch: 0030 Auc 0.8136535663097663\n",
      "Epoch: 0031 train_loss= 2.47335 train/struct_loss= 7.66678 train/feat_loss= 1.17500\n",
      "Epoch: 0032 train_loss= 2.47323 train/struct_loss= 7.66617 train/feat_loss= 1.17500\n",
      "Epoch: 0033 train_loss= 2.47404 train/struct_loss= 7.67020 train/feat_loss= 1.17500\n",
      "Epoch: 0034 train_loss= 2.47302 train/struct_loss= 7.66510 train/feat_loss= 1.17500\n",
      "Epoch: 0035 train_loss= 2.47262 train/struct_loss= 7.66309 train/feat_loss= 1.17500\n",
      "Epoch: 0036 train_loss= 2.47176 train/struct_loss= 7.65882 train/feat_loss= 1.17500\n",
      "Epoch: 0037 train_loss= 2.47184 train/struct_loss= 7.65923 train/feat_loss= 1.17500\n",
      "Epoch: 0038 train_loss= 2.47245 train/struct_loss= 7.66225 train/feat_loss= 1.17500\n",
      "Epoch: 0039 train_loss= 2.47259 train/struct_loss= 7.66294 train/feat_loss= 1.17500\n",
      "Epoch: 0040 train_loss= 2.47300 train/struct_loss= 7.66501 train/feat_loss= 1.17500\n",
      "Epoch: 0040 Auc 0.8138316968163969\n",
      "Epoch: 0041 train_loss= 2.47249 train/struct_loss= 7.66245 train/feat_loss= 1.17500\n",
      "Epoch: 0042 train_loss= 2.47182 train/struct_loss= 7.65912 train/feat_loss= 1.17500\n",
      "Epoch: 0043 train_loss= 2.47273 train/struct_loss= 7.66366 train/feat_loss= 1.17500\n",
      "Epoch: 0044 train_loss= 2.47228 train/struct_loss= 7.66142 train/feat_loss= 1.17500\n",
      "Epoch: 0045 train_loss= 2.47208 train/struct_loss= 7.66042 train/feat_loss= 1.17500\n",
      "Epoch: 0046 train_loss= 2.47258 train/struct_loss= 7.66292 train/feat_loss= 1.17500\n",
      "Epoch: 0047 train_loss= 2.47171 train/struct_loss= 7.65858 train/feat_loss= 1.17500\n",
      "Epoch: 0048 train_loss= 2.47202 train/struct_loss= 7.66012 train/feat_loss= 1.17500\n",
      "Epoch: 0049 train_loss= 2.47193 train/struct_loss= 7.65968 train/feat_loss= 1.17500\n",
      "Epoch: 0050 train_loss= 2.47173 train/struct_loss= 7.65863 train/feat_loss= 1.17500\n",
      "Epoch: 0050 Auc 0.8138570461577248\n",
      "Epoch: 0051 train_loss= 2.47230 train/struct_loss= 7.66148 train/feat_loss= 1.17500\n",
      "Epoch: 0052 train_loss= 2.47173 train/struct_loss= 7.65868 train/feat_loss= 1.17500\n",
      "Epoch: 0053 train_loss= 2.47189 train/struct_loss= 7.65944 train/feat_loss= 1.17500\n",
      "Epoch: 0054 train_loss= 2.47182 train/struct_loss= 7.65912 train/feat_loss= 1.17500\n",
      "Epoch: 0055 train_loss= 2.47203 train/struct_loss= 7.66016 train/feat_loss= 1.17500\n",
      "Epoch: 0056 train_loss= 2.47183 train/struct_loss= 7.65914 train/feat_loss= 1.17500\n",
      "Epoch: 0057 train_loss= 2.47182 train/struct_loss= 7.65908 train/feat_loss= 1.17500\n",
      "Epoch: 0058 train_loss= 2.47171 train/struct_loss= 7.65854 train/feat_loss= 1.17500\n",
      "Epoch: 0059 train_loss= 2.47219 train/struct_loss= 7.66098 train/feat_loss= 1.17500\n",
      "Epoch: 0060 train_loss= 2.47175 train/struct_loss= 7.65878 train/feat_loss= 1.17500\n",
      "Epoch: 0060 Auc 0.8138632122137237\n",
      "Epoch: 0061 train_loss= 2.47214 train/struct_loss= 7.66069 train/feat_loss= 1.17500\n",
      "Epoch: 0062 train_loss= 2.47203 train/struct_loss= 7.66013 train/feat_loss= 1.17500\n",
      "Epoch: 0063 train_loss= 2.47146 train/struct_loss= 7.65730 train/feat_loss= 1.17500\n",
      "Epoch: 0064 train_loss= 2.47222 train/struct_loss= 7.66109 train/feat_loss= 1.17500\n",
      "Epoch: 0065 train_loss= 2.47192 train/struct_loss= 7.65960 train/feat_loss= 1.17500\n",
      "Epoch: 0066 train_loss= 2.47220 train/struct_loss= 7.66100 train/feat_loss= 1.17500\n",
      "Epoch: 0067 train_loss= 2.47188 train/struct_loss= 7.65941 train/feat_loss= 1.17500\n",
      "Epoch: 0068 train_loss= 2.47181 train/struct_loss= 7.65907 train/feat_loss= 1.17500\n",
      "Epoch: 0069 train_loss= 2.47150 train/struct_loss= 7.65753 train/feat_loss= 1.17500\n",
      "Epoch: 0070 train_loss= 2.47178 train/struct_loss= 7.65889 train/feat_loss= 1.17500\n",
      "Epoch: 0070 Auc 0.8138193647043992\n",
      "Epoch: 0071 train_loss= 2.47184 train/struct_loss= 7.65918 train/feat_loss= 1.17500\n",
      "Epoch: 0072 train_loss= 2.47210 train/struct_loss= 7.66049 train/feat_loss= 1.17500\n",
      "Epoch: 0073 train_loss= 2.47154 train/struct_loss= 7.65772 train/feat_loss= 1.17500\n",
      "Epoch: 0074 train_loss= 2.47182 train/struct_loss= 7.65913 train/feat_loss= 1.17500\n",
      "Epoch: 0075 train_loss= 2.47219 train/struct_loss= 7.66094 train/feat_loss= 1.17500\n",
      "Epoch: 0076 train_loss= 2.47199 train/struct_loss= 7.65996 train/feat_loss= 1.17500\n",
      "Epoch: 0077 train_loss= 2.47198 train/struct_loss= 7.65989 train/feat_loss= 1.17500\n",
      "Epoch: 0078 train_loss= 2.47188 train/struct_loss= 7.65939 train/feat_loss= 1.17500\n",
      "Epoch: 0079 train_loss= 2.47212 train/struct_loss= 7.66062 train/feat_loss= 1.17500\n",
      "Epoch: 0080 train_loss= 2.47195 train/struct_loss= 7.65975 train/feat_loss= 1.17500\n",
      "Epoch: 0080 Auc 0.813835807520396\n",
      "Epoch: 0081 train_loss= 2.47158 train/struct_loss= 7.65790 train/feat_loss= 1.17500\n",
      "Epoch: 0082 train_loss= 2.47180 train/struct_loss= 7.65900 train/feat_loss= 1.17500\n",
      "Epoch: 0083 train_loss= 2.47169 train/struct_loss= 7.65845 train/feat_loss= 1.17500\n",
      "Epoch: 0084 train_loss= 2.47168 train/struct_loss= 7.65838 train/feat_loss= 1.17500\n",
      "Epoch: 0085 train_loss= 2.47215 train/struct_loss= 7.66078 train/feat_loss= 1.17500\n",
      "Epoch: 0086 train_loss= 2.47207 train/struct_loss= 7.66036 train/feat_loss= 1.17500\n",
      "Epoch: 0087 train_loss= 2.47145 train/struct_loss= 7.65728 train/feat_loss= 1.17500\n",
      "Epoch: 0088 train_loss= 2.47189 train/struct_loss= 7.65945 train/feat_loss= 1.17500\n",
      "Epoch: 0089 train_loss= 2.47176 train/struct_loss= 7.65879 train/feat_loss= 1.17500\n",
      "Epoch: 0090 train_loss= 2.47146 train/struct_loss= 7.65731 train/feat_loss= 1.17500\n",
      "Epoch: 0090 Auc 0.8138426586937278\n",
      "Epoch: 0091 train_loss= 2.47180 train/struct_loss= 7.65902 train/feat_loss= 1.17500\n",
      "Epoch: 0092 train_loss= 2.47159 train/struct_loss= 7.65794 train/feat_loss= 1.17500\n",
      "Epoch: 0093 train_loss= 2.47139 train/struct_loss= 7.65697 train/feat_loss= 1.17500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0094 train_loss= 2.47205 train/struct_loss= 7.66024 train/feat_loss= 1.17500\n",
      "Epoch: 0095 train_loss= 2.47147 train/struct_loss= 7.65734 train/feat_loss= 1.17500\n",
      "Epoch: 0096 train_loss= 2.47184 train/struct_loss= 7.65922 train/feat_loss= 1.17500\n",
      "Epoch: 0097 train_loss= 2.47201 train/struct_loss= 7.66006 train/feat_loss= 1.17500\n",
      "Epoch: 0098 train_loss= 2.47163 train/struct_loss= 7.65815 train/feat_loss= 1.17500\n",
      "Epoch: 0099 train_loss= 2.47213 train/struct_loss= 7.66067 train/feat_loss= 1.17500\n",
      "Epoch: 0100 train_loss= 2.47139 train/struct_loss= 7.65697 train/feat_loss= 1.17500\n",
      "Epoch: 0100 Auc 0.8138474545150602\n",
      "Epoch: 0101 train_loss= 2.47151 train/struct_loss= 7.65755 train/feat_loss= 1.17500\n",
      "Epoch: 0102 train_loss= 2.47147 train/struct_loss= 7.65737 train/feat_loss= 1.17500\n",
      "Epoch: 0103 train_loss= 2.47169 train/struct_loss= 7.65843 train/feat_loss= 1.17500\n",
      "Epoch: 0104 train_loss= 2.47185 train/struct_loss= 7.65927 train/feat_loss= 1.17500\n",
      "Epoch: 0105 train_loss= 2.47138 train/struct_loss= 7.65689 train/feat_loss= 1.17500\n",
      "Epoch: 0106 train_loss= 2.47165 train/struct_loss= 7.65826 train/feat_loss= 1.17500\n",
      "Epoch: 0107 train_loss= 2.47164 train/struct_loss= 7.65820 train/feat_loss= 1.17500\n",
      "Epoch: 0108 train_loss= 2.47185 train/struct_loss= 7.65924 train/feat_loss= 1.17500\n",
      "Epoch: 0109 train_loss= 2.47196 train/struct_loss= 7.65980 train/feat_loss= 1.17500\n",
      "Epoch: 0110 train_loss= 2.47170 train/struct_loss= 7.65852 train/feat_loss= 1.17500\n",
      "Epoch: 0110 Auc 0.8138275861123976\n",
      "Epoch: 0111 train_loss= 2.47198 train/struct_loss= 7.65991 train/feat_loss= 1.17500\n",
      "Epoch: 0112 train_loss= 2.47219 train/struct_loss= 7.66095 train/feat_loss= 1.17500\n",
      "Epoch: 0113 train_loss= 2.47214 train/struct_loss= 7.66070 train/feat_loss= 1.17500\n",
      "Epoch: 0114 train_loss= 2.47158 train/struct_loss= 7.65792 train/feat_loss= 1.17500\n",
      "Epoch: 0115 train_loss= 2.47171 train/struct_loss= 7.65855 train/feat_loss= 1.17500\n",
      "Epoch: 0116 train_loss= 2.47185 train/struct_loss= 7.65923 train/feat_loss= 1.17500\n",
      "Epoch: 0117 train_loss= 2.47196 train/struct_loss= 7.65982 train/feat_loss= 1.17500\n",
      "Epoch: 0118 train_loss= 2.47171 train/struct_loss= 7.65857 train/feat_loss= 1.17500\n",
      "Epoch: 0119 train_loss= 2.47173 train/struct_loss= 7.65865 train/feat_loss= 1.17500\n",
      "Epoch: 0120 train_loss= 2.47181 train/struct_loss= 7.65907 train/feat_loss= 1.17500\n",
      "Epoch: 0120 Auc 0.8138337521683963\n",
      "Epoch: 0121 train_loss= 2.47177 train/struct_loss= 7.65888 train/feat_loss= 1.17500\n",
      "Epoch: 0122 train_loss= 2.47167 train/struct_loss= 7.65835 train/feat_loss= 1.17500\n",
      "Epoch: 0123 train_loss= 2.47153 train/struct_loss= 7.65768 train/feat_loss= 1.17500\n",
      "Epoch: 0124 train_loss= 2.47239 train/struct_loss= 7.66195 train/feat_loss= 1.17500\n",
      "Epoch: 0125 train_loss= 2.47199 train/struct_loss= 7.65997 train/feat_loss= 1.17500\n",
      "Epoch: 0126 train_loss= 2.47195 train/struct_loss= 7.65978 train/feat_loss= 1.17500\n",
      "Epoch: 0127 train_loss= 2.47173 train/struct_loss= 7.65864 train/feat_loss= 1.17500\n",
      "Epoch: 0128 train_loss= 2.47252 train/struct_loss= 7.66259 train/feat_loss= 1.17500\n",
      "Epoch: 0129 train_loss= 2.47138 train/struct_loss= 7.65688 train/feat_loss= 1.17500\n",
      "Epoch: 0130 train_loss= 2.47186 train/struct_loss= 7.65933 train/feat_loss= 1.17500\n",
      "Epoch: 0130 Auc 0.8138296414643972\n",
      "Epoch: 0131 train_loss= 2.47163 train/struct_loss= 7.65814 train/feat_loss= 1.17500\n",
      "Epoch: 0132 train_loss= 2.47184 train/struct_loss= 7.65919 train/feat_loss= 1.17500\n",
      "Epoch: 0133 train_loss= 2.47183 train/struct_loss= 7.65918 train/feat_loss= 1.17500\n",
      "Epoch: 0134 train_loss= 2.47201 train/struct_loss= 7.66008 train/feat_loss= 1.17500\n",
      "Epoch: 0135 train_loss= 2.47146 train/struct_loss= 7.65733 train/feat_loss= 1.17500\n",
      "Epoch: 0136 train_loss= 2.47181 train/struct_loss= 7.65904 train/feat_loss= 1.17500\n",
      "Epoch: 0137 train_loss= 2.47146 train/struct_loss= 7.65731 train/feat_loss= 1.17500\n",
      "Epoch: 0138 train_loss= 2.47169 train/struct_loss= 7.65846 train/feat_loss= 1.17500\n",
      "Epoch: 0139 train_loss= 2.47217 train/struct_loss= 7.66087 train/feat_loss= 1.17500\n",
      "Epoch: 0140 train_loss= 2.47155 train/struct_loss= 7.65778 train/feat_loss= 1.17500\n",
      "Epoch: 0140 Auc 0.8138412884590613\n",
      "Epoch: 0141 train_loss= 2.47200 train/struct_loss= 7.66001 train/feat_loss= 1.17500\n",
      "Epoch: 0142 train_loss= 2.47181 train/struct_loss= 7.65906 train/feat_loss= 1.17500\n",
      "Epoch: 0143 train_loss= 2.47186 train/struct_loss= 7.65929 train/feat_loss= 1.17500\n",
      "Epoch: 0144 train_loss= 2.47152 train/struct_loss= 7.65763 train/feat_loss= 1.17500\n",
      "Epoch: 0145 train_loss= 2.47223 train/struct_loss= 7.66118 train/feat_loss= 1.17500\n",
      "Epoch: 0146 train_loss= 2.47174 train/struct_loss= 7.65869 train/feat_loss= 1.17500\n",
      "Epoch: 0147 train_loss= 2.47124 train/struct_loss= 7.65618 train/feat_loss= 1.17500\n",
      "Epoch: 0148 train_loss= 2.47187 train/struct_loss= 7.65936 train/feat_loss= 1.17500\n",
      "Epoch: 0149 train_loss= 2.47189 train/struct_loss= 7.65945 train/feat_loss= 1.17500\n",
      "Epoch: 0150 train_loss= 2.47145 train/struct_loss= 7.65727 train/feat_loss= 1.17500\n",
      "Epoch: 0150 Auc 0.8138255307603979\n",
      "Epoch: 0151 train_loss= 2.47156 train/struct_loss= 7.65783 train/feat_loss= 1.17500\n",
      "Epoch: 0152 train_loss= 2.47163 train/struct_loss= 7.65815 train/feat_loss= 1.17500\n",
      "Epoch: 0153 train_loss= 2.47224 train/struct_loss= 7.66120 train/feat_loss= 1.17500\n",
      "Epoch: 0154 train_loss= 2.47180 train/struct_loss= 7.65901 train/feat_loss= 1.17500\n",
      "Epoch: 0155 train_loss= 2.47174 train/struct_loss= 7.65871 train/feat_loss= 1.17500\n",
      "Epoch: 0156 train_loss= 2.47164 train/struct_loss= 7.65822 train/feat_loss= 1.17500\n",
      "Epoch: 0157 train_loss= 2.47164 train/struct_loss= 7.65820 train/feat_loss= 1.17500\n",
      "Epoch: 0158 train_loss= 2.47194 train/struct_loss= 7.65972 train/feat_loss= 1.17500\n",
      "Epoch: 0159 train_loss= 2.47189 train/struct_loss= 7.65946 train/feat_loss= 1.17500\n",
      "Epoch: 0160 train_loss= 2.47201 train/struct_loss= 7.66008 train/feat_loss= 1.17500\n",
      "Epoch: 0160 Auc 0.8138042921230689\n",
      "Epoch: 0161 train_loss= 2.47174 train/struct_loss= 7.65870 train/feat_loss= 1.17500\n",
      "Epoch: 0162 train_loss= 2.47184 train/struct_loss= 7.65922 train/feat_loss= 1.17500\n",
      "Epoch: 0163 train_loss= 2.47160 train/struct_loss= 7.65799 train/feat_loss= 1.17500\n",
      "Epoch: 0164 train_loss= 2.47195 train/struct_loss= 7.65978 train/feat_loss= 1.17500\n",
      "Epoch: 0165 train_loss= 2.47219 train/struct_loss= 7.66093 train/feat_loss= 1.17500\n",
      "Epoch: 0166 train_loss= 2.47180 train/struct_loss= 7.65902 train/feat_loss= 1.17500\n",
      "Epoch: 0167 train_loss= 2.47140 train/struct_loss= 7.65698 train/feat_loss= 1.17500\n",
      "Epoch: 0168 train_loss= 2.47160 train/struct_loss= 7.65800 train/feat_loss= 1.17500\n",
      "Epoch: 0169 train_loss= 2.47147 train/struct_loss= 7.65733 train/feat_loss= 1.17500\n",
      "Epoch: 0170 train_loss= 2.47167 train/struct_loss= 7.65835 train/feat_loss= 1.17500\n",
      "Epoch: 0170 Auc 0.8138419735763948\n",
      "Epoch: 0171 train_loss= 2.47134 train/struct_loss= 7.65669 train/feat_loss= 1.17500\n",
      "Epoch: 0172 train_loss= 2.47159 train/struct_loss= 7.65795 train/feat_loss= 1.17500\n",
      "Epoch: 0173 train_loss= 2.47161 train/struct_loss= 7.65807 train/feat_loss= 1.17500\n",
      "Epoch: 0174 train_loss= 2.47176 train/struct_loss= 7.65880 train/feat_loss= 1.17500\n",
      "Epoch: 0175 train_loss= 2.47170 train/struct_loss= 7.65853 train/feat_loss= 1.17500\n",
      "Epoch: 0176 train_loss= 2.47137 train/struct_loss= 7.65688 train/feat_loss= 1.17500\n",
      "Epoch: 0177 train_loss= 2.47157 train/struct_loss= 7.65786 train/feat_loss= 1.17500\n",
      "Epoch: 0178 train_loss= 2.47194 train/struct_loss= 7.65968 train/feat_loss= 1.17500\n",
      "Epoch: 0179 train_loss= 2.47204 train/struct_loss= 7.66022 train/feat_loss= 1.17500\n",
      "Epoch: 0180 train_loss= 2.47200 train/struct_loss= 7.65999 train/feat_loss= 1.17500\n",
      "Epoch: 0180 Auc 0.8138495098670598\n",
      "Epoch: 0181 train_loss= 2.47197 train/struct_loss= 7.65988 train/feat_loss= 1.17500\n",
      "Epoch: 0182 train_loss= 2.47172 train/struct_loss= 7.65859 train/feat_loss= 1.17500\n",
      "Epoch: 0183 train_loss= 2.47200 train/struct_loss= 7.65999 train/feat_loss= 1.17500\n",
      "Epoch: 0184 train_loss= 2.47140 train/struct_loss= 7.65702 train/feat_loss= 1.17500\n",
      "Epoch: 0185 train_loss= 2.47176 train/struct_loss= 7.65879 train/feat_loss= 1.17500\n",
      "Epoch: 0186 train_loss= 2.47217 train/struct_loss= 7.66084 train/feat_loss= 1.17500\n",
      "Epoch: 0187 train_loss= 2.47208 train/struct_loss= 7.66040 train/feat_loss= 1.17500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0188 train_loss= 2.47217 train/struct_loss= 7.66084 train/feat_loss= 1.17500\n",
      "Epoch: 0189 train_loss= 2.47180 train/struct_loss= 7.65902 train/feat_loss= 1.17500\n",
      "Epoch: 0190 train_loss= 2.47183 train/struct_loss= 7.65915 train/feat_loss= 1.17500\n",
      "Epoch: 0190 Auc 0.8138597866270577\n",
      "Epoch: 0191 train_loss= 2.47165 train/struct_loss= 7.65827 train/feat_loss= 1.17500\n",
      "Epoch: 0192 train_loss= 2.47161 train/struct_loss= 7.65805 train/feat_loss= 1.17500\n",
      "Epoch: 0193 train_loss= 2.47175 train/struct_loss= 7.65877 train/feat_loss= 1.17500\n",
      "Epoch: 0194 train_loss= 2.47164 train/struct_loss= 7.65820 train/feat_loss= 1.17500\n",
      "Epoch: 0195 train_loss= 2.47195 train/struct_loss= 7.65976 train/feat_loss= 1.17500\n",
      "Epoch: 0196 train_loss= 2.47151 train/struct_loss= 7.65757 train/feat_loss= 1.17500\n",
      "Epoch: 0197 train_loss= 2.47179 train/struct_loss= 7.65893 train/feat_loss= 1.17500\n",
      "Epoch: 0198 train_loss= 2.47197 train/struct_loss= 7.65988 train/feat_loss= 1.17500\n",
      "Epoch: 0199 train_loss= 2.47204 train/struct_loss= 7.66020 train/feat_loss= 1.17500\n",
      "Epoch: 0200 train_loss= 2.47166 train/struct_loss= 7.65830 train/feat_loss= 1.17500\n",
      "Epoch: 0200 Auc 0.8138474545150602\n",
      "Epoch: 0201 train_loss= 2.47179 train/struct_loss= 7.65894 train/feat_loss= 1.17500\n",
      "Epoch: 0202 train_loss= 2.47176 train/struct_loss= 7.65881 train/feat_loss= 1.17500\n",
      "Epoch: 0203 train_loss= 2.47172 train/struct_loss= 7.65863 train/feat_loss= 1.17500\n",
      "Epoch: 0204 train_loss= 2.47155 train/struct_loss= 7.65775 train/feat_loss= 1.17500\n",
      "Epoch: 0205 train_loss= 2.47187 train/struct_loss= 7.65937 train/feat_loss= 1.17500\n",
      "Epoch: 0206 train_loss= 2.47173 train/struct_loss= 7.65866 train/feat_loss= 1.17500\n",
      "Epoch: 0207 train_loss= 2.47171 train/struct_loss= 7.65854 train/feat_loss= 1.17500\n",
      "Epoch: 0208 train_loss= 2.47168 train/struct_loss= 7.65841 train/feat_loss= 1.17500\n",
      "Epoch: 0209 train_loss= 2.47183 train/struct_loss= 7.65914 train/feat_loss= 1.17500\n",
      "Epoch: 0210 train_loss= 2.47229 train/struct_loss= 7.66145 train/feat_loss= 1.17500\n",
      "Epoch: 0210 Auc 0.8138481396323934\n",
      "Epoch: 0211 train_loss= 2.47158 train/struct_loss= 7.65792 train/feat_loss= 1.17500\n",
      "Epoch: 0212 train_loss= 2.47162 train/struct_loss= 7.65812 train/feat_loss= 1.17500\n",
      "Epoch: 0213 train_loss= 2.47176 train/struct_loss= 7.65880 train/feat_loss= 1.17500\n",
      "Epoch: 0214 train_loss= 2.47137 train/struct_loss= 7.65686 train/feat_loss= 1.17500\n",
      "Epoch: 0215 train_loss= 2.47168 train/struct_loss= 7.65843 train/feat_loss= 1.17500\n",
      "Epoch: 0216 train_loss= 2.47192 train/struct_loss= 7.65961 train/feat_loss= 1.17500\n",
      "Epoch: 0217 train_loss= 2.47134 train/struct_loss= 7.65673 train/feat_loss= 1.17500\n",
      "Epoch: 0218 train_loss= 2.47161 train/struct_loss= 7.65808 train/feat_loss= 1.17500\n",
      "Epoch: 0219 train_loss= 2.47205 train/struct_loss= 7.66028 train/feat_loss= 1.17500\n",
      "Epoch: 0220 train_loss= 2.47143 train/struct_loss= 7.65715 train/feat_loss= 1.17500\n",
      "Epoch: 0220 Auc 0.8138179944697328\n",
      "Epoch: 0221 train_loss= 2.47172 train/struct_loss= 7.65859 train/feat_loss= 1.17500\n",
      "Epoch: 0222 train_loss= 2.47212 train/struct_loss= 7.66058 train/feat_loss= 1.17500\n",
      "Epoch: 0223 train_loss= 2.47192 train/struct_loss= 7.65959 train/feat_loss= 1.17500\n",
      "Epoch: 0224 train_loss= 2.47170 train/struct_loss= 7.65852 train/feat_loss= 1.17500\n",
      "Epoch: 0225 train_loss= 2.47157 train/struct_loss= 7.65785 train/feat_loss= 1.17500\n",
      "Epoch: 0226 train_loss= 2.47136 train/struct_loss= 7.65681 train/feat_loss= 1.17500\n",
      "Epoch: 0227 train_loss= 2.47196 train/struct_loss= 7.65979 train/feat_loss= 1.17500\n",
      "Epoch: 0228 train_loss= 2.47166 train/struct_loss= 7.65829 train/feat_loss= 1.17500\n",
      "Epoch: 0229 train_loss= 2.47179 train/struct_loss= 7.65895 train/feat_loss= 1.17500\n",
      "Epoch: 0230 train_loss= 2.47156 train/struct_loss= 7.65781 train/feat_loss= 1.17500\n",
      "Epoch: 0230 Auc 0.8138316968163968\n",
      "Epoch: 0231 train_loss= 2.47176 train/struct_loss= 7.65880 train/feat_loss= 1.17500\n",
      "Epoch: 0232 train_loss= 2.47185 train/struct_loss= 7.65926 train/feat_loss= 1.17500\n",
      "Epoch: 0233 train_loss= 2.47153 train/struct_loss= 7.65768 train/feat_loss= 1.17500\n",
      "Epoch: 0234 train_loss= 2.47188 train/struct_loss= 7.65942 train/feat_loss= 1.17500\n",
      "Epoch: 0235 train_loss= 2.47174 train/struct_loss= 7.65872 train/feat_loss= 1.17500\n",
      "Epoch: 0236 train_loss= 2.47151 train/struct_loss= 7.65758 train/feat_loss= 1.17500\n",
      "Epoch: 0237 train_loss= 2.47140 train/struct_loss= 7.65699 train/feat_loss= 1.17500\n",
      "Epoch: 0238 train_loss= 2.47148 train/struct_loss= 7.65742 train/feat_loss= 1.17500\n",
      "Epoch: 0239 train_loss= 2.47158 train/struct_loss= 7.65791 train/feat_loss= 1.17500\n",
      "Epoch: 0240 train_loss= 2.47173 train/struct_loss= 7.65863 train/feat_loss= 1.17500\n",
      "Epoch: 0240 Auc 0.8137977835084036\n",
      "Epoch: 0241 train_loss= 2.47196 train/struct_loss= 7.65979 train/feat_loss= 1.17500\n",
      "Epoch: 0242 train_loss= 2.47159 train/struct_loss= 7.65796 train/feat_loss= 1.17500\n",
      "Epoch: 0243 train_loss= 2.47127 train/struct_loss= 7.65635 train/feat_loss= 1.17500\n",
      "Epoch: 0244 train_loss= 2.47139 train/struct_loss= 7.65695 train/feat_loss= 1.17500\n",
      "Epoch: 0245 train_loss= 2.47171 train/struct_loss= 7.65855 train/feat_loss= 1.17500\n",
      "Epoch: 0246 train_loss= 2.47149 train/struct_loss= 7.65745 train/feat_loss= 1.17500\n",
      "Epoch: 0247 train_loss= 2.47207 train/struct_loss= 7.66035 train/feat_loss= 1.17500\n",
      "Epoch: 0248 train_loss= 2.47178 train/struct_loss= 7.65892 train/feat_loss= 1.17500\n",
      "Epoch: 0249 train_loss= 2.47155 train/struct_loss= 7.65777 train/feat_loss= 1.17500\n",
      "Epoch: 0250 train_loss= 2.47182 train/struct_loss= 7.65910 train/feat_loss= 1.17500\n",
      "Epoch: 0250 Auc 0.8138474545150601\n",
      "Epoch: 0251 train_loss= 2.47157 train/struct_loss= 7.65787 train/feat_loss= 1.17500\n",
      "Epoch: 0252 train_loss= 2.47187 train/struct_loss= 7.65938 train/feat_loss= 1.17500\n",
      "Epoch: 0253 train_loss= 2.47228 train/struct_loss= 7.66139 train/feat_loss= 1.17500\n",
      "Epoch: 0254 train_loss= 2.47205 train/struct_loss= 7.66025 train/feat_loss= 1.17500\n",
      "Epoch: 0255 train_loss= 2.47152 train/struct_loss= 7.65760 train/feat_loss= 1.17500\n",
      "Epoch: 0256 train_loss= 2.47156 train/struct_loss= 7.65781 train/feat_loss= 1.17500\n",
      "Epoch: 0257 train_loss= 2.47168 train/struct_loss= 7.65842 train/feat_loss= 1.17500\n",
      "Epoch: 0258 train_loss= 2.47209 train/struct_loss= 7.66044 train/feat_loss= 1.17500\n",
      "Epoch: 0259 train_loss= 2.47199 train/struct_loss= 7.65997 train/feat_loss= 1.17500\n",
      "Epoch: 0260 train_loss= 2.47177 train/struct_loss= 7.65886 train/feat_loss= 1.17500\n",
      "Epoch: 0260 Auc 0.8139330941817096\n",
      "Epoch: 0261 train_loss= 2.47224 train/struct_loss= 7.66119 train/feat_loss= 1.17500\n",
      "Epoch: 0262 train_loss= 2.47239 train/struct_loss= 7.66195 train/feat_loss= 1.17500\n",
      "Epoch: 0263 train_loss= 2.47204 train/struct_loss= 7.66020 train/feat_loss= 1.17500\n",
      "Epoch: 0264 train_loss= 2.47325 train/struct_loss= 7.66626 train/feat_loss= 1.17500\n",
      "Epoch: 0265 train_loss= 2.47290 train/struct_loss= 7.66449 train/feat_loss= 1.17500\n",
      "Epoch: 0266 train_loss= 2.47266 train/struct_loss= 7.66330 train/feat_loss= 1.17500\n",
      "Epoch: 0267 train_loss= 2.47302 train/struct_loss= 7.66508 train/feat_loss= 1.17500\n",
      "Epoch: 0268 train_loss= 2.47149 train/struct_loss= 7.65747 train/feat_loss= 1.17500\n",
      "Epoch: 0269 train_loss= 2.47268 train/struct_loss= 7.66339 train/feat_loss= 1.17500\n",
      "Epoch: 0270 train_loss= 2.47166 train/struct_loss= 7.65830 train/feat_loss= 1.17500\n",
      "Epoch: 0270 Auc 0.8139303537123769\n",
      "Epoch: 0271 train_loss= 2.47272 train/struct_loss= 7.66362 train/feat_loss= 1.17500\n",
      "Epoch: 0272 train_loss= 2.47212 train/struct_loss= 7.66060 train/feat_loss= 1.17500\n",
      "Epoch: 0273 train_loss= 2.47198 train/struct_loss= 7.65991 train/feat_loss= 1.17500\n",
      "Epoch: 0274 train_loss= 2.47192 train/struct_loss= 7.65960 train/feat_loss= 1.17500\n",
      "Epoch: 0275 train_loss= 2.47192 train/struct_loss= 7.65962 train/feat_loss= 1.17500\n",
      "Epoch: 0276 train_loss= 2.47173 train/struct_loss= 7.65866 train/feat_loss= 1.17500\n",
      "Epoch: 0277 train_loss= 2.47164 train/struct_loss= 7.65820 train/feat_loss= 1.17500\n",
      "Epoch: 0278 train_loss= 2.47187 train/struct_loss= 7.65938 train/feat_loss= 1.17500\n",
      "Epoch: 0279 train_loss= 2.47196 train/struct_loss= 7.65982 train/feat_loss= 1.17500\n",
      "Epoch: 0280 train_loss= 2.47195 train/struct_loss= 7.65973 train/feat_loss= 1.17500\n",
      "Epoch: 0280 Auc 0.8138940424937176\n",
      "Epoch: 0281 train_loss= 2.47214 train/struct_loss= 7.66069 train/feat_loss= 1.17500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0282 train_loss= 2.47213 train/struct_loss= 7.66068 train/feat_loss= 1.17500\n",
      "Epoch: 0283 train_loss= 2.47224 train/struct_loss= 7.66119 train/feat_loss= 1.17500\n",
      "Epoch: 0284 train_loss= 2.47227 train/struct_loss= 7.66137 train/feat_loss= 1.17500\n",
      "Epoch: 0285 train_loss= 2.47183 train/struct_loss= 7.65918 train/feat_loss= 1.17500\n",
      "Epoch: 0286 train_loss= 2.47224 train/struct_loss= 7.66123 train/feat_loss= 1.17500\n",
      "Epoch: 0287 train_loss= 2.47175 train/struct_loss= 7.65877 train/feat_loss= 1.17500\n",
      "Epoch: 0288 train_loss= 2.47148 train/struct_loss= 7.65742 train/feat_loss= 1.17500\n",
      "Epoch: 0289 train_loss= 2.47158 train/struct_loss= 7.65793 train/feat_loss= 1.17500\n",
      "Epoch: 0290 train_loss= 2.47206 train/struct_loss= 7.66031 train/feat_loss= 1.17500\n",
      "Epoch: 0290 Auc 0.8138412884590616\n",
      "Epoch: 0291 train_loss= 2.47155 train/struct_loss= 7.65777 train/feat_loss= 1.17500\n",
      "Epoch: 0292 train_loss= 2.47176 train/struct_loss= 7.65882 train/feat_loss= 1.17500\n",
      "Epoch: 0293 train_loss= 2.47141 train/struct_loss= 7.65706 train/feat_loss= 1.17500\n",
      "Epoch: 0294 train_loss= 2.47180 train/struct_loss= 7.65900 train/feat_loss= 1.17500\n",
      "Epoch: 0295 train_loss= 2.47249 train/struct_loss= 7.66248 train/feat_loss= 1.17500\n",
      "Epoch: 0296 train_loss= 2.47191 train/struct_loss= 7.65958 train/feat_loss= 1.17500\n",
      "Epoch: 0297 train_loss= 2.47155 train/struct_loss= 7.65778 train/feat_loss= 1.17500\n",
      "Epoch: 0298 train_loss= 2.47157 train/struct_loss= 7.65788 train/feat_loss= 1.17500\n",
      "Epoch: 0299 train_loss= 2.47163 train/struct_loss= 7.65814 train/feat_loss= 1.17500\n",
      "Epoch: 0300 train_loss= 2.47178 train/struct_loss= 7.65891 train/feat_loss= 1.17500\n",
      "Epoch: 0300 Auc 0.8138563610403917\n",
      "Epoch: 0301 train_loss= 2.47174 train/struct_loss= 7.65870 train/feat_loss= 1.17500\n",
      "Epoch: 0302 train_loss= 2.47210 train/struct_loss= 7.66052 train/feat_loss= 1.17500\n",
      "Epoch: 0303 train_loss= 2.47120 train/struct_loss= 7.65602 train/feat_loss= 1.17500\n",
      "Epoch: 0304 train_loss= 2.47175 train/struct_loss= 7.65874 train/feat_loss= 1.17500\n",
      "Epoch: 0305 train_loss= 2.47188 train/struct_loss= 7.65941 train/feat_loss= 1.17500\n",
      "Epoch: 0306 train_loss= 2.47183 train/struct_loss= 7.65915 train/feat_loss= 1.17500\n",
      "Epoch: 0307 train_loss= 2.47141 train/struct_loss= 7.65708 train/feat_loss= 1.17500\n",
      "Epoch: 0308 train_loss= 2.47204 train/struct_loss= 7.66020 train/feat_loss= 1.17500\n",
      "Epoch: 0309 train_loss= 2.47184 train/struct_loss= 7.65923 train/feat_loss= 1.17500\n",
      "Epoch: 0310 train_loss= 2.47162 train/struct_loss= 7.65810 train/feat_loss= 1.17500\n",
      "Epoch: 0310 Auc 0.8138344372857296\n",
      "Epoch: 0311 train_loss= 2.47190 train/struct_loss= 7.65950 train/feat_loss= 1.17500\n",
      "Epoch: 0312 train_loss= 2.47118 train/struct_loss= 7.65593 train/feat_loss= 1.17500\n",
      "Epoch: 0313 train_loss= 2.47100 train/struct_loss= 7.65502 train/feat_loss= 1.17500\n",
      "Epoch: 0314 train_loss= 2.47200 train/struct_loss= 7.65999 train/feat_loss= 1.17500\n",
      "Epoch: 0315 train_loss= 2.47167 train/struct_loss= 7.65834 train/feat_loss= 1.17500\n",
      "Epoch: 0316 train_loss= 2.47141 train/struct_loss= 7.65707 train/feat_loss= 1.17500\n",
      "Epoch: 0317 train_loss= 2.47162 train/struct_loss= 7.65811 train/feat_loss= 1.17500\n",
      "Epoch: 0318 train_loss= 2.47156 train/struct_loss= 7.65781 train/feat_loss= 1.17500\n",
      "Epoch: 0319 train_loss= 2.47209 train/struct_loss= 7.66046 train/feat_loss= 1.17500\n",
      "Epoch: 0320 train_loss= 2.47150 train/struct_loss= 7.65749 train/feat_loss= 1.17500\n",
      "Epoch: 0320 Auc 0.8138474545150602\n",
      "Epoch: 0321 train_loss= 2.47183 train/struct_loss= 7.65914 train/feat_loss= 1.17500\n",
      "Epoch: 0322 train_loss= 2.47180 train/struct_loss= 7.65901 train/feat_loss= 1.17500\n",
      "Epoch: 0323 train_loss= 2.47195 train/struct_loss= 7.65976 train/feat_loss= 1.17500\n",
      "Epoch: 0324 train_loss= 2.47198 train/struct_loss= 7.65993 train/feat_loss= 1.17500\n",
      "Epoch: 0325 train_loss= 2.47134 train/struct_loss= 7.65670 train/feat_loss= 1.17500\n",
      "Epoch: 0326 train_loss= 2.47172 train/struct_loss= 7.65859 train/feat_loss= 1.17500\n",
      "Epoch: 0327 train_loss= 2.47157 train/struct_loss= 7.65784 train/feat_loss= 1.17500\n",
      "Epoch: 0328 train_loss= 2.47148 train/struct_loss= 7.65743 train/feat_loss= 1.17500\n",
      "Epoch: 0329 train_loss= 2.47183 train/struct_loss= 7.65914 train/feat_loss= 1.17500\n",
      "Epoch: 0330 train_loss= 2.47158 train/struct_loss= 7.65792 train/feat_loss= 1.17500\n",
      "Epoch: 0330 Auc 0.8138495098670598\n",
      "Epoch: 0331 train_loss= 2.47177 train/struct_loss= 7.65885 train/feat_loss= 1.17500\n",
      "Epoch: 0332 train_loss= 2.47162 train/struct_loss= 7.65811 train/feat_loss= 1.17500\n",
      "Epoch: 0333 train_loss= 2.47160 train/struct_loss= 7.65801 train/feat_loss= 1.17500\n",
      "Epoch: 0334 train_loss= 2.47227 train/struct_loss= 7.66134 train/feat_loss= 1.17500\n",
      "Epoch: 0335 train_loss= 2.47208 train/struct_loss= 7.66043 train/feat_loss= 1.17500\n",
      "Epoch: 0336 train_loss= 2.47171 train/struct_loss= 7.65856 train/feat_loss= 1.17500\n",
      "Epoch: 0337 train_loss= 2.47192 train/struct_loss= 7.65962 train/feat_loss= 1.17500\n",
      "Epoch: 0338 train_loss= 2.47131 train/struct_loss= 7.65657 train/feat_loss= 1.17500\n",
      "Epoch: 0339 train_loss= 2.47190 train/struct_loss= 7.65953 train/feat_loss= 1.17500\n",
      "Epoch: 0340 train_loss= 2.47134 train/struct_loss= 7.65672 train/feat_loss= 1.17500\n",
      "Epoch: 0340 Auc 0.813808402827068\n",
      "Epoch: 0341 train_loss= 2.47209 train/struct_loss= 7.66046 train/feat_loss= 1.17500\n",
      "Epoch: 0342 train_loss= 2.47157 train/struct_loss= 7.65785 train/feat_loss= 1.17500\n",
      "Epoch: 0343 train_loss= 2.47131 train/struct_loss= 7.65657 train/feat_loss= 1.17500\n",
      "Epoch: 0344 train_loss= 2.47212 train/struct_loss= 7.66060 train/feat_loss= 1.17500\n",
      "Epoch: 0345 train_loss= 2.47183 train/struct_loss= 7.65916 train/feat_loss= 1.17500\n",
      "Epoch: 0346 train_loss= 2.47212 train/struct_loss= 7.66059 train/feat_loss= 1.17500\n",
      "Epoch: 0347 train_loss= 2.47162 train/struct_loss= 7.65810 train/feat_loss= 1.17500\n",
      "Epoch: 0348 train_loss= 2.47173 train/struct_loss= 7.65865 train/feat_loss= 1.17500\n",
      "Epoch: 0349 train_loss= 2.47149 train/struct_loss= 7.65747 train/feat_loss= 1.17500\n",
      "Epoch: 0350 train_loss= 2.47226 train/struct_loss= 7.66131 train/feat_loss= 1.17500\n",
      "Epoch: 0350 Auc 0.8138597866270576\n",
      "Epoch: 0351 train_loss= 2.47139 train/struct_loss= 7.65696 train/feat_loss= 1.17500\n",
      "Epoch: 0352 train_loss= 2.47164 train/struct_loss= 7.65819 train/feat_loss= 1.17500\n",
      "Epoch: 0353 train_loss= 2.47193 train/struct_loss= 7.65966 train/feat_loss= 1.17500\n",
      "Epoch: 0354 train_loss= 2.47194 train/struct_loss= 7.65969 train/feat_loss= 1.17500\n",
      "Epoch: 0355 train_loss= 2.47169 train/struct_loss= 7.65848 train/feat_loss= 1.17500\n",
      "Epoch: 0356 train_loss= 2.47147 train/struct_loss= 7.65737 train/feat_loss= 1.17500\n",
      "Epoch: 0357 train_loss= 2.47184 train/struct_loss= 7.65922 train/feat_loss= 1.17500\n",
      "Epoch: 0358 train_loss= 2.47140 train/struct_loss= 7.65701 train/feat_loss= 1.17500\n",
      "Epoch: 0359 train_loss= 2.47142 train/struct_loss= 7.65712 train/feat_loss= 1.17500\n",
      "Epoch: 0360 train_loss= 2.47130 train/struct_loss= 7.65648 train/feat_loss= 1.17500\n",
      "Epoch: 0360 Auc 0.813852935453726\n",
      "Epoch: 0361 train_loss= 2.47154 train/struct_loss= 7.65769 train/feat_loss= 1.17500\n",
      "Epoch: 0362 train_loss= 2.47171 train/struct_loss= 7.65856 train/feat_loss= 1.17500\n",
      "Epoch: 0363 train_loss= 2.47196 train/struct_loss= 7.65982 train/feat_loss= 1.17500\n",
      "Epoch: 0364 train_loss= 2.47165 train/struct_loss= 7.65825 train/feat_loss= 1.17500\n",
      "Epoch: 0365 train_loss= 2.47157 train/struct_loss= 7.65785 train/feat_loss= 1.17500\n",
      "Epoch: 0366 train_loss= 2.47224 train/struct_loss= 7.66121 train/feat_loss= 1.17500\n",
      "Epoch: 0367 train_loss= 2.47142 train/struct_loss= 7.65713 train/feat_loss= 1.17500\n",
      "Epoch: 0368 train_loss= 2.47158 train/struct_loss= 7.65792 train/feat_loss= 1.17500\n",
      "Epoch: 0369 train_loss= 2.47156 train/struct_loss= 7.65783 train/feat_loss= 1.17500\n",
      "Epoch: 0370 train_loss= 2.47210 train/struct_loss= 7.66052 train/feat_loss= 1.17500\n",
      "Epoch: 0370 Auc 0.8138597866270578\n",
      "Epoch: 0371 train_loss= 2.47173 train/struct_loss= 7.65866 train/feat_loss= 1.17500\n",
      "Epoch: 0372 train_loss= 2.47186 train/struct_loss= 7.65930 train/feat_loss= 1.17500\n",
      "Epoch: 0373 train_loss= 2.47159 train/struct_loss= 7.65796 train/feat_loss= 1.17500\n",
      "Epoch: 0374 train_loss= 2.47191 train/struct_loss= 7.65956 train/feat_loss= 1.17500\n",
      "Epoch: 0375 train_loss= 2.47123 train/struct_loss= 7.65615 train/feat_loss= 1.17500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0376 train_loss= 2.47150 train/struct_loss= 7.65752 train/feat_loss= 1.17500\n",
      "Epoch: 0377 train_loss= 2.47175 train/struct_loss= 7.65876 train/feat_loss= 1.17500\n",
      "Epoch: 0378 train_loss= 2.47133 train/struct_loss= 7.65664 train/feat_loss= 1.17500\n",
      "Epoch: 0379 train_loss= 2.47205 train/struct_loss= 7.66025 train/feat_loss= 1.17500\n",
      "Epoch: 0380 train_loss= 2.47189 train/struct_loss= 7.65947 train/feat_loss= 1.17500\n",
      "Epoch: 0380 Auc 0.8138522503363925\n",
      "Epoch: 0381 train_loss= 2.47227 train/struct_loss= 7.66134 train/feat_loss= 1.17500\n",
      "Epoch: 0382 train_loss= 2.47151 train/struct_loss= 7.65757 train/feat_loss= 1.17500\n",
      "Epoch: 0383 train_loss= 2.47147 train/struct_loss= 7.65736 train/feat_loss= 1.17500\n",
      "Epoch: 0384 train_loss= 2.47191 train/struct_loss= 7.65957 train/feat_loss= 1.17500\n",
      "Epoch: 0385 train_loss= 2.47161 train/struct_loss= 7.65805 train/feat_loss= 1.17500\n",
      "Epoch: 0386 train_loss= 2.47198 train/struct_loss= 7.65989 train/feat_loss= 1.17500\n",
      "Epoch: 0387 train_loss= 2.47192 train/struct_loss= 7.65959 train/feat_loss= 1.17500\n",
      "Epoch: 0388 train_loss= 2.47154 train/struct_loss= 7.65772 train/feat_loss= 1.17500\n",
      "Epoch: 0389 train_loss= 2.47149 train/struct_loss= 7.65745 train/feat_loss= 1.17500\n",
      "Epoch: 0390 train_loss= 2.47149 train/struct_loss= 7.65746 train/feat_loss= 1.17500\n",
      "Epoch: 0390 Auc 0.8138447140457274\n",
      "Epoch: 0391 train_loss= 2.47180 train/struct_loss= 7.65901 train/feat_loss= 1.17500\n",
      "Epoch: 0392 train_loss= 2.47159 train/struct_loss= 7.65794 train/feat_loss= 1.17500\n",
      "Epoch: 0393 train_loss= 2.47185 train/struct_loss= 7.65924 train/feat_loss= 1.17500\n",
      "Epoch: 0394 train_loss= 2.47169 train/struct_loss= 7.65847 train/feat_loss= 1.17500\n",
      "Epoch: 0395 train_loss= 2.47225 train/struct_loss= 7.66126 train/feat_loss= 1.17500\n",
      "Epoch: 0396 train_loss= 2.47183 train/struct_loss= 7.65916 train/feat_loss= 1.17500\n",
      "Epoch: 0397 train_loss= 2.47137 train/struct_loss= 7.65686 train/feat_loss= 1.17500\n",
      "Epoch: 0398 train_loss= 2.47155 train/struct_loss= 7.65776 train/feat_loss= 1.17500\n",
      "Epoch: 0399 train_loss= 2.47163 train/struct_loss= 7.65816 train/feat_loss= 1.17500\n",
      "Epoch: 0400 train_loss= 2.47169 train/struct_loss= 7.65844 train/feat_loss= 1.17500\n",
      "Epoch: 0400 Auc 0.8138227902910653\n",
      "Epoch: 0401 train_loss= 2.47211 train/struct_loss= 7.66054 train/feat_loss= 1.17500\n",
      "Epoch: 0402 train_loss= 2.47172 train/struct_loss= 7.65860 train/feat_loss= 1.17500\n",
      "Epoch: 0403 train_loss= 2.47178 train/struct_loss= 7.65889 train/feat_loss= 1.17500\n",
      "Epoch: 0404 train_loss= 2.47182 train/struct_loss= 7.65913 train/feat_loss= 1.17500\n",
      "Epoch: 0405 train_loss= 2.47160 train/struct_loss= 7.65801 train/feat_loss= 1.17500\n",
      "Epoch: 0406 train_loss= 2.47199 train/struct_loss= 7.65994 train/feat_loss= 1.17500\n",
      "Epoch: 0407 train_loss= 2.47189 train/struct_loss= 7.65944 train/feat_loss= 1.17500\n",
      "Epoch: 0408 train_loss= 2.47188 train/struct_loss= 7.65939 train/feat_loss= 1.17500\n",
      "Epoch: 0409 train_loss= 2.47206 train/struct_loss= 7.66032 train/feat_loss= 1.17500\n",
      "Epoch: 0410 train_loss= 2.47141 train/struct_loss= 7.65704 train/feat_loss= 1.17500\n",
      "Epoch: 0410 Auc 0.8138227902910653\n",
      "Epoch: 0411 train_loss= 2.47168 train/struct_loss= 7.65841 train/feat_loss= 1.17500\n",
      "Epoch: 0412 train_loss= 2.47200 train/struct_loss= 7.66003 train/feat_loss= 1.17500\n",
      "Epoch: 0413 train_loss= 2.47193 train/struct_loss= 7.65968 train/feat_loss= 1.17500\n",
      "Epoch: 0414 train_loss= 2.47149 train/struct_loss= 7.65746 train/feat_loss= 1.17500\n",
      "Epoch: 0415 train_loss= 2.47222 train/struct_loss= 7.66112 train/feat_loss= 1.17500\n",
      "Epoch: 0416 train_loss= 2.47181 train/struct_loss= 7.65907 train/feat_loss= 1.17500\n",
      "Epoch: 0417 train_loss= 2.47169 train/struct_loss= 7.65845 train/feat_loss= 1.17500\n",
      "Epoch: 0418 train_loss= 2.47149 train/struct_loss= 7.65747 train/feat_loss= 1.17500\n",
      "Epoch: 0419 train_loss= 2.47181 train/struct_loss= 7.65907 train/feat_loss= 1.17500\n",
      "Epoch: 0420 train_loss= 2.47168 train/struct_loss= 7.65843 train/feat_loss= 1.17500\n",
      "Epoch: 0420 Auc 0.8138015516537362\n",
      "Epoch: 0421 train_loss= 2.47209 train/struct_loss= 7.66046 train/feat_loss= 1.17500\n",
      "Epoch: 0422 train_loss= 2.47148 train/struct_loss= 7.65743 train/feat_loss= 1.17500\n",
      "Epoch: 0423 train_loss= 2.47202 train/struct_loss= 7.66008 train/feat_loss= 1.17500\n",
      "Epoch: 0424 train_loss= 2.47170 train/struct_loss= 7.65850 train/feat_loss= 1.17500\n",
      "Epoch: 0425 train_loss= 2.47201 train/struct_loss= 7.66005 train/feat_loss= 1.17500\n",
      "Epoch: 0426 train_loss= 2.47156 train/struct_loss= 7.65779 train/feat_loss= 1.17500\n",
      "Epoch: 0427 train_loss= 2.47166 train/struct_loss= 7.65832 train/feat_loss= 1.17500\n",
      "Epoch: 0428 train_loss= 2.47188 train/struct_loss= 7.65943 train/feat_loss= 1.17500\n",
      "Epoch: 0429 train_loss= 2.47143 train/struct_loss= 7.65714 train/feat_loss= 1.17500\n",
      "Epoch: 0430 train_loss= 2.47148 train/struct_loss= 7.65742 train/feat_loss= 1.17500\n",
      "Epoch: 0430 Auc 0.8138104581790677\n",
      "Epoch: 0431 train_loss= 2.47187 train/struct_loss= 7.65937 train/feat_loss= 1.17500\n",
      "Epoch: 0432 train_loss= 2.47129 train/struct_loss= 7.65648 train/feat_loss= 1.17500\n",
      "Epoch: 0433 train_loss= 2.47170 train/struct_loss= 7.65852 train/feat_loss= 1.17500\n",
      "Epoch: 0434 train_loss= 2.47187 train/struct_loss= 7.65934 train/feat_loss= 1.17500\n",
      "Epoch: 0435 train_loss= 2.47186 train/struct_loss= 7.65932 train/feat_loss= 1.17500\n",
      "Epoch: 0436 train_loss= 2.47137 train/struct_loss= 7.65688 train/feat_loss= 1.17500\n",
      "Epoch: 0437 train_loss= 2.47219 train/struct_loss= 7.66096 train/feat_loss= 1.17500\n",
      "Epoch: 0438 train_loss= 2.47152 train/struct_loss= 7.65763 train/feat_loss= 1.17500\n",
      "Epoch: 0439 train_loss= 2.47182 train/struct_loss= 7.65913 train/feat_loss= 1.17500\n",
      "Epoch: 0440 train_loss= 2.47205 train/struct_loss= 7.66028 train/feat_loss= 1.17500\n",
      "Epoch: 0440 Auc 0.8138176519110664\n",
      "Epoch: 0441 train_loss= 2.47197 train/struct_loss= 7.65984 train/feat_loss= 1.17500\n",
      "Epoch: 0442 train_loss= 2.47211 train/struct_loss= 7.66054 train/feat_loss= 1.17500\n",
      "Epoch: 0443 train_loss= 2.47147 train/struct_loss= 7.65737 train/feat_loss= 1.17500\n",
      "Epoch: 0444 train_loss= 2.47173 train/struct_loss= 7.65864 train/feat_loss= 1.17500\n",
      "Epoch: 0445 train_loss= 2.47157 train/struct_loss= 7.65783 train/feat_loss= 1.17500\n",
      "Epoch: 0446 train_loss= 2.47160 train/struct_loss= 7.65800 train/feat_loss= 1.17500\n",
      "Epoch: 0447 train_loss= 2.47126 train/struct_loss= 7.65631 train/feat_loss= 1.17500\n",
      "Epoch: 0448 train_loss= 2.47210 train/struct_loss= 7.66052 train/feat_loss= 1.17500\n",
      "Epoch: 0449 train_loss= 2.47173 train/struct_loss= 7.65864 train/feat_loss= 1.17500\n",
      "Epoch: 0450 train_loss= 2.47167 train/struct_loss= 7.65837 train/feat_loss= 1.17500\n",
      "Epoch: 0450 Auc 0.8138597866270578\n",
      "Epoch: 0451 train_loss= 2.47195 train/struct_loss= 7.65975 train/feat_loss= 1.17500\n",
      "Epoch: 0452 train_loss= 2.47174 train/struct_loss= 7.65869 train/feat_loss= 1.17500\n",
      "Epoch: 0453 train_loss= 2.47183 train/struct_loss= 7.65916 train/feat_loss= 1.17500\n",
      "Epoch: 0454 train_loss= 2.47179 train/struct_loss= 7.65896 train/feat_loss= 1.17500\n",
      "Epoch: 0455 train_loss= 2.47226 train/struct_loss= 7.66131 train/feat_loss= 1.17500\n",
      "Epoch: 0456 train_loss= 2.47213 train/struct_loss= 7.66067 train/feat_loss= 1.17500\n",
      "Epoch: 0457 train_loss= 2.47128 train/struct_loss= 7.65640 train/feat_loss= 1.17500\n",
      "Epoch: 0458 train_loss= 2.47180 train/struct_loss= 7.65899 train/feat_loss= 1.17500\n",
      "Epoch: 0459 train_loss= 2.47176 train/struct_loss= 7.65882 train/feat_loss= 1.17500\n",
      "Epoch: 0460 train_loss= 2.47162 train/struct_loss= 7.65811 train/feat_loss= 1.17500\n",
      "Epoch: 0460 Auc 0.8138604717443909\n",
      "Epoch: 0461 train_loss= 2.47188 train/struct_loss= 7.65940 train/feat_loss= 1.17500\n",
      "Epoch: 0462 train_loss= 2.47132 train/struct_loss= 7.65663 train/feat_loss= 1.17500\n",
      "Epoch: 0463 train_loss= 2.47163 train/struct_loss= 7.65817 train/feat_loss= 1.17500\n",
      "Epoch: 0464 train_loss= 2.47146 train/struct_loss= 7.65732 train/feat_loss= 1.17500\n",
      "Epoch: 0465 train_loss= 2.47170 train/struct_loss= 7.65853 train/feat_loss= 1.17500\n",
      "Epoch: 0466 train_loss= 2.47168 train/struct_loss= 7.65842 train/feat_loss= 1.17500\n",
      "Epoch: 0467 train_loss= 2.47170 train/struct_loss= 7.65851 train/feat_loss= 1.17500\n",
      "Epoch: 0468 train_loss= 2.47132 train/struct_loss= 7.65663 train/feat_loss= 1.17500\n",
      "Epoch: 0469 train_loss= 2.47173 train/struct_loss= 7.65866 train/feat_loss= 1.17500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0470 train_loss= 2.47165 train/struct_loss= 7.65825 train/feat_loss= 1.17500\n",
      "Epoch: 0470 Auc 0.81378305348574\n",
      "Epoch: 0471 train_loss= 2.47162 train/struct_loss= 7.65810 train/feat_loss= 1.17500\n",
      "Epoch: 0472 train_loss= 2.47139 train/struct_loss= 7.65695 train/feat_loss= 1.17500\n",
      "Epoch: 0473 train_loss= 2.47203 train/struct_loss= 7.66014 train/feat_loss= 1.17500\n",
      "Epoch: 0474 train_loss= 2.47172 train/struct_loss= 7.65860 train/feat_loss= 1.17500\n",
      "Epoch: 0475 train_loss= 2.47186 train/struct_loss= 7.65932 train/feat_loss= 1.17500\n",
      "Epoch: 0476 train_loss= 2.47227 train/struct_loss= 7.66135 train/feat_loss= 1.17500\n",
      "Epoch: 0477 train_loss= 2.47215 train/struct_loss= 7.66078 train/feat_loss= 1.17500\n",
      "Epoch: 0478 train_loss= 2.47213 train/struct_loss= 7.66068 train/feat_loss= 1.17500\n",
      "Epoch: 0479 train_loss= 2.47181 train/struct_loss= 7.65906 train/feat_loss= 1.17500\n",
      "Epoch: 0480 train_loss= 2.47212 train/struct_loss= 7.66062 train/feat_loss= 1.17500\n",
      "Epoch: 0480 Auc 0.8138214200563989\n",
      "Epoch: 0481 train_loss= 2.47155 train/struct_loss= 7.65777 train/feat_loss= 1.17500\n",
      "Epoch: 0482 train_loss= 2.47186 train/struct_loss= 7.65931 train/feat_loss= 1.17500\n",
      "Epoch: 0483 train_loss= 2.47187 train/struct_loss= 7.65938 train/feat_loss= 1.17500\n",
      "Epoch: 0484 train_loss= 2.47169 train/struct_loss= 7.65846 train/feat_loss= 1.17500\n",
      "Epoch: 0485 train_loss= 2.47201 train/struct_loss= 7.66008 train/feat_loss= 1.17500\n",
      "Epoch: 0486 train_loss= 2.47170 train/struct_loss= 7.65852 train/feat_loss= 1.17500\n",
      "Epoch: 0487 train_loss= 2.47155 train/struct_loss= 7.65775 train/feat_loss= 1.17500\n",
      "Epoch: 0488 train_loss= 2.47189 train/struct_loss= 7.65944 train/feat_loss= 1.17500\n",
      "Epoch: 0489 train_loss= 2.47197 train/struct_loss= 7.65984 train/feat_loss= 1.17500\n",
      "Epoch: 0490 train_loss= 2.47203 train/struct_loss= 7.66015 train/feat_loss= 1.17500\n",
      "Epoch: 0490 Auc 0.8138714336217221\n",
      "Epoch: 0491 train_loss= 2.47195 train/struct_loss= 7.65976 train/feat_loss= 1.17500\n",
      "Epoch: 0492 train_loss= 2.47163 train/struct_loss= 7.65818 train/feat_loss= 1.17500\n",
      "Epoch: 0493 train_loss= 2.47181 train/struct_loss= 7.65908 train/feat_loss= 1.17500\n",
      "Epoch: 0494 train_loss= 2.47145 train/struct_loss= 7.65727 train/feat_loss= 1.17500\n",
      "Epoch: 0495 train_loss= 2.47184 train/struct_loss= 7.65923 train/feat_loss= 1.17500\n",
      "Epoch: 0496 train_loss= 2.47181 train/struct_loss= 7.65904 train/feat_loss= 1.17500\n",
      "Epoch: 0497 train_loss= 2.47152 train/struct_loss= 7.65758 train/feat_loss= 1.17500\n",
      "Epoch: 0498 train_loss= 2.47186 train/struct_loss= 7.65931 train/feat_loss= 1.17500\n",
      "Epoch: 0499 train_loss= 2.47165 train/struct_loss= 7.65827 train/feat_loss= 1.17500\n",
      "Epoch: 0499 Auc 0.8138207349390657\n"
     ]
    }
   ],
   "source": [
    "train_dominant(dataset=\"BlogCatalog\", hidden_dim=64, max_epoch=500, lr=5e-2, dropout=0.3, alpha=0.8, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f3cce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train_loss= 5.35934 train/struct_loss= 23.50783 train/feat_loss= 0.82222\n",
      "Epoch: 0000 Auc 0.8080926231628126\n",
      "Epoch: 0001 train_loss= 3.13666 train/struct_loss= 12.82364 train/feat_loss= 0.71492\n",
      "Epoch: 0002 train_loss= 1.81711 train/struct_loss= 6.31251 train/feat_loss= 0.69326\n",
      "Epoch: 0003 train_loss= 1.30341 train/struct_loss= 3.74537 train/feat_loss= 0.69292\n",
      "Epoch: 0004 train_loss= 1.24686 train/struct_loss= 3.46262 train/feat_loss= 0.69292\n",
      "Epoch: 0005 train_loss= 1.24628 train/struct_loss= 3.45979 train/feat_loss= 0.69290\n",
      "Epoch: 0006 train_loss= 1.24634 train/struct_loss= 3.46011 train/feat_loss= 0.69290\n",
      "Epoch: 0007 train_loss= 1.24630 train/struct_loss= 3.45987 train/feat_loss= 0.69290\n",
      "Epoch: 0008 train_loss= 1.24670 train/struct_loss= 3.46184 train/feat_loss= 0.69292\n",
      "Epoch: 0009 train_loss= 1.24629 train/struct_loss= 3.45984 train/feat_loss= 0.69290\n",
      "Epoch: 0010 train_loss= 1.24640 train/struct_loss= 3.46042 train/feat_loss= 0.69290\n",
      "Epoch: 0010 Auc 0.896741423067584\n",
      "Epoch: 0011 train_loss= 1.24645 train/struct_loss= 3.46065 train/feat_loss= 0.69290\n",
      "Epoch: 0012 train_loss= 1.24641 train/struct_loss= 3.46045 train/feat_loss= 0.69290\n",
      "Epoch: 0013 train_loss= 1.24642 train/struct_loss= 3.46052 train/feat_loss= 0.69290\n",
      "Epoch: 0014 train_loss= 1.24643 train/struct_loss= 3.46055 train/feat_loss= 0.69290\n",
      "Epoch: 0015 train_loss= 1.24645 train/struct_loss= 3.46064 train/feat_loss= 0.69290\n",
      "Epoch: 0016 train_loss= 1.24643 train/struct_loss= 3.46053 train/feat_loss= 0.69290\n",
      "Epoch: 0017 train_loss= 1.24642 train/struct_loss= 3.46050 train/feat_loss= 0.69290\n",
      "Epoch: 0018 train_loss= 1.24641 train/struct_loss= 3.46043 train/feat_loss= 0.69290\n",
      "Epoch: 0019 train_loss= 1.24642 train/struct_loss= 3.46048 train/feat_loss= 0.69290\n",
      "Epoch: 0020 train_loss= 1.24641 train/struct_loss= 3.46044 train/feat_loss= 0.69290\n",
      "Epoch: 0020 Auc 0.8966660899385832\n",
      "Epoch: 0021 train_loss= 1.24640 train/struct_loss= 3.46038 train/feat_loss= 0.69290\n",
      "Epoch: 0022 train_loss= 1.24639 train/struct_loss= 3.46035 train/feat_loss= 0.69290\n",
      "Epoch: 0023 train_loss= 1.24640 train/struct_loss= 3.46041 train/feat_loss= 0.69290\n",
      "Epoch: 0024 train_loss= 1.24638 train/struct_loss= 3.46029 train/feat_loss= 0.69290\n",
      "Epoch: 0025 train_loss= 1.24643 train/struct_loss= 3.46053 train/feat_loss= 0.69290\n",
      "Epoch: 0026 train_loss= 1.24643 train/struct_loss= 3.46053 train/feat_loss= 0.69290\n",
      "Epoch: 0027 train_loss= 1.24647 train/struct_loss= 3.46073 train/feat_loss= 0.69290\n",
      "Epoch: 0028 train_loss= 1.24648 train/struct_loss= 3.46077 train/feat_loss= 0.69290\n",
      "Epoch: 0029 train_loss= 1.24647 train/struct_loss= 3.46072 train/feat_loss= 0.69290\n",
      "Epoch: 0030 train_loss= 1.24642 train/struct_loss= 3.46051 train/feat_loss= 0.69290\n",
      "Epoch: 0030 Auc 0.8966612926574501\n",
      "Epoch: 0031 train_loss= 1.24638 train/struct_loss= 3.46029 train/feat_loss= 0.69290\n",
      "Epoch: 0032 train_loss= 1.24637 train/struct_loss= 3.46026 train/feat_loss= 0.69290\n",
      "Epoch: 0033 train_loss= 1.24640 train/struct_loss= 3.46037 train/feat_loss= 0.69290\n",
      "Epoch: 0034 train_loss= 1.24639 train/struct_loss= 3.46035 train/feat_loss= 0.69290\n",
      "Epoch: 0035 train_loss= 1.24634 train/struct_loss= 3.46008 train/feat_loss= 0.69290\n",
      "Epoch: 0036 train_loss= 1.24644 train/struct_loss= 3.46058 train/feat_loss= 0.69290\n",
      "Epoch: 0037 train_loss= 1.24644 train/struct_loss= 3.46060 train/feat_loss= 0.69290\n",
      "Epoch: 0038 train_loss= 1.24649 train/struct_loss= 3.46086 train/feat_loss= 0.69290\n",
      "Epoch: 0039 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0040 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0040 Auc 0.8967425301324609\n",
      "Epoch: 0041 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0042 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0043 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0044 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0045 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0046 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0047 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0048 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0049 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0050 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0050 Auc 0.8967425301324609\n",
      "Epoch: 0051 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0052 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0053 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0054 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0055 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0056 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0057 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0058 train_loss= 1.24651 train/struct_loss= 3.46089 train/feat_loss= 0.69292\n",
      "Epoch: 0059 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0060 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0060 Auc 0.8967425301324609\n",
      "Epoch: 0061 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0062 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0063 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0064 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0065 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0066 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0067 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0068 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0069 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0070 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0070 Auc 0.8967425301324609\n",
      "Epoch: 0071 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0072 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0073 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0074 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0075 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0076 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0077 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0078 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0079 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0080 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0080 Auc 0.8967425301324609\n",
      "Epoch: 0081 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0082 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0083 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0084 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0085 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0086 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0087 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0088 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0089 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0090 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0090 Auc 0.8967425301324609\n",
      "Epoch: 0091 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0092 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0093 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0094 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0095 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0096 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0097 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0098 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0099 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0099 Auc 0.8967425301324609\n"
     ]
    }
   ],
   "source": [
    "train_dominant(dataset=\"ACM\", hidden_dim=64, max_epoch=100, lr=5e-3, dropout=0.3, alpha=0.8, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd49974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train_loss= 4.63550 train/struct_loss= 18.08544 train/feat_loss= 1.27301\n",
      "Score:  [11.331537   8.407093   8.669939  ...  3.9367468  4.41934    5.138904 ] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0000 Auc 0.7683332020108105 precision-recall auc 0.3199356391805774\n",
      "Epoch: 0001 train_loss= 3.51235 train/struct_loss= 12.77481 train/feat_loss= 1.19674\n",
      "Epoch: 0002 train_loss= 2.82027 train/struct_loss= 9.37342 train/feat_loss= 1.18198\n",
      "Epoch: 0003 train_loss= 2.42017 train/struct_loss= 7.37486 train/feat_loss= 1.18150\n",
      "Epoch: 0004 train_loss= 2.27135 train/struct_loss= 6.63073 train/feat_loss= 1.18150\n",
      "Epoch: 0005 train_loss= 2.25135 train/struct_loss= 6.53094 train/feat_loss= 1.18146\n",
      "Epoch: 0006 train_loss= 2.21896 train/struct_loss= 6.36893 train/feat_loss= 1.18146\n",
      "Epoch: 0007 train_loss= 2.20871 train/struct_loss= 6.31779 train/feat_loss= 1.18144\n",
      "Epoch: 0008 train_loss= 2.21536 train/struct_loss= 6.35101 train/feat_loss= 1.18144\n",
      "Epoch: 0009 train_loss= 2.21940 train/struct_loss= 6.37127 train/feat_loss= 1.18143\n",
      "Epoch: 0010 train_loss= 2.21479 train/struct_loss= 6.34827 train/feat_loss= 1.18143\n",
      "Score:  [11.691444   8.112262   8.357562  ...  1.073791   1.3300126  1.557188 ] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0010 Auc 0.7924840443134722 precision-recall auc 0.21928145615933786\n",
      "Epoch: 0011 train_loss= 2.21585 train/struct_loss= 6.35354 train/feat_loss= 1.18143\n",
      "Epoch: 0012 train_loss= 2.21832 train/struct_loss= 6.36587 train/feat_loss= 1.18143\n",
      "Epoch: 0013 train_loss= 2.20876 train/struct_loss= 6.31810 train/feat_loss= 1.18143\n",
      "Epoch: 0014 train_loss= 2.20818 train/struct_loss= 6.31518 train/feat_loss= 1.18143\n",
      "Epoch: 0015 train_loss= 2.21053 train/struct_loss= 6.32694 train/feat_loss= 1.18143\n",
      "Epoch: 0016 train_loss= 2.20834 train/struct_loss= 6.31600 train/feat_loss= 1.18143\n",
      "Epoch: 0017 train_loss= 2.21245 train/struct_loss= 6.33652 train/feat_loss= 1.18143\n",
      "Epoch: 0018 train_loss= 2.21089 train/struct_loss= 6.32874 train/feat_loss= 1.18143\n",
      "Epoch: 0019 train_loss= 2.20873 train/struct_loss= 6.31793 train/feat_loss= 1.18143\n",
      "Epoch: 0020 train_loss= 2.21045 train/struct_loss= 6.32653 train/feat_loss= 1.18143\n",
      "Score:  [11.728529   8.30931    8.401905  ...  1.0738276  1.3300067  1.5575988] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0020 Auc 0.7928871519296532 precision-recall auc 0.22237615131071842\n",
      "Epoch: 0021 train_loss= 2.20830 train/struct_loss= 6.31578 train/feat_loss= 1.18143\n",
      "Epoch: 0022 train_loss= 2.20790 train/struct_loss= 6.31380 train/feat_loss= 1.18143\n",
      "Epoch: 0023 train_loss= 2.20942 train/struct_loss= 6.32139 train/feat_loss= 1.18143\n",
      "Epoch: 0024 train_loss= 2.20805 train/struct_loss= 6.31453 train/feat_loss= 1.18143\n",
      "Epoch: 0025 train_loss= 2.20777 train/struct_loss= 6.31315 train/feat_loss= 1.18143\n",
      "Epoch: 0026 train_loss= 2.20783 train/struct_loss= 6.31346 train/feat_loss= 1.18143\n",
      "Epoch: 0027 train_loss= 2.20685 train/struct_loss= 6.30855 train/feat_loss= 1.18143\n",
      "Epoch: 0028 train_loss= 2.20693 train/struct_loss= 6.30895 train/feat_loss= 1.18143\n",
      "Epoch: 0029 train_loss= 2.20706 train/struct_loss= 6.30959 train/feat_loss= 1.18143\n",
      "Epoch: 0030 train_loss= 2.20632 train/struct_loss= 6.30588 train/feat_loss= 1.18143\n",
      "Score:  [12.101147   7.994265   8.366257  ...  1.0738276  1.3300067  1.5575988] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0030 Auc 0.7934379185905416 precision-recall auc 0.2238616968276115\n",
      "Epoch: 0031 train_loss= 2.20629 train/struct_loss= 6.30573 train/feat_loss= 1.18143\n",
      "Epoch: 0032 train_loss= 2.20673 train/struct_loss= 6.30796 train/feat_loss= 1.18143\n",
      "Epoch: 0033 train_loss= 2.20573 train/struct_loss= 6.30293 train/feat_loss= 1.18143\n",
      "Epoch: 0034 train_loss= 2.20593 train/struct_loss= 6.30395 train/feat_loss= 1.18143\n",
      "Epoch: 0035 train_loss= 2.20502 train/struct_loss= 6.29941 train/feat_loss= 1.18143\n",
      "Epoch: 0036 train_loss= 2.20528 train/struct_loss= 6.30069 train/feat_loss= 1.18143\n",
      "Epoch: 0037 train_loss= 2.20544 train/struct_loss= 6.30150 train/feat_loss= 1.18143\n",
      "Epoch: 0038 train_loss= 2.20586 train/struct_loss= 6.30357 train/feat_loss= 1.18143\n",
      "Epoch: 0039 train_loss= 2.20639 train/struct_loss= 6.30626 train/feat_loss= 1.18143\n",
      "Epoch: 0040 train_loss= 2.20508 train/struct_loss= 6.29971 train/feat_loss= 1.18143\n",
      "Score:  [11.56407    8.205689   8.070801  ...  1.0738276  1.3300067  1.5575988] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0040 Auc 0.7940138991758198 precision-recall auc 0.22413051751863866\n",
      "Epoch: 0041 train_loss= 2.20563 train/struct_loss= 6.30247 train/feat_loss= 1.18143\n",
      "Epoch: 0042 train_loss= 2.20398 train/struct_loss= 6.29419 train/feat_loss= 1.18143\n",
      "Epoch: 0043 train_loss= 2.20403 train/struct_loss= 6.29444 train/feat_loss= 1.18143\n",
      "Epoch: 0044 train_loss= 2.20523 train/struct_loss= 6.30045 train/feat_loss= 1.18143\n",
      "Epoch: 0045 train_loss= 2.20482 train/struct_loss= 6.29840 train/feat_loss= 1.18143\n",
      "Epoch: 0046 train_loss= 2.20460 train/struct_loss= 6.29731 train/feat_loss= 1.18143\n",
      "Epoch: 0047 train_loss= 2.20553 train/struct_loss= 6.30192 train/feat_loss= 1.18143\n",
      "Epoch: 0048 train_loss= 2.20507 train/struct_loss= 6.29967 train/feat_loss= 1.18143\n",
      "Epoch: 0049 train_loss= 2.20455 train/struct_loss= 6.29704 train/feat_loss= 1.18143\n",
      "Epoch: 0050 train_loss= 2.20605 train/struct_loss= 6.30454 train/feat_loss= 1.18143\n",
      "Score:  [12.519575   7.9039826  8.183222  ...  1.0738192  1.329448   1.5570635] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0050 Auc 0.7936448303575651 precision-recall auc 0.2239763232329401\n",
      "Epoch: 0051 train_loss= 2.20577 train/struct_loss= 6.30313 train/feat_loss= 1.18143\n",
      "Epoch: 0052 train_loss= 2.20341 train/struct_loss= 6.29134 train/feat_loss= 1.18143\n",
      "Epoch: 0053 train_loss= 2.20445 train/struct_loss= 6.29656 train/feat_loss= 1.18143\n",
      "Epoch: 0054 train_loss= 2.20445 train/struct_loss= 6.29655 train/feat_loss= 1.18143\n",
      "Epoch: 0055 train_loss= 2.20493 train/struct_loss= 6.29894 train/feat_loss= 1.18143\n",
      "Epoch: 0056 train_loss= 2.20439 train/struct_loss= 6.29624 train/feat_loss= 1.18143\n",
      "Epoch: 0057 train_loss= 2.20545 train/struct_loss= 6.30155 train/feat_loss= 1.18143\n",
      "Epoch: 0058 train_loss= 2.20497 train/struct_loss= 6.29917 train/feat_loss= 1.18143\n",
      "Epoch: 0059 train_loss= 2.20670 train/struct_loss= 6.30778 train/feat_loss= 1.18143\n",
      "Epoch: 0060 train_loss= 2.20409 train/struct_loss= 6.29475 train/feat_loss= 1.18143\n",
      "Score:  [11.474302   7.8921723  8.40244   ...  1.0738276  1.3297278  1.5567659] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0060 Auc 0.7939710355043573 precision-recall auc 0.22563170066940635\n",
      "Epoch: 0061 train_loss= 2.20481 train/struct_loss= 6.29833 train/feat_loss= 1.18143\n",
      "Epoch: 0062 train_loss= 2.20348 train/struct_loss= 6.29171 train/feat_loss= 1.18143\n",
      "Epoch: 0063 train_loss= 2.20362 train/struct_loss= 6.29238 train/feat_loss= 1.18143\n",
      "Epoch: 0064 train_loss= 2.20543 train/struct_loss= 6.30143 train/feat_loss= 1.18143\n",
      "Epoch: 0065 train_loss= 2.20429 train/struct_loss= 6.29576 train/feat_loss= 1.18143\n",
      "Epoch: 0066 train_loss= 2.20276 train/struct_loss= 6.28810 train/feat_loss= 1.18143\n",
      "Epoch: 0067 train_loss= 2.20427 train/struct_loss= 6.29563 train/feat_loss= 1.18143\n",
      "Epoch: 0068 train_loss= 2.20489 train/struct_loss= 6.29873 train/feat_loss= 1.18143\n",
      "Epoch: 0069 train_loss= 2.20361 train/struct_loss= 6.29237 train/feat_loss= 1.18143\n",
      "Epoch: 0070 train_loss= 2.20555 train/struct_loss= 6.30205 train/feat_loss= 1.18143\n",
      "Score:  [11.640598   8.112657   8.15417   ...  1.0738276  1.3297406  1.5575988] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0070 Auc 0.7935389318751279 precision-recall auc 0.2233997854795238\n",
      "Epoch: 0071 train_loss= 2.20523 train/struct_loss= 6.30045 train/feat_loss= 1.18143\n",
      "Epoch: 0072 train_loss= 2.20485 train/struct_loss= 6.29854 train/feat_loss= 1.18143\n",
      "Epoch: 0073 train_loss= 2.20419 train/struct_loss= 6.29523 train/feat_loss= 1.18143\n",
      "Epoch: 0074 train_loss= 2.20516 train/struct_loss= 6.30012 train/feat_loss= 1.18143\n",
      "Epoch: 0075 train_loss= 2.20375 train/struct_loss= 6.29304 train/feat_loss= 1.18143\n",
      "Epoch: 0076 train_loss= 2.20357 train/struct_loss= 6.29213 train/feat_loss= 1.18143\n",
      "Epoch: 0077 train_loss= 2.20522 train/struct_loss= 6.30038 train/feat_loss= 1.18143\n",
      "Epoch: 0078 train_loss= 2.20393 train/struct_loss= 6.29395 train/feat_loss= 1.18143\n",
      "Epoch: 0079 train_loss= 2.20420 train/struct_loss= 6.29531 train/feat_loss= 1.18143\n",
      "Epoch: 0080 train_loss= 2.20499 train/struct_loss= 6.29923 train/feat_loss= 1.18143\n",
      "Score:  [11.65122    7.87007    8.371678  ...  1.0739012  1.3295375  1.5565039] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0080 Auc 0.7937819310714341 precision-recall auc 0.22497114918282266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0081 train_loss= 2.20404 train/struct_loss= 6.29452 train/feat_loss= 1.18143\n",
      "Epoch: 0082 train_loss= 2.20465 train/struct_loss= 6.29754 train/feat_loss= 1.18143\n",
      "Epoch: 0083 train_loss= 2.20406 train/struct_loss= 6.29458 train/feat_loss= 1.18143\n",
      "Epoch: 0084 train_loss= 2.20494 train/struct_loss= 6.29898 train/feat_loss= 1.18143\n",
      "Epoch: 0085 train_loss= 2.20598 train/struct_loss= 6.30419 train/feat_loss= 1.18143\n",
      "Epoch: 0086 train_loss= 2.20527 train/struct_loss= 6.30064 train/feat_loss= 1.18143\n",
      "Epoch: 0087 train_loss= 2.20528 train/struct_loss= 6.30070 train/feat_loss= 1.18143\n",
      "Epoch: 0088 train_loss= 2.20572 train/struct_loss= 6.30288 train/feat_loss= 1.18143\n",
      "Epoch: 0089 train_loss= 2.20461 train/struct_loss= 6.29735 train/feat_loss= 1.18143\n",
      "Epoch: 0090 train_loss= 2.20579 train/struct_loss= 6.30322 train/feat_loss= 1.18143\n",
      "Score:  [11.545618   7.989866   8.279737  ...  1.0738684  1.3293853  1.5575988] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0090 Auc 0.7940422648407583 precision-recall auc 0.2251481637010963\n",
      "Epoch: 0091 train_loss= 2.20440 train/struct_loss= 6.29630 train/feat_loss= 1.18143\n",
      "Epoch: 0092 train_loss= 2.20477 train/struct_loss= 6.29815 train/feat_loss= 1.18143\n",
      "Epoch: 0093 train_loss= 2.20440 train/struct_loss= 6.29631 train/feat_loss= 1.18143\n",
      "Epoch: 0094 train_loss= 2.20575 train/struct_loss= 6.30305 train/feat_loss= 1.18143\n",
      "Epoch: 0095 train_loss= 2.20440 train/struct_loss= 6.29630 train/feat_loss= 1.18143\n",
      "Epoch: 0096 train_loss= 2.20764 train/struct_loss= 6.31248 train/feat_loss= 1.18143\n",
      "Epoch: 0097 train_loss= 2.20643 train/struct_loss= 6.30643 train/feat_loss= 1.18143\n",
      "Epoch: 0098 train_loss= 2.20578 train/struct_loss= 6.30319 train/feat_loss= 1.18143\n",
      "Epoch: 0099 train_loss= 2.20489 train/struct_loss= 6.29872 train/feat_loss= 1.18143\n",
      "Score:  [11.485475   7.91902    8.182805  ...  1.0738319  1.3297378  1.5575988] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0099 Auc 0.7937179507382952 precision-recall auc 0.22427299859143884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0], dtype=uint8),\n",
       " array([11.485475 ,  7.91902  ,  8.182805 , ...,  1.0738319,  1.3297378,\n",
       "         1.5575988], dtype=float32))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dominant(dataset=\"Flickr\", hidden_dim=64, max_epoch=100, lr=5e-3, dropout=0.3, alpha=0.8, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7aeb78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ed40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beed7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_mat = sio.loadmat(f'data/Amazon.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8fac41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Mon Aug 17 19:18:11 2015',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'gnd': array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], dtype=uint8),\n",
       " 'X': array([[2.06610e-02, 3.33333e-01, 1.90000e-05, ..., 1.30400e-03,\n",
       "         0.00000e+00, 0.00000e+00],\n",
       "        [0.00000e+00, 1.22449e-01, 1.22000e-04, ..., 4.95500e-03,\n",
       "         3.06122e-01, 2.24490e-01],\n",
       "        [0.00000e+00, 0.00000e+00, 1.36000e-04, ..., 5.22000e-04,\n",
       "         1.00000e+00, 0.00000e+00],\n",
       "        ...,\n",
       "        [0.00000e+00, 0.00000e+00, 1.00000e-05, ..., 2.61000e-04,\n",
       "         1.00000e+00, 0.00000e+00],\n",
       "        [0.00000e+00, 2.00000e-01, 1.00000e-05, ..., 1.04300e-03,\n",
       "         4.00000e-01, 0.00000e+00],\n",
       "        [1.23970e-02, 4.54550e-02, 1.00000e-05, ..., 6.25900e-03,\n",
       "         6.81818e-01, 0.00000e+00]]),\n",
       " 'A': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb02c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa40cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8452d9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = data_mat['Network']\n",
    "feat = data_mat['Attributes']\n",
    "truth = data_mat['Label']\n",
    "truth = truth.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c74a7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=uint8), array([4898,  298]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(truth, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8b0ffdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5196x5196 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 350577 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_norm = normalize_adj(adj + sp.eye(adj.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68a84290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0012987 , 0.00156096, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00156096, 0.00187617, 0.00195477, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.00195477, 0.00203666, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.0625    , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.05263158,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.05263158]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_norm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aee509d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_norm = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "adj_norm = adj_norm.toarray()\n",
    "adj = adj + sp.eye(adj.shape[0])\n",
    "adj = adj.toarray()\n",
    "feat = feat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8cb3d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0012987 , 0.00156096, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00156096, 0.00187617, 0.00195477, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.00195477, 0.00203666, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.0625    , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.05263158,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.05263158]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5045946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_norm[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf48adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b570fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
