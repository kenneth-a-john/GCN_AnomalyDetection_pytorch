{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a5f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from model import Dominant\n",
    "from torch_geometric.nn import SAGEConv, GATConv, DenseSAGEConv, DenseGATConv\n",
    "from time import process_time_ns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beed7c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ACM.mat : \n",
      "total nodes:  16484\n",
      "normal nodes:  15887\n",
      "anomalous nodes:  597\n",
      "avg. degree:  9.970274205289979 \n",
      "\n",
      "\n",
      "data/Flickr.mat : \n",
      "total nodes:  7575\n",
      "normal nodes:  7130\n",
      "anomalous nodes:  445\n",
      "avg. degree:  63.703630363036304 \n",
      "\n",
      "\n",
      "data/BlogCatalog.mat : \n",
      "total nodes:  5196\n",
      "normal nodes:  4898\n",
      "anomalous nodes:  298\n",
      "avg. degree:  66.49711316397229 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datum = [f'data/ACM.mat', f'data/Flickr.mat', f'data/BlogCatalog.mat']\n",
    "for data in datum:\n",
    "    print(data, \": \")\n",
    "    print(\"avg. degree: \", getAverageDegree(data), \"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad1eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    #co ordinate matrix \n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def load_anomaly_detection_dataset(dataset, datadir='data'):\n",
    "    \n",
    "    data_mat = sio.loadmat(f'{datadir}/{dataset}.mat')\n",
    "    adj = data_mat['Network']\n",
    "    feat = data_mat['Attributes']\n",
    "    truth = data_mat['Label']\n",
    "    truth = truth.flatten()\n",
    "\n",
    "    adj_norm = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    adj_norm = adj_norm.toarray()\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    adj = adj.toarray()\n",
    "    feat = feat.toarray()\n",
    "    return adj_norm, feat, truth, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "487f3b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    # Initializes the GCN layer with input and output feature dimensions,\n",
    "    # and an optional bias term.\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # Define the weight matrix as a learnable parameter of shape (in_features, out_features)\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    \n",
    "    # Initialize the weights and biases using a uniform distribution\n",
    "    # with range determined by the inverse square root of the number of output features.\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # linear transformation of the input features using the weight matrix.\n",
    "        support = torch.mm(input, self.weight)\n",
    "        \n",
    "        # apply the adjacency matrix to the transformed features to propagate the features across the graph\n",
    "        output = torch.spmm(adj, support) if adj.is_sparse else torch.mm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "153e9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nhid)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        I = torch.eye(adj.size(0)).to(adj.device)\n",
    "        if adj.is_sparse:\n",
    "            I = I.to_sparse()\n",
    "        adj_with_self_loops = adj + I\n",
    "        \n",
    "        x = F.relu(self.gc1(x, adj_with_self_loops))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, adj_with_self_loops))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Attribute_Decoder(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, dropout):\n",
    "        super(Attribute_Decoder, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nhid, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nfeat)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, adj))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Structure_Decoder(nn.Module):\n",
    "    def __init__(self, nhid, dropout):\n",
    "        super(Structure_Decoder, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nhid, nhid)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = x @ x.T        #this is matrix multiplication\n",
    "\n",
    "        return x\n",
    "\n",
    "class Dominant(nn.Module):\n",
    "    def __init__(self, feat_size, hidden_size, dropout):\n",
    "        super(Dominant, self).__init__()\n",
    "        \n",
    "        self.shared_encoder = Encoder(feat_size, hidden_size, dropout)\n",
    "        self.attr_decoder = Attribute_Decoder(feat_size, hidden_size, dropout)\n",
    "        self.struct_decoder = Structure_Decoder(hidden_size, dropout)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        # encode\n",
    "        x = self.shared_encoder(x, adj)\n",
    "        # decode feature matrix\n",
    "        x_hat = self.attr_decoder(x, adj)\n",
    "        # decode adjacency matrix\n",
    "        struct_reconstructed = self.struct_decoder(x, adj)\n",
    "        # return reconstructed matrices\n",
    "        return struct_reconstructed, x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8758cb62",
   "metadata": {},
   "source": [
    "## GAT Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce9066a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, N, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bffd4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATEncoder(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, dropout, alpha=0.2, concat=True):\n",
    "        super(GATEncoder, self).__init__()\n",
    "        self.gat1 = GATLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=concat)\n",
    "        self.gat2 = GATLayer(nhid, nhid, dropout=dropout, alpha=alpha, concat=False)  # For the second layer, we might not want to concatenate features\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gat1(x, adj)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gat2(x, adj)\n",
    "        return x\n",
    "\n",
    "class Dominant_GAT(nn.Module):\n",
    "    def __init__(self, feat_size, hidden_size, dropout):\n",
    "        super(Dominant_GAT, self).__init__()\n",
    "        \n",
    "        self.shared_encoder = GATEncoder(feat_size, hidden_size, dropout)\n",
    "        self.attr_decoder = Attribute_Decoder(feat_size, hidden_size, dropout)\n",
    "        self.struct_decoder = Structure_Decoder(hidden_size, dropout)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        # encode\n",
    "        x = self.shared_encoder(x, adj)\n",
    "        # decode feature matrix\n",
    "        x_hat = self.attr_decoder(x, adj)\n",
    "        # decode adjacency matrix\n",
    "        struct_reconstructed = self.struct_decoder(x, adj)\n",
    "        # return reconstructed matrices\n",
    "        return struct_reconstructed, x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65ecbb3",
   "metadata": {},
   "source": [
    "## GAT Encoder using pgG GATConvLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e858cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pyGATEncoder(Module):\n",
    "    def __init__(self, in_features, out_features, dropout, concat=True):\n",
    "        super(pyGATEncoder, self).__init__()\n",
    "        self.gat1 = GATConv(in_features, out_features, concat=concat)\n",
    "        self.gat2 = GATConv(out_features, out_features, concat=False)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        edge_index = adj.nonzero().t().contiguous()\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class Dominant_pyGGAT(nn.Module):\n",
    "    def __init__(self, feat_size, hidden_size, dropout):\n",
    "        super(Dominant_pyGGAT, self).__init__()\n",
    "        \n",
    "        self.shared_encoder = pyGATEncoder(feat_size, hidden_size, dropout)\n",
    "        self.attr_decoder = Attribute_Decoder(feat_size, hidden_size, dropout)\n",
    "        self.struct_decoder = Structure_Decoder(hidden_size, dropout)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        # encode\n",
    "        x = self.shared_encoder(x, adj)\n",
    "        # decode feature matrix\n",
    "        x_hat = self.attr_decoder(x, adj)\n",
    "        # decode adjacency matrix\n",
    "        struct_reconstructed = self.struct_decoder(x, adj)\n",
    "        # return reconstructed matrices\n",
    "        return struct_reconstructed, x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a877d9",
   "metadata": {},
   "source": [
    "## WIP: DenseGAT Encoder using pgG DenseGATConv Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "746e2b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pyDenseGATEncoder(Module):\n",
    "    def __init__(self, in_features, out_features, dropout, concat=True):\n",
    "        super(pyDenseGATEncoder, self).__init__()\n",
    "        self.dense_gat1 = DenseGATConv(in_features, out_features, concat=concat)\n",
    "        self.dense_gat2 = DenseGATConv(out_features, out_features, concat=False)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.dense_gat1(x, adj)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.dense_gat2(x, adj)\n",
    "        return x\n",
    "\n",
    "class Dominant_pyGDenseGAT(nn.Module):\n",
    "    def __init__(self, feat_size, hidden_size, dropout):\n",
    "        super(Dominant_pyGDenseGAT, self).__init__()\n",
    "        \n",
    "        self.shared_encoder = pyDenseGATEncoder(feat_size, hidden_size, dropout)\n",
    "        self.attr_decoder = Attribute_Decoder(feat_size, hidden_size, dropout)\n",
    "        self.struct_decoder = Structure_Decoder(hidden_size, dropout)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        # encode\n",
    "        x = self.shared_encoder(x, adj)\n",
    "        # decode feature matrix\n",
    "        x_hat = self.attr_decoder(x, adj)\n",
    "        # decode adjacency matrix\n",
    "        struct_reconstructed = self.struct_decoder(x, adj)\n",
    "        # return reconstructed matrices\n",
    "        return struct_reconstructed, x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc10dde",
   "metadata": {},
   "source": [
    "## GraphSAGE Encoder using pgG SAGEConv Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cc65c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGEEncoder(Module):\n",
    "    def __init__(self, in_features, out_features, dropout):\n",
    "        super(GraphSAGEEncoder, self).__init__()\n",
    "        # Define two GraphSAGE convolutional layers\n",
    "        self.sage1 = SAGEConv(in_features, out_features)\n",
    "        self.sage2 = SAGEConv(out_features, out_features)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        edge_index = adj.nonzero().t().contiguous()\n",
    "        # Apply the first GraphSAGE layer and a ReLU activation\n",
    "        x = F.relu(self.sage1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # Apply the second GraphSAGE layer\n",
    "        x = self.sage2(x, edge_index)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Dominant_GraphSAGE(nn.Module):\n",
    "    def __init__(self, feat_size, hidden_size, dropout):\n",
    "        super(Dominant_GraphSAGE, self).__init__()\n",
    "        \n",
    "        self.shared_encoder = GraphSAGEEncoder(feat_size, hidden_size, dropout)\n",
    "        self.attr_decoder = Attribute_Decoder(feat_size, hidden_size, dropout)\n",
    "        self.struct_decoder = Structure_Decoder(hidden_size, dropout)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        # encode\n",
    "        x = self.shared_encoder(x, adj)\n",
    "        # decode feature matrix\n",
    "        x_hat = self.attr_decoder(x, adj)\n",
    "        # decode adjacency matrix\n",
    "        struct_reconstructed = self.struct_decoder(x, adj)\n",
    "        # return reconstructed matrices\n",
    "        return struct_reconstructed, x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573393ea",
   "metadata": {},
   "source": [
    "## WIP: DenseGrpahSAGE Encoder using pgG DenseSAGEConv Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44d8b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseSAGEEncoder(Module):\n",
    "    def __init__(self, in_features, out_features, dropout):\n",
    "        super(DenseSAGEEncoder, self).__init__()\n",
    "        # Define two GraphSAGE convolutional layers\n",
    "        self.dense_sage1 = DenseSAGEConv(in_features, out_features)\n",
    "        self.dense_sage2 = DenseSAGEConv(out_features, out_features)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "#         edge_index = adj.nonzero().t().contiguous()\n",
    "        # Apply the first GraphSAGE layer and a ReLU activation\n",
    "        x = F.relu(self.dense_sage1(x, adj))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # Apply the second GraphSAGE layer\n",
    "        x = self.dense_sage2(x, adj)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Dominant_DenseSAGE(nn.Module):\n",
    "    def __init__(self, feat_size, hidden_size, dropout):\n",
    "        super(Dominant_DenseSAGE, self).__init__()\n",
    "        \n",
    "        self.shared_encoder = DenseSAGEEncoder(feat_size, hidden_size, dropout)\n",
    "        self.attr_decoder = Attribute_Decoder(feat_size, hidden_size, dropout)\n",
    "        self.struct_decoder = Structure_Decoder(hidden_size, dropout)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        # encode\n",
    "        x = self.shared_encoder(x, adj)\n",
    "        # decode feature matrix\n",
    "        x_hat = self.attr_decoder(x, adj)\n",
    "        # decode adjacency matrix\n",
    "        struct_reconstructed = self.struct_decoder(x, adj)\n",
    "        # return reconstructed matrices\n",
    "        return struct_reconstructed, x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f65441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(adj, A_hat, attrs, X_hat, alpha):\n",
    "    # Attribute reconstruction loss\n",
    "    diff_attribute = torch.pow(X_hat - attrs, 2)\n",
    "    attribute_reconstruction_errors = torch.sqrt(torch.sum(diff_attribute, 1))\n",
    "    attribute_cost = torch.mean(attribute_reconstruction_errors)\n",
    "\n",
    "    # structure reconstruction loss\n",
    "    diff_structure = torch.pow(A_hat - adj, 2)\n",
    "    structure_reconstruction_errors = torch.sqrt(torch.sum(diff_structure, 1))\n",
    "    structure_cost = torch.mean(structure_reconstruction_errors)\n",
    "\n",
    "\n",
    "    cost =  alpha * attribute_reconstruction_errors + (1-alpha) * structure_reconstruction_errors\n",
    "\n",
    "    return cost, structure_cost, attribute_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6ac6218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dominant(encoder=\"OG\", dataset=\"BlogCatalog\", hidden_dim=64, max_epoch=100, lr=5e-3, dropout=0.3, alpha=0.8, device=\"cpu\"):\n",
    "    adj, attrs, label, adj_label = load_anomaly_detection_dataset(dataset)\n",
    "    adj = torch.FloatTensor(adj)\n",
    "    adj_label = torch.FloatTensor(adj_label)\n",
    "    attrs = torch.FloatTensor(attrs)\n",
    "    start_time = process_time_ns()\n",
    "    if encoder == \"OG\":\n",
    "        model = Dominant(feat_size = attrs.size(1), hidden_size = hidden_dim, dropout = dropout)\n",
    "    elif encoder == \"GAT\":\n",
    "        model = Dominant_GAT(feat_size = attrs.size(1), hidden_size = hidden_dim, dropout = dropout)\n",
    "    elif encoder == \"pyGGAT\":\n",
    "        model= Dominant_pyGGAT(feat_size = attrs.size(1), hidden_size = hidden_dim, dropout = dropout)\n",
    "    elif encoder == \"GraphSAGE\":\n",
    "        model = Dominant_GraphSAGE(feat_size = attrs.size(1), hidden_size = hidden_dim, dropout = dropout)\n",
    "    elif encoder == \"DenseSage\":\n",
    "        model = Dominant_DenseSAGE(feat_size = attrs.size(1), hidden_size = hidden_dim, dropout = dropout)\n",
    "    elif encoder == \"DenseGAT\":\n",
    "        model = Dominant_pyGDenseGAT(feat_size = attrs.size(1), hidden_size = hidden_dim, dropout = dropout)\n",
    "    else:\n",
    "        print(\"Invalid Encoder specified\\n\")\n",
    "        return\n",
    "\n",
    "    if device == 'cuda':\n",
    "        device = torch.device(device)\n",
    "        adj = adj.to(device)\n",
    "        adj_label = adj_label.to(device)\n",
    "        attrs = attrs.to(device)\n",
    "        model = model.cuda() \n",
    "        \n",
    "    \n",
    "    optimizer =  torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_dict = {\n",
    "        \"struct_loss\": [],\n",
    "        \"feat_loss\": [],\n",
    "        \"loss\": []\n",
    "    }\n",
    "    for epoch in range(max_epoch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # returns the reconstructed matrices\n",
    "        A_hat, X_hat = model(attrs, adj)\n",
    "        loss, struct_loss, feat_loss = loss_func(adj_label, A_hat, attrs, X_hat, alpha)\n",
    "        l = torch.mean(loss)\n",
    "        l.backward()\n",
    "        optimizer.step()        \n",
    "#         print(\"Epoch:\", '%04d' % (epoch), \n",
    "#               \"train_loss=\", \"{:.5f}\".format(l.item()), \n",
    "#               \"train/struct_loss=\", \"{:.5f}\".format(struct_loss.item()),\n",
    "#               \"train/feat_loss=\", \"{:.5f}\".format(feat_loss.item()))\n",
    "        loss_dict[\"struct_loss\"].append(struct_loss.item())\n",
    "        loss_dict[\"feat_loss\"].append(feat_loss.item())\n",
    "        loss_dict[\"loss\"].append(l.item())\n",
    "        if epoch%10 == 0 or epoch == max_epoch - 1:\n",
    "            model.eval()\n",
    "            score = loss.detach().cpu().numpy()\n",
    "            normalizedScore = (score - np.min(score))/(np.max(score)-np.min(score))\n",
    "            precision, recall, thresholds = precision_recall_curve(label, normalizedScore) #modified \n",
    "            print(\"Epoch:\", '%04d' % (epoch), \n",
    "                  \"train_loss=\", \"{:.5f}\".format(l.item()), \n",
    "                  \"train/struct_loss=\", \"{:.5f}\".format(struct_loss.item()),\n",
    "                  \"train/feat_loss=\", \"{:.5f}\".format(feat_loss.item()),\n",
    "                  'Auc', roc_auc_score(label, score))\n",
    "#     return label, score\n",
    "    end_time = process_time_ns()\n",
    "    response = {\n",
    "        \"loss_dict\": loss_dict,\n",
    "        \"time_elapsed\": end_time - start_time,\n",
    "        \"PR_curve\": {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"thresholds\": thresholds\n",
    "        }\n",
    "    }\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fbb613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train_loss= 5.40353 train/struct_loss= 21.97941 train/feat_loss= 1.25956 Auc 0.755572744388204\n",
      "Epoch: 0010 train_loss= 2.47462 train/struct_loss= 7.67452 train/feat_loss= 1.17464 Auc 0.8135144874911278\n",
      "Epoch: 0020 train_loss= 2.47310 train/struct_loss= 7.66699 train/feat_loss= 1.17463 Auc 0.8134124050084818\n",
      "Epoch: 0030 train_loss= 2.47038 train/struct_loss= 7.65338 train/feat_loss= 1.17463 Auc 0.8137748320777416\n",
      "Epoch: 0040 train_loss= 2.46979 train/struct_loss= 7.65043 train/feat_loss= 1.17463 Auc 0.8138419735763947\n",
      "Epoch: 0050 train_loss= 2.46930 train/struct_loss= 7.64800 train/feat_loss= 1.17463 Auc 0.8138837657337197\n",
      "Epoch: 0060 train_loss= 2.46941 train/struct_loss= 7.64853 train/feat_loss= 1.17463 Auc 0.8138597866270578\n",
      "Epoch: 0070 train_loss= 2.46900 train/struct_loss= 7.64647 train/feat_loss= 1.17463 Auc 0.8141674043096622\n",
      "Epoch: 0080 train_loss= 2.46890 train/struct_loss= 7.64597 train/feat_loss= 1.17463 Auc 0.813922474863045\n",
      "Epoch: 0090 train_loss= 2.46853 train/struct_loss= 7.64413 train/feat_loss= 1.17463 Auc 0.8139282983603773\n",
      "Epoch: 0099 train_loss= 2.46874 train/struct_loss= 7.64519 train/feat_loss= 1.17463 Auc 0.8142030304109882\n"
     ]
    }
   ],
   "source": [
    "OG_response = train_dominant(encoder=\"OG\",dataset=\"BlogCatalog\", hidden_dim=64, max_epoch=100, lr=5e-3, dropout=0.3, alpha=0.8, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "819834a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train_loss= 3.52915 train/struct_loss= 12.62698 train/feat_loss= 1.25470 Auc 0.7952300761028334\n",
      "Epoch: 0010 train_loss= 2.48083 train/struct_loss= 7.70595 train/feat_loss= 1.17455 Auc 0.8132356447365177\n",
      "Epoch: 0020 train_loss= 2.47148 train/struct_loss= 7.65930 train/feat_loss= 1.17452 Auc 0.81388171038172\n",
      "Epoch: 0030 train_loss= 2.46904 train/struct_loss= 7.64715 train/feat_loss= 1.17451 Auc 0.8137899046590718\n",
      "Epoch: 0040 train_loss= 2.46828 train/struct_loss= 7.64336 train/feat_loss= 1.17451 Auc 0.813748112501747\n",
      "Epoch: 0050 train_loss= 2.46812 train/struct_loss= 7.64256 train/feat_loss= 1.17451 Auc 0.8138577312750581\n",
      "Epoch: 0060 train_loss= 2.46743 train/struct_loss= 7.63912 train/feat_loss= 1.17451 Auc 0.8139330941817096\n",
      "Epoch: 0070 train_loss= 2.46730 train/struct_loss= 7.63848 train/feat_loss= 1.17451 Auc 0.8139995505630294\n",
      "Epoch: 0080 train_loss= 2.46705 train/struct_loss= 7.63720 train/feat_loss= 1.17451 Auc 0.8140605260056838\n",
      "Epoch: 0090 train_loss= 2.46687 train/struct_loss= 7.63634 train/feat_loss= 1.17451 Auc 0.8139474816457067\n",
      "Epoch: 0099 train_loss= 2.46707 train/struct_loss= 7.63733 train/feat_loss= 1.17451 Auc 0.8140687474136821\n"
     ]
    }
   ],
   "source": [
    "pyGAT_response = train_dominant(encoder=\"pyGGAT\", dataset=\"BlogCatalog\", hidden_dim=64, max_epoch=100, lr=5e-3, dropout=0.3, alpha=0.8, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67299d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train_loss= 5.30873 train/struct_loss= 21.52270 train/feat_loss= 1.25524 Auc 0.7606330210111785\n",
      "Epoch: 0010 train_loss= 2.47587 train/struct_loss= 7.68105 train/feat_loss= 1.17458 Auc 0.8132349596191843\n",
      "Epoch: 0020 train_loss= 2.48359 train/struct_loss= 7.71969 train/feat_loss= 1.17456 Auc 0.8130814933365488\n",
      "Epoch: 0030 train_loss= 2.48374 train/struct_loss= 7.72046 train/feat_loss= 1.17456 Auc 0.8130568291125537\n",
      "Epoch: 0040 train_loss= 2.48374 train/struct_loss= 7.72046 train/feat_loss= 1.17456 Auc 0.8130547737605544\n",
      "Epoch: 0050 train_loss= 2.48374 train/struct_loss= 7.72046 train/feat_loss= 1.17456 Auc 0.8130527184085546\n",
      "Epoch: 0060 train_loss= 2.47533 train/struct_loss= 7.67843 train/feat_loss= 1.17456 Auc 0.8136001271577771\n",
      "Epoch: 0070 train_loss= 2.48374 train/struct_loss= 7.72046 train/feat_loss= 1.17456 Auc 0.8130527184085545\n",
      "Epoch: 0080 train_loss= 2.48374 train/struct_loss= 7.72046 train/feat_loss= 1.17456 Auc 0.813053403525888\n",
      "Epoch: 0090 train_loss= 2.48374 train/struct_loss= 7.72046 train/feat_loss= 1.17456 Auc 0.8130527184085546\n",
      "Epoch: 0099 train_loss= 2.48374 train/struct_loss= 7.72046 train/feat_loss= 1.17456 Auc 0.8130527184085546\n"
     ]
    }
   ],
   "source": [
    "GraphSAGE_response = train_dominant(encoder=\"GraphSAGE\",dataset=\"BlogCatalog\", hidden_dim=64, max_epoch=100, lr=5e-3, dropout=0.3, alpha=0.8, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32bc9eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function names\n",
    "functions = ['OG', 'pyGAT', 'GraphSAGE']\n",
    "\n",
    "# Execution times for each function\n",
    "execution_times = [\n",
    "    OG_response[\"time_elapsed\"],\n",
    "    pyGAT_response[\"time_elapsed\"],\n",
    "    GraphSAGE_response[\"time_elapsed\"]\n",
    "]\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.bar(functions, execution_times, color=['blue', 'green', 'red'])\n",
    "\n",
    "plt.xlabel('Encoder')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Comparison of Function Execution Times')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b04050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses():\n",
    "    # Example data setup\n",
    "    epochs = list(range(1, 101))  # Assuming 100 epochs\n",
    "\n",
    "    # Assuming data for each function (func1, func2, func3) and each type of loss\n",
    "    # Replace these with your actual data\n",
    "    loss_data = {\n",
    "        'OG': {\n",
    "            'loss': OG_response[\"loss_dict\"][\"loss\"],\n",
    "            'struct_loss': OG_response[\"loss_dict\"][\"struct_loss\"],\n",
    "            'feat_loss': OG_response[\"loss_dict\"][\"feat_loss\"],\n",
    "        },\n",
    "        'pyGAT': {\n",
    "            'loss': pyGAT_response[\"loss_dict\"][\"loss\"],\n",
    "            'struct_loss': pyGAT_response[\"loss_dict\"][\"struct_loss\"],\n",
    "            'feat_loss': pyGAT_response[\"loss_dict\"][\"feat_loss\"],\n",
    "        },\n",
    "        'GraphSAGE': {\n",
    "            'loss': GraphSAGE_response[\"loss_dict\"][\"loss\"],\n",
    "            'struct_loss': GraphSAGE_response[\"loss_dict\"][\"struct_loss\"],\n",
    "            'feat_loss': GraphSAGE_response[\"loss_dict\"][\"feat_loss\"],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Creating a figure with subplots for each type of loss\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))  # 1 row, 3 columns\n",
    "\n",
    "    # Loss names for titles and iteration\n",
    "    loss_types = ['loss', 'struct_loss', 'feat_loss']\n",
    "\n",
    "    # Colors for each function\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    function_names = ['OG', 'pyGAT', 'GraphSAGE']\n",
    "\n",
    "    # Plotting data for each type of loss across all functions\n",
    "    for i, loss_type in enumerate(loss_types):\n",
    "        for func_name, color in zip(function_names, colors):\n",
    "            axs[i].plot(epochs, loss_data[func_name][loss_type], label=func_name, color=color)\n",
    "\n",
    "        axs[i].set_title(f'{loss_type.capitalize()} Comparison')\n",
    "        axs[i].set_xlabel('Epoch')\n",
    "        axs[i].set_ylabel('Loss Value')\n",
    "        axs[i].legend()\n",
    "\n",
    "    # Adjust layout\n",
    "#     plt.tight_layout(pad=3.0)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0018f772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbUAAAHWCAYAAABAE2SrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAAClgUlEQVR4nOzdeXhTZdrH8V+6b7SlQFvKvikoCiiroIKAUBEFUVxQQVBmFBwQHRRHVBBBUUdcWESR5VXAZQB3FFHEGVkEREWQRSsg0KICLRS65rx/pDk0NC1JmzRJ+/1cVy7pOSfJk+LFnXM/93M/FsMwDAEAAAAAAAAAEACCfD0AAAAAAAAAAABcRVIbAAAAAAAAABAwSGoDAAAAAAAAAAIGSW0AAAAAAAAAQMAgqQ0AAAAAAAAACBgktQEAAAAAAAAAAYOkNgAAAAAAAAAgYJDUBgAAAAAAAAAEDJLaAAAAAAAAAICAQVIbQED47bffZLFYtGDBAl8PBQBQDVksFj3++OO+HgbOwPcDAKheHn/8cVksFl8PA04MGzZMjRs39vUwUI2Q1Ea1tmDBAlksFm3atMnXQ3HJ1q1bdeutt6pBgwYKDw9XQkKCevXqpfnz56uwsNDXwwMAVEM//vijrr/+ejVq1EgRERGqV6+eevfurZdeesnhuqlTp2rFihW+GaQfjcEb+H4AACgv+z2xs8dDDz3klff85ptv9Pjjj+vYsWNeeX1fycnJ0fPPP69OnTopLi5OEREROuecczR69Gjt2rXL18MDqpwQXw8AgGtee+01/f3vf1dSUpJuu+02tWjRQsePH9fq1as1YsQIHTp0SA8//LCvh+k1jRo10qlTpxQaGurroQAAinzzzTfq0aOHGjZsqLvuukvJycnav3+/1q9frxdeeEH33nuvee3UqVN1/fXXa8CAAT4brz+MwdP4fsD3AwDwhMmTJ6tJkyYOx1q3bu2V9/rmm280adIkDRs2TPHx8V55j8r2559/qm/fvtq8ebOuvvpq3XLLLYqJidHOnTu1dOlSzZ07V3l5eb4eple9+uqrslqtvh4GqhGS2kAAWL9+vf7+97+rS5cu+vjjj1WjRg3z3NixY7Vp0yZt27bNhyP0noKCAlmtVoWFhSkiIsLXwwEAFPPkk08qLi5O3377bYmb0sOHD5f7dbOzsxUdHV3B0VV9fD/g+wEAeEpqaqrat2/v62EErGHDhum7777Tu+++q0GDBjmce+KJJ/Svf/3LRyPzPvv3NiaYUdloPwK44LvvvlNqaqpiY2MVExOjnj17av369Q7X5Ofna9KkSWrRooUiIiJUq1YtdevWTatWrTKvSU9P1x133KH69esrPDxcdevW1bXXXqvffvutzPefNGmSLBaL3nzzTYcbVrv27dtr2LBh5s/Z2dm6//77zWXI5557rp599lkZhuHwPIvFotGjR+udd97Reeedp8jISHXp0kU//vijJOmVV15R8+bNFRERoe7du5cYZ/fu3dW6dWtt3rxZl1xyiSIjI9WkSRPNmTPH4bq8vDw9+uijuvjiixUXF6fo6Ghdeuml+vLLLx2us/fFfPbZZzVjxgw1a9ZM4eHh2r59u9Oema7+PmfNmqXzzz9f4eHhSklJ0ahRo0osdbN/lu3bt6tHjx6KiopSvXr1NH369DL+ZgCgevvll190/vnnO62ySkxMNP9ssViUnZ2thQsXmkua7XHL3htz+/btuuWWW1SzZk1169ZNku3f5u7du5d4bWc9G61Wq1544QVdcMEFioiIUJ06ddS3b1+zxVhZYygvvh/w/QAAqotPPvlEl156qaKjo1WjRg3169dPP/30k8M1P/zwg4YNG6amTZsqIiJCycnJGj58uP766y/zmscff1z//Oc/JUlNmjQxY/LZYl5ZCgoK9MQTT5jxoXHjxnr44YeVm5vrcN2mTZvUp08f1a5d24xNw4cPd7hm6dKluvjii1WjRg3Fxsbqggsu0AsvvFDm+2/YsEEfffSRRowYUSKhLUnh4eF69tlnHY598cUX5u8zPj5e1157rXbs2OFwjf070q5du3TrrbcqLi5OderU0cSJE2UYhvbv369rr71WsbGxSk5O1nPPPefw/DVr1shiseitt97Sww8/rOTkZEVHR+uaa67R/v37Ha79+uuvdcMNN6hhw4YKDw9XgwYNdN999+nUqVMO1w0bNkwxMTH65ZdfdNVVV6lGjRoaMmSIee7M72eu/D5//fVX3XDDDUpISFBUVJQ6d+6sjz76yOlnefvtt/Xkk0+qfv36ioiIUM+ePbVnz55S/mZQ1VGpDZzFTz/9pEsvvVSxsbEaP368QkND9corr6h79+766quv1KlTJ0m2gDNt2jTdeeed6tixo7KysrRp0yZt2bJFvXv3liQNGjRIP/30k+699141btxYhw8f1qpVq7Rv375SN1Q4efKkVq9ercsuu0wNGzY863gNw9A111yjL7/8UiNGjFDbtm316aef6p///KcOHDig559/3uH6r7/+Wu+//75GjRolSZo2bZquvvpqjR8/XrNmzdI999yjo0ePavr06Ro+fLi++OILh+cfPXpUV111lQYPHqybb75Zb7/9tu6++26FhYWZXxCysrL02muv6eabb9Zdd92l48ePa968eerTp482btyotm3bOrzm/PnzlZOTo5EjR5q9QZ0tY3Ll9/n4449r0qRJ6tWrl+6++27t3LlTs2fP1rfffqv//e9/DrPJR48eVd++fXXddddp8ODBevfdd/Xggw/qggsuUGpq6ll/9wBQ3TRq1Ejr1q3Ttm3bylyi/H//939mfBw5cqQkqVmzZg7X3HDDDWrRooWmTp1aIsnqihEjRmjBggVKTU3VnXfeqYKCAn399ddav3692rdv79IY3MH3A74f8P0AQFWSmZmpP//80+FY7dq1Jdni+NChQ9WnTx89/fTTOnnypGbPnq1u3brpu+++M/9tXbVqlX799VfdcccdSk5O1k8//aS5c+fqp59+0vr162WxWHTddddp165dWrJkiZ5//nnzPerUqVPusd95551auHChrr/+et1///3asGGDpk2bph07dmj58uWSbCvIrrzyStWpU0cPPfSQ4uPj9dtvv2nZsmXm66xatUo333yzevbsqaefflqStGPHDv3vf//TmDFjSn3/999/X5J02223uTTezz//XKmpqWratKkef/xxnTp1Si+99JK6du2qLVu2lIj9N954o1q1aqWnnnpKH330kaZMmaKEhAS98soruuKKK/T000/rzTff1AMPPKAOHTrosssuc3j+k08+KYvFogcffFCHDx/WjBkz1KtXL23dulWRkZGSpHfeeUcnT57U3XffrVq1amnjxo166aWX9Pvvv+udd95xeL2CggL16dNH3bp107PPPquoqCinn9OV32dGRoYuueQSnTx5Uv/4xz9Uq1YtLVy4UNdcc43effddDRw40OE1n3rqKQUFBemBBx5QZmampk+friFDhmjDhg0u/e5RxRhANTZ//nxDkvHtt9+Wes2AAQOMsLAw45dffjGPHTx40KhRo4Zx2WWXmcfatGlj9OvXr9TXOXr0qCHJeOaZZ9wa4/fff29IMsaMGePS9StWrDAkGVOmTHE4fv311xsWi8XYs2ePeUySER4ebqSlpZnHXnnlFUOSkZycbGRlZZnHJ0yYYEhyuPbyyy83JBnPPfeceSw3N9do27atkZiYaOTl5RmGYRgFBQVGbm6uw3iOHj1qJCUlGcOHDzePpaWlGZKM2NhY4/Dhww7X28/Nnz/ffP7Zfp+HDx82wsLCjCuvvNIoLCw0j7/88suGJOP1118v8VkWLVrk8FmSk5ONQYMGlfoeAFCdffbZZ0ZwcLARHBxsdOnSxRg/frzx6aefmv/+FxcdHW0MHTq0xPHHHnvMkGTcfPPNJc5dfvnlxuWXX17i+NChQ41GjRqZP3/xxReGJOMf//hHiWutVutZx+AKScZjjz1m/sz3Axu+HwBAYLPfEzt7GIZhHD9+3IiPjzfuuusuh+elp6cbcXFxDsdPnjxZ4vWXLFliSDLWrl1rHnvmmWdKxA5X2b832G3dutWQZNx5550O1z3wwAOGJOOLL74wDMMwli9fftZ7/zFjxhixsbFGQUGBW2MaOHCgIck4evSoS9fb4+Fff/1lHvv++++NoKAg4/bbbzeP2T/ryJEjzWMFBQVG/fr1DYvFYjz11FPm8aNHjxqRkZEO33O+/PJLQ5JRr149h9j99ttvG5KMF154wTzm7O9u2rRphsViMfbu3WseGzp0qCHJeOihh0pcf+b3M1d+n2PHjjUkGV9//bV57Pjx40aTJk2Mxo0bm3Ha/llatWrl8N3hhRdeMCQZP/74Y6nvgaqL9iNAGQoLC/XZZ59pwIABatq0qXm8bt26uuWWW/Tf//5XWVlZkqT4+Hj99NNP2r17t9PXioyMVFhYmNasWaOjR4+6PAb76ztbVuzMxx9/rODgYP3jH/9wOH7//ffLMAx98sknDsd79uzpMBNsrywbNGiQw3vaj//6668Ozw8JCdHf/vY38+ewsDD97W9/0+HDh7V582ZJUnBwsMLCwiTZlocfOXJEBQUFat++vbZs2VLiMwwaNOisM/Wu/D4///xz5eXlaezYsQoKOv3P3V133aXY2NgSS5piYmJ06623OnyWjh07lvjMAACb3r17a926dbrmmmv0/fffa/r06erTp4/q1atnVi256u9//3u5x/Gf//xHFotFjz32WIlzFoul3K9bGr4f8P2A7wcAqpqZM2dq1apVDg/JVm177Ngx3Xzzzfrzzz/NR3BwsDp16uTQMspe9StJOTk5+vPPP9W5c2dJcvrvuid8/PHHkqRx48Y5HL///vslyfw33d4q7cMPP1R+fr7T14qPj1d2drZDizBXuBOTDx06pK1bt2rYsGFKSEgwj1944YXq3bu3+XmKu/POO80/BwcHq3379jIMQyNGjHAY+7nnnus0Nt1+++0OY7v++utVt25dh/cq/neXnZ2tP//8U5dccokMw9B3331X4jXvvvvus35WV36fH3/8sTp27Gi2npNscXfkyJH67bfftH37dofr77jjDvO7gyRdeumlkkp+D0H1QFIbKMMff/yhkydP6txzzy1xrlWrVrJarWYvqsmTJ+vYsWM655xzdMEFF+if//ynfvjhB/P68PBwPf300/rkk0+UlJSkyy67TNOnT1d6enqZY4iNjZUkHT9+3KUx7927VykpKSUCaqtWrczzxZ25ZDkuLk6S1KBBA6fHz7xBTElJKbGZ1znnnCNJDn3RFi5cqAsvvNDsJ1qnTh199NFHyszMLPEZztx12xlXfp/2z3rm319YWJiaNm1a4ndRv379EsmPmjVrupVkAIDqpkOHDlq2bJmOHj2qjRs3asKECTp+/Liuv/76EjciZXHl3/7S/PLLL0pJSXG4OfQmvh+UPM73AwAIbB07dlSvXr0cHpLMSdkrrrhCderUcXh89tlnDhtDHzlyRGPGjFFSUpIiIyNVp04d899uZ/+ue8LevXsVFBSk5s2bOxxPTk5WfHy8+W/65ZdfrkGDBmnSpEmqXbu2rr32Ws2fP9+h7/Y999yjc845R6mpqapfv76GDx+ulStXnnUM7sTk0mKQZIvJf/75p7Kzsx2OO4vJERERZuuW4sedxaYWLVo4/GyxWNS8eXOHeLxv3z4z0R4TE6M6dero8ssvl1Ty7y4kJET169c/yyd17fe5d+/eUn8X9vPFnfm7qFmzpqSS30NQPZDUBjzksssu0y+//KLXX39drVu31muvvaaLLrpIr732mnnN2LFjtWvXLk2bNk0RERGaOHGiWrVq5XTm06558+YKCQkxN2fytODgYLeOG+Xoc/rGG29o2LBhatasmebNm6eVK1dq1apVuuKKK5z2wiw+S1yW8vw+y+LJzwwA1U1YWJg6dOigqVOnavbs2crPzy/Rg7Eszv7tL63KurCwsNzjrGx8Pygd3w8AwP/Z/z3+v//7vxKV3KtWrdJ7771nXjt48GC9+uqr+vvf/65ly5bps88+M5OYzv5d96SzrcyyWCx69913tW7dOo0ePVoHDhzQ8OHDdfHFF+vEiROSbJtcb926Ve+//765D0VqaqqGDh1a5mu3bNlSkio1JnsyNhUWFqp379766KOP9OCDD2rFihVatWqVuQnzmX934eHhDiudSlPe32dZiMkojqQ2UIY6deooKipKO3fuLHHu559/VlBQkEPFUkJCgu644w4tWbJE+/fv14UXXqjHH3/c4XnNmjXT/fffr88++0zbtm1TXl5eiV2Ki4uKitIVV1yhtWvXltih2JlGjRrp4MGDJWaJf/75Z/O8Jx08eLDETPKuXbskyVy2/O6776pp06ZatmyZbrvtNvXp00e9evVSTk5Ohd+/rN+n/bOe+feXl5entLQ0j/8uAAA27du3l2RbYmtXnjYgNWvW1LFjx0ocP7Nqp1mzZjp48KCOHDlS5ut5qhUJ3w/Oju8HAFA12DdVTkxMLFHJ3atXL3Xv3l2SrVJ29erVeuihhzRp0iQNHDhQvXv3dmjTZefJ1mCNGjWS1Wot0eYrIyNDx44dK/FveufOnfXkk09q06ZNevPNN/XTTz9p6dKl5vmwsDD1799fs2bN0i+//KK//e1vWrRokfbs2VPqGPr37y/JNlnrynilkjFIssXk2rVrl1jpVFFn/m4Mw9CePXvMePzjjz9q165deu655/Tggw/q2muvVa9evZSSklLh9z7b77NRo0al/i7s54HSkNQGyhAcHKwrr7xS7733nsPSnIyMDC1evFjdunUzlxr99ddfDs+NiYlR8+bNzeVMJ0+eLHGT1qxZM9WoUcNhyZMzjz32mAzD0G233WbOIhe3efNmLVy4UJJ01VVXqbCwUC+//LLDNc8//7wsFotSU1Nd+/AuKigo0CuvvGL+nJeXp1deeUV16tTRxRdfLOn0bGrx2dMNGzZo3bp15X5fV36fvXr1UlhYmF588UWH9543b54yMzPVr1+/cr8/AED68ssvnVbG2Hs0Fl9OGh0d7TRBXZZmzZrp559/1h9//GEe+/777/W///3P4bpBgwbJMAxNmjSpxGsUH195xuAM3w/Oju8HAFA19OnTR7GxsZo6darTXtT2GO3s33RJmjFjRonn2JO2nojJV111ldP3+fe//y1J5r/pR48eLTG2tm3bSpIZH86M2UFBQbrwwgsdrnGmS5cu6tu3r1577TWtWLGixPm8vDw98MADkmz7b7Rt21YLFy50+Pzbtm3TZ599Zn4eT1q0aJHDpPa7776rQ4cOmbHf2d+dYRh64YUXKvS+rvw+r7rqKm3cuNEh9mdnZ2vu3Llq3LixzjvvvAqNAVVbiK8HAPiD119/3WmvrDFjxmjKlClatWqVunXrpnvuuUchISF65ZVXlJubq+nTp5vXnnfeeerevbsuvvhiJSQkaNOmTXr33Xc1evRoSbbqpJ49e2rw4ME677zzFBISouXLlysjI0M33XRTmeO75JJLNHPmTN1zzz1q2bKlbrvtNrVo0ULHjx/XmjVr9P7772vKlCmSbLPEPXr00L/+9S/99ttvatOmjT777DO99957Gjt2rDnT7ikpKSl6+umn9dtvv+mcc87RW2+9pa1bt2ru3LkKDQ2VJF199dVatmyZBg4cqH79+iktLU1z5szReeed5/Qm3BWu/D7r1KmjCRMmaNKkSerbt6+uueYa7dy5U7NmzVKHDh0cNn0CALjv3nvv1cmTJzVw4EC1bNlSeXl5+uabb/TWW2+pcePGuuOOO8xrL774Yn3++ef697//rZSUFDVp0sTcZLA0w4cP17///W/16dNHI0aM0OHDhzVnzhydf/755qZMktSjRw/ddtttevHFF7V792717dtXVqtVX3/9tXr06GHG4vKMoTR8Pygb3w8AoGqIjY3V7Nmzddttt+miiy7STTfdpDp16mjfvn366KOP1LVrV7388suKjY019zHIz89XvXr19NlnnyktLa3Ea9onN//1r3/ppptuUmhoqPr371+uCuU2bdpo6NChmjt3ro4dO6bLL79cGzdu1MKFCzVgwAD16NFDkm0Ph1mzZmngwIFq1qyZjh8/rldffVWxsbFmIvnOO+/UkSNHdMUVV6h+/frau3evXnrpJbVt29bs8VyaRYsW6corr9R1112n/v37q2fPnoqOjtbu3bu1dOlSHTp0SM8++6wk6ZlnnlFqaqq6dOmiESNG6NSpU3rppZcUFxdXYiWXJyQkJKhbt2664447lJGRoRkzZqh58+a66667JNnapzRr1kwPPPCADhw4oNjYWP3nP/+pcJ9qV36fDz30kJYsWaLU1FT94x//UEJCghYuXKi0tDT95z//canNCaoxA6jG5s+fb0gq9bF//37DMAxjy5YtRp8+fYyYmBgjKirK6NGjh/HNN984vNaUKVOMjh07GvHx8UZkZKTRsmVL48knnzTy8vIMwzCMP//80xg1apTRsmVLIzo62oiLizM6depkvP322y6Pd/PmzcYtt9xipKSkGKGhoUbNmjWNnj17GgsXLjQKCwvN644fP27cd9995nUtWrQwnnnmGcNqtTq8niRj1KhRDsfS0tIMScYzzzzjcPzLL780JBnvvPOOeezyyy83zj//fGPTpk1Gly5djIiICKNRo0bGyy+/7PBcq9VqTJ061WjUqJERHh5utGvXzvjwww+NoUOHGo0aNTrrexc/N3/+fLd/ny+//LLRsmVLIzQ01EhKSjLuvvtu4+jRow7X2D/Lmc4cIwDgtE8++cQYPny40bJlSyMmJsYICwszmjdvbtx7771GRkaGw7U///yzcdlllxmRkZGGJGPo0KGGYRjGY489Zkgy/vjjD6fv8cYbbxhNmzY1wsLCjLZt2xqffvqp03+bCwoKjGeeecZo2bKlERYWZtSpU8dITU01Nm/efNYxuEKS8dhjjzkc4/sB3w8AINDZ74m//fbbMq/78ssvjT59+hhxcXFGRESE0axZM2PYsGHGpk2bzGt+//13Y+DAgUZ8fLwRFxdn3HDDDcbBgwedxtAnnnjCqFevnhEUFGRIMtLS0lwar/17Q3H5+fnGpEmTjCZNmhihoaFGgwYNjAkTJhg5OTnmNVu2bDFuvvlmo2HDhkZ4eLiRmJhoXH311Q7jf/fdd40rr7zSSExMNMLCwoyGDRsaf/vb34xDhw65NLaTJ08azz77rNGhQwfze1GLFi2Me++919izZ4/DtZ9//rnRtWtXIzIy0oiNjTX69+9vbN++3elnPfM70tChQ43o6OgS739mzLLH6CVLlhgTJkwwEhMTjcjISKNfv37G3r17HZ67fft2o1evXkZMTIxRu3Zt46677jK+//57hxhb1nvbzxWPja7+Pn/55Rfj+uuvN+Lj442IiAijY8eOxocffuhwjbPvG4ZR8nsAqheLYdBNHUD5dO/eXX/++ae2bdvm66EAAAA/wfcDAAB8b82aNerRo4feeecdXX/99b4eDuBx1PEDAAAAAAAAAAIGPbUBAABQLRUWFjpsQulMTEyMYmJiKmlEAABUT5mZmTp16lSZ1yQnJ1fSaAAEApLaAAAAqJb279+vJk2alHnNY4895pVNmwAAwGljxozRwoULy7yG7rkAiqOnNgAAAKqlnJwc/fe//y3zmqZNm6pp06aVNCIAAKqn7du36+DBg2Ve06tXr0oaDYBAQFIbAAAAAAAAABAwfLpR5LRp09ShQwfVqFFDiYmJGjBggHbu3OlwTffu3WWxWBwef//73300YgAAAAAAAACAL/m0Urtv37666aab1KFDBxUUFOjhhx/Wtm3btH37dkVHR0uyJbXPOeccTZ482XxeVFSUYmNjXXoPq9WqgwcPqkaNGrJYLF75HACA6sswDB0/flwpKSkKCvLpXHG5rV27Vs8884w2b96sQ4cOafny5RowYIAkKT8/X4888og+/vhj/frrr4qLi1OvXr301FNPKSUlxXyNI0eO6N5779UHH3ygoKAgDRo0SC+88ILLG+wRrwEA3lYVYrY/IGYDALzJ1Xjt040iV65c6fDzggULlJiYqM2bN+uyyy4zj0dFRZV7l9uDBw+qQYMGFRonAABns3//ftWvX9/XwyiX7OxstWnTRsOHD9d1113ncO7kyZPasmWLJk6cqDZt2ujo0aMaM2aMrrnmGm3atMm8bsiQITp06JBWrVql/Px83XHHHRo5cqQWL17s0hiI1wCAyhLIMdsfELMBAJXhbPHar3pq79mzRy1atNCPP/6o1q1bS7JVav/0008yDEPJycnq37+/Jk6cqKioKKevkZubq9zcXPPnzMxMNWzYUPv373e5uhsAAFdlZWWpQYMGOnbsmOLi4nw9nAqzWCwOldrOfPvtt+rYsaP27t2rhg0baseOHTrvvPP07bffqn379pJsE9dXXXWVfv/9d4eK7tJkZmYqPj6eeA0A8JqqFrN9hZgNAPAmV+O1Tyu1i7NarRo7dqy6du1qJrQl6ZZbblGjRo2UkpKiH374QQ8++KB27typZcuWOX2dadOmadKkSSWOx8bGEnABAF5TnZbfZmZmymKxKD4+XpK0bt06xcfHmwltybY7fVBQkDZs2KCBAweWeI0zJ6GPHz8uiXgNAPC+6hSzvcH++yNmAwC86Wzx2m+S2qNGjdK2bdv03//+1+H4yJEjzT9fcMEFqlu3rnr27KlffvlFzZo1K/E6EyZM0Lhx48yf7dl9AABQcTk5OXrwwQd18803mzey6enpSkxMdLguJCRECQkJSk9Pd/o6pU1CAwAAAABwNn6xO8bo0aP14Ycf6ssvvzxrb7NOnTpJsrUqcSY8PNycMWbmGAAAz8nPz9fgwYNlGIZmz55dodeaMGGCMjMzzcf+/fs9NEoAAAAAQFXn00ptwzB07733avny5VqzZo2aNGly1uds3bpVklS3bl0vjw4AANjZE9p79+7VF1984TBpnJycrMOHDztcX1BQoCNHjpS60XN4eLjCw8O9OmYAAAAAQNXk06T2qFGjtHjxYr333nuqUaOGuUQ5Li5OkZGR+uWXX7R48WJdddVVqlWrln744Qfdd999uuyyy3ThhRf6cugAqhnDMFRQUKDCwkJfDwWVLDg4WCEhIdW6/6Y9ob179259+eWXqlWrlsP5Ll266NixY9q8ebMuvvhiSdIXX3whq9VqrrACgMpAvK7eiNkAEBiI19Wbp+K1T5Pa9qXL3bt3dzg+f/58DRs2TGFhYfr88881Y8YMZWdnq0GDBho0aJAeeeQRH4wWQHWVl5enQ4cO6eTJk74eCnwkKipKdevWVVhYmK+H4hUnTpxwaOuVlpamrVu3KiEhQXXr1tX111+vLVu26MMPP1RhYaE5CZ2QkKCwsDC1atVKffv21V133aU5c+YoPz9fo0eP1k033aSUlBRffSwA1QzxGlLVj9kAEOiI15A8E68thmEYHhyT38nKylJcXJwyMzPprw3AbVarVbt371ZwcLDq1KmjsLAwqn+qEcMwlJeXpz/++EOFhYVq0aKFgoIct6OoCnFmzZo16tGjR4njQ4cO1eOPP15qe7Avv/zSnJg+cuSIRo8erQ8++EBBQUEaNGiQXnzxRcXExLg0hqrwewTgO8RrVJeY7Q/4PQIoL+I1PBmvfVqpDQD+Li8vT1arVQ0aNFBUVJSvhwMfiIyMVGhoqPbu3au8vDxFRET4ekge1717d5U1x+3K/HdCQoIWL17syWEBgMuI15CqR8wGgEBGvIbkuXgddPZLAABnzhyieuHvHwACA/9eg/8HAMD/8W81PPH/AP8XAQAAAAAAAAACBkltAAAAAAD8zNq1a9W/f3+lpKTIYrFoxYoVZV6/bNky9e7dW3Xq1FFsbKy6dOmiTz/9tMR1Bw4c0K233qpatWopMjJSF1xwgTZt2uSlTwEAgHeQ1AYAAAAAwM9kZ2erTZs2mjlzpkvXr127Vr1799bHH3+szZs3q0ePHurfv7++++4785qjR4+qa9euCg0N1SeffKLt27frueeeU82aNb31MQAA8AqS2gBQhe3fv1/Dhw9XSkqKwsLC1KhRI40ZM0Z//fWXw3V79uzR8OHD1bBhQ4WHh6tevXrq2bOn3nzzTRUUFPho9AAAVA/EaziTmpqqKVOmaODAgS5dP2PGDI0fP14dOnRQixYtNHXqVLVo0UIffPCBec3TTz+tBg0aaP78+erYsaOaNGmiK6+8Us2aNSv1dXNzc5WVleXwAIDqipjtP0hqA0AV9euvv6p9+/bavXu3lixZoj179mjOnDlavXq1unTpoiNHjkiSNm7cqIsuukg7duzQzJkztW3bNq1Zs0Z33nmnZs+erZ9++snHnwQAgKqLeA1vsVqtOn78uBISEsxj77//vtq3b68bbrhBiYmJateunV599dUyX2fatGmKi4szHw0aNPD20AHALxGz/YvFMAzD14PwpqysLMXFxSkzM1OxsbGV8p4F1gLdtvw2darXSWM7j62U9wTgHTk5OUpLS1OTJk0UEREhw5BOnvTNWKKiJIvF9etTU1O1bds27dq1S5GRkebx9PR0NWvWTLfffrtmzZql888/X1FRUdq4caPTHYgNw5DFnTeugs78/6A4X8SZqshXv8fXtrymj3Z/pCWDligiJOLsTwDgl5z9O+2rmE289q2qGrMtFouWL1+uAQMGuPyc6dOn66mnntLPP/+sxMRESTJ/J+PGjdMNN9ygb7/9VmPGjNGcOXM0dOhQp6+Tm5ur3Nxc8+esrCw1aNCgwr/HW26RfvhBmjlTuvzycr8MgADiT/FaImb7kifidYi3B1kd/XT4Jy3dtlRr964lqQ1UMSdPSjExvnnvEyek6GjXrj1y5Ig+/fRTPfnkkw7BVpKSk5M1ZMgQvfXWWxo5cqR27NihJUuWOA22kqp9sEXVNmP9DP30x0/a8PsGXd6YO2qgKvFVzCZewx8sXrxYkyZN0nvvvWcmtCVb9Xb79u01depUSVK7du20bdu2MpPa4eHhCg8P9/gYf/1V+uknKTPT4y8NIIBwj43yov2IF+Rb8yXZKrYBwBd2794twzDUqlUrp+dbtWqlo0ePateuXZKkc8891zx3+PBhxcTEmI9Zs2ZVypgBXyBmA/Al4jW8YenSpbrzzjv19ttvq1evXg7n6tatq/POO8/hWKtWrbRv377KHKIkKaSoxI7WsgACATHb/1Cp7QWF1kKH/wKoOqKibLO5vnpvd5Wnw1StWrW0detWSVL37t2Vl5fn/hsDAcKM2QYxG6hqfBWzidfwpSVLlmj48OFaunSp+vXrV+J8165dtXPnTodju3btUqNGjSpriCaS2gAk7rFRfiS1vcB+Y8wNMlD1WCyuL0/ypebNm8tisWjHjh0aOHBgifM7duxQzZo11bx5c0nSzp071a5dO0lScHCweTwkhDCBqs2M2UxEA1VOIMRs4jXKcuLECe3Zs8f8OS0tTVu3blVCQoIaNmyoCRMm6MCBA1q0aJEkW8uRoUOH6oUXXlCnTp2Unp4uSYqMjFRcXJwk6b777tMll1yiqVOnavDgwdq4caPmzp2ruXPnVvrnI6kNQAqMeC0Rs/0R7Ue8gEptAL5Wq1Yt9e7dW7NmzdKpU6cczqWnp+vNN9/UjTfeqIsuukgtW7bUs88+K6vV6qPRAr5DpTYAXyJeoyybNm1Su3btzKTIuHHj1K5dOz366KOSpEOHDjm0DZk7d64KCgo0atQo1a1b13yMGTPGvKZDhw5avny5lixZotatW+uJJ57QjBkzNGTIkMr9cCKpDSCwELP9D0ltL7AaVof/AoAvvPzyy8rNzVWfPn20du1a7d+/XytXrlTv3r1Vr149Pfnkk7JYLJo/f7527typrl276v3339fu3bu1fft2zZkzR3/88YeCg4N9/VEAryFmA/A14jVK0717dxmGUeKxYMECSdKCBQu0Zs0a8/o1a9aUeb3d1VdfrR9//FE5OTnasWOH7rrrrsr7UMWQ1AYQaIjZ/oWkthfQfgSAP2jRooU2bdqkpk2bavDgwWrWrJlGjhypHj16aN26dUpISJAkde7cWZs3b9a5556rUaNG6bzzztMll1yiJUuW6Pnnn9fdd9/t408CeA/tRwD4GvEa1RVJbQCBhpjtX2jk4gW0HwHgLxo1alSiOseZc845x6XrgKqG9iMA/AHxGtURSW0AgYiY7T+o1PYCKrUBAAgMVGoDAOAbJLUBABVBUtsL6M8JAEBgIGYDAOAbJLUBABVBUtsLild7cZMMAID/ov0IAAC+QVIbAFARJLW9oPiNMcuZAQDwX7QfAQDAN0hqAwAqgqS2FxS/MabyCwAA/0WlNgAAvkFSGwBQESS1vaB4yxHajwAA4L+o1AYAwDdIagMAKoKkthfQfgQAgMDARpEAAPgGSW0AQEWQ1PYC2o8AAOD/DMMwk9nEawAAKhdJbQBARZDU9gIqtQEA8H/Fq7OJ1wAAVK7QYKtClafCPGIwAMB9JLW9gEptAAD8n8MkNPEaAIBKNer/OilP4Wrxy0pfDwUAEIBIansBG0UCqAqysrI0ceJEnX/++YqMjFStWrXUoUMHTZ8+XUePHi1x/ZIlSxQcHKxRo0aZx7p37y6LxVLqo3v37pX4iQBHxGsAVQUxG4HICKb/CIDqhXjtWSS1vYD2IwAC3ZEjR9S5c2fNnz9fDzzwgDZs2KAtW7boySef1HfffafFixeXeM68efM0fvx4LVmyRDk5OZKkZcuW6dChQzp06JA2btwoSfr888/NY8uWLavUzwUU57CyingNIEARsxGo/gz6VZJ0VD/6eCQA4H3Ea88L8fUAqiLajwBVl2EYOpl/0ifvHRUaJYvF4tK13bt3V+vWrSVJ//d//6fQ0FDdfffdmjx5sp544gm9/fbb2rZtm8Nz2rZtq/79++uJJ57Qww8/rH379mnXrl1KSUkxr2nUqJGuvPJKGYbh8Ny0tDR98803+s9//qMvv/xSy5Yt0y233KKEhATzGnsQrlWrlpKTk8v1OwA8ifYjQNXmq5jtTryWiNmovk4FnZIkFRZm+ngkAHyJe2zidXmR1PYCKrWBqutk/knFTIvxyXufmHBC0WHRLl+/cOFCjRgxQhs3btSmTZs0cuRINWzYUMOHD9ekSZP07bffqkOHDpKk7777Tj/88IOWLVsmq9Wqt956S7feeqtDsC3uzMA/f/589evXT3Fxcbr11ls1b9483XLLLeX/sEAloFIbqNp8FbPdjdcSMRvVkzXItnDcKMzz8UgA+BL32DbEa/fRfsQL6NEJwB80aNBAzz//vM4991wNGTJE9957r55//nnVr19fffr00fz5881r58+fr8svv1xNmzbVH3/8oWPHjuncc891eL2LL75YMTExiomJ0c0332wet1qtWrBggW699VZJ0k033aT//ve/SktLq5wPCpQT8RqAvyBmozoqDA6WJFmsJLUBBAbitX+hUtsLaD8CVF1RoVE6MeGEz97bHZ07d3aY7e3SpYuee+45FRYW6q677tLw4cP173//W0FBQVq8eLGef/75Ml9v+fLlysvL04MPPqhTp06Zx1etWqXs7GxdddVVkqTatWurd+/eev311/XEE0+4NWagMtF+BKjafBWz3Y3XEjEb1ZO9UluF+b4dCACf4h6beF1eJLW9gPYjQNVlsVjcXlLsj/r376/w8HAtX75cYWFhys/P1/XXXy9JqlOnjuLj47Vz506H5zRs2FCSVKNGDR07dsw8Pm/ePB05ckSRkZHmMavVqh9++EGTJk1SUBCLguCfaD8CVG3EbGI2/JuVSm0AIl5LxOvyIqntBVRqA/AHGzZscPh5/fr1atGihYKLbiCGDh2q+fPnKywsTDfddJMZMIOCgjR48GC98cYbevTRR0vt+SVJf/31l9577z0tXbpU559/vnm8sLBQ3bp102effaa+fft64dMBFUelNgB/QcxGdWRPalOpDSBQEK/9C0ltL6BHJwB/sG/fPo0bN05/+9vftGXLFr300kt67rnnzPN33nmnWrVqJUn63//+5/DcqVOnas2aNerYsaMmT56s9u3bKzo6Wj/88IPWrVvnsOtzrVq1NHjw4BIbW1x11VWaN28eARd+i3gNwF8Qs1Ed2duPBJPUBhAgiNf+haS2F9B+BIA/uP3223Xq1Cl17NhRwcHBGjNmjEaOHGmeb9GihS655BIdOXJEnTp1cnhurVq1tHHjRj399NN65plnlJaWpqCgILVo0UI33nijxo4dK0l6/fXXNXDgwBLBVpIGDRqk2267TX/++adq167t1c8KlAftRwD4C2I2qqPT7UdIagMIDMRr/0JS2wtoPwLAH4SGhmrGjBmaPXu20/OGYejgwYO65557nJ6Pi4vT1KlTNXXq1FLf44cffij13ODBgzV48GDz58aNG8swDBdHD3gf7UcA+AtiNqoja4gtHRFEUhtAgCBe+xeS2l5ApTYAf/fHH39o6dKlSk9P1x133OHr4QA+QaU2gEBAzEZVZVCpDaAKIV5XPpLaXkCPTgD+LjExUbVr19bcuXNVs2ZNXw8H8AniNYBAQMxGVWUEU6kNoOogXlc+ktpeQPsRAL62Zs2aMs9X5yVKgB3tRwD4A2I2qi0zqV3g44EAwNkRr/1PkK8HUBXRfgQAAP9H+xEAAHzHKOqpHWyQ1AYAuI+kthdQqQ0AgP+jUhsAAB8KsfXUplIbAFAeJLW9gB6dAAD4P+I1AAA+FBIqiUptAED5kNT2AtqPAADg/1hZBQCAD4XQUxsAUH4ktb2Am2QAAPwfk9AAAPiQ2VObGAwAcB9JbS/gJhkAAP/HJDQAAL5jKWo/EkL7EQBAOZDU9gJ6dAKATffu3TV27FhfDwNwingNADbEa/iCJbSopzbtRwDAZcTs00hqewGVXwD8RXp6usaMGaPmzZsrIiJCSUlJ6tq1q2bPnq2TJ0/6eniSpO+//17XXHONEhMTFRERocaNG+vGG2/U4cOHS1w7bdo0BQcH65lnnnH6Wq5+3saNG8tisZR4PPXUU177nPA/rKwC4C+I18Tr6shManPPDCCAELP9J2aHeO2VqzFukgH4g19//VVdu3ZVfHy8pk6dqgsuuEDh4eH68ccfNXfuXNWrV0/XXHNNiefl5+crtOgmw9v++OMP9ezZU1dffbU+/fRTxcfH67ffftP777+v7OzsEte//vrrGj9+vF5//XX985//dDjn7uedPHmy7rrrLofXqFGjhnc+KPwSk9AA/AHxmnhdXdmT2iGslgIQIIjZ/hWzSWp7gUNSm5tkoGoxDMlXs69RUZLF4vLl99xzj0JCQrRp0yZFR0ebx5s2baprr71WhmFIkiwWi2bNmqVPPvlEq1ev1j//+U9NnDhRI0eO1BdffKH09HQ1bNhQ99xzj8aMGWO+zrBhw3Ts2DG1a9dOL7/8snJzc3XLLbfoxRdfVFhYmHmd1WrV+PHj9dprryksLEx///vf9fjjj0uS/ve//ykzM1OvvfaaQoo2C2rSpIl69OhR4vN89dVXOnXqlCZPnqxFixbpm2++0SWXXOL257WrUaOGkpOTXf59ouphEhqo4nwVs4nXxGu4JIhKbQAS99jE7HIjqe0F9OgEqrCTJ6WYGN+894kTUrFAUpa//vpLn332maZOneoQfIqzFAvejz/+uJ566inNmDFDISEhslqtql+/vt555x3VqlVL33zzjUaOHKm6detq8ODB5vNWr16tiIgIrVmzRr/99pvuuOMO1apVS08++aR5zcKFCzVu3Dht2LBB69at07Bhw9S1a1f17t1bycnJKigo0PLly3X99dc7jOlM8+bN080336zQ0FDdfPPNmjdvnhlw3f28gES8Bqo8X8Vs4jXxGi4JKkrQhJDUBqo37rGJ2eVlVHGZmZmGJCMzM7PS3vPGd2409LgMPS5j0dZFlfa+ADzv1KlTxvbt241Tp07ZDpw4YRi2ueTKf5w44fK4169fb0gyli1b5nC8Vq1aRnR0tBEdHW2MHz/eMAzDkGSMHTv2rK85atQoY9CgQebPQ4cONRISEozs7Gzz2OzZs42YmBijsLDQMAzDuPzyy41u3bo5vE6HDh2MBx980Pz54YcfNkJCQoyEhASjb9++xvTp04309HSH52RmZhqRkZHG1q1bDcMwjO+++86IiYkxjh8/7vbnNQzDaNSokREWFmaesz/Wrl3r9LOX+P/gjLFVdpypinzxe3z/5/fNeN33jb6V9r4APM/pv9O+itnEa5/Fa8MgZlcGT/0eP39wpGFIxof1Ezw0MgD+zq/iNTE74O+xqdT2AtqPAFVYVJRtNtdX711BGzdulNVq1ZAhQ5Sbm2seb9++fYlrZ86cqddff1379u3TqVOnlJeXp7Zt2zpc06ZNG0UVG1eXLl104sQJ7d+/X40aNZIkXXjhhQ7PqVu3rsMGFU8++aTGjRunL774Qhs2bNCcOXM0depUrV27VhdccIEkacmSJWrWrJnatGkjSWrbtq0aNWqkt956SyNGjHD780rSP//5Tw0bNszhWL169Up9LVQ9tB8BqjhfxWziNfEaLgm2V2pbWS0FVGvcYztcQ8x2HUltL3DYeIqbZKBqsVhcXp7kS82bN5fFYtHOnTsdjjdt2lSSFBkZ6XD8zOVES5cu1QMPPKDnnntOXbp0UY0aNfTMM89ow4YNbo/lzA0xLBaLrGfcvNSqVUs33HCDbrjhBk2dOlXt2rXTs88+q4ULF0qyLYv66aefzJ5gkq2P2Ouvv64RI0a4/XklqXbt2mrevLnbnwdVBxtFAlVcAMRs4nXZn1ciXldlwaFFSW1ZZbVKQUE+HhAA3wiAeC0Rs8/2eaXKj9kktb2AHp0AfK1WrVrq3bu3Xn75Zd17772l9sAqzf/+9z9dcskluueee8xjv/zyS4nrvv/+e506dcoMaOvXr1dMTIwaNGhQ7rGHhYWpWbNm5s7MP/74ozZt2qQ1a9YoISHBvO7IkSPq3r27fv75Z7Vs2bJCnxfVU/EYzSQ0AF8gXhOvq7PgcHtPbasKCqRie6ABgN8hZvtfzGYu1AtoPwLAH8yaNUsFBQVq37693nrrLe3YsUM7d+7UG2+8oZ9//lnBwcGlPrdFixbatGmTPv30U+3atUsTJ07Ut99+W+K6vLw8jRgxQtu3b9fHH3+sxx57TKNHj1aQi6U2H374oW699VZ9+OGH2rVrl3bu3Klnn31WH3/8sa699lpJthnkjh076rLLLlPr1q3Nx2WXXaYOHTpo3rx55fq8x48fV3p6usMjKyvL1V9vlbJ27Vr1799fKSkpslgsWrFihcN5wzD06KOPqm7duoqMjFSvXr20e/duh2uOHDmiIUOGKDY2VvHx8RoxYoRO+GoZoYuKx2gmoQH4CvGaeF1dhYSfbj9SUODjwQCAC4jZ/hWzSWp7Ae1HAPiDZs2a6bvvvlOvXr00YcIEtWnTRu3bt9dLL72kBx54QE888USpz/3b3/6m6667TjfeeKM6deqkv/76y2FG2a5nz55q0aKFLrvsMt1444265ppr9Pjjj7s8xvPOO09RUVG6//771bZtW3Xu3Flvv/22XnvtNd12223Ky8vTG2+8oUGDBjl9/qBBg7Ro0SLl5+e7/XntSdrij/Hjx7s89qokOztbbdq00cyZM52enz59ul588UXNmTNHGzZsUHR0tPr06aOcnBzzmiFDhuinn37SqlWr9OGHH2rt2rUaOXJkZX2EcqH9CAB/QLwmXldXIUWl2aEGSW0AgYGY7V8x22IYhuG1V/cDWVlZiouLU2ZmpmJjYyvlPfu80Uef/fKZJOmFvi/oH53+USnvC8DzcnJylJaWpiZNmigiIsLXw/Erw4YN07Fjx0pU9VZFZf1/4Is4400Wi0XLly/XgAEDJNmqtFNSUnT//ffrgQcekCRlZmYqKSlJCxYs0E033aQdO3bovPPO07fffmtuiLJy5UpdddVV+v3335WSklLifXJzcx02FsnKylKDBg0q9fe46PtFGrpiqCSpU71OWn/n+kp5XwCeR7wuXXWK11L1itm+4qnf4w8LntOFdzygLXXC1fjnHBVbAQ+giiJel606xWxPxGsqtb2AntoAgKoiLS1N6enp6tWrl3ksLi5OnTp10rp16yRJ69atU3x8vMMO37169VJQUFCpG59MmzZNcXFx5qMiPeLKi0ptAAB8x16pHUKlNgCgHEhqewHtRwAAVUV6erokKSkpyeF4UlKSeS49PV2JiYkO50NCQpSQkGBec6YJEyYoMzPTfOzfv98Loy8bk9AAAPhOSHi47b9Wg6Q2AMBtIb4eQFXERpEAqoMFCxb4eggIYOHh4Qovupn1FYd4zSQ0gCqKeA1/FRxalNQ2SGoDgETMdheV2l5ApTYAoKpITk6WJGVkZDgcz8jIMM8lJyfr8OHDDucLCgp05MgR8xp/RPsRAAB8JziMSm0AQPmR1PYCljMDVU8V31MXZ1Gd//6bNGmi5ORkrV692jyWlZWlDRs2qEuXLpKkLl266NixY9q8ebN5zRdffCGr1apOnTpV+phdRaU2UPVU53+vYcP/A4EjJNS2MRiV2kD1w7/V8MT/A7Qf8QLajwBVR2hoqCTp5MmTioyM9PFo4CsnT56UdPr/h6rmxIkT2rNnj/lzWlqatm7dqoSEBDVs2FBjx47VlClT1KJFCzVp0kQTJ05USkqKBgwYIElq1aqV+vbtq7vuuktz5sxRfn6+Ro8erZtuukkpKSk++lRnxyQ0UHUQr2FX1WN2VRJk3yiSSm2g2iBew84T8ZqkthfQfgSoOoKDgxUfH2+2VoiKipLFYvHxqFBZDMPQyZMndfjwYcXHxys4ONjXQ/KKTZs2qUePHubP48aNkyQNHTpUCxYs0Pjx45Wdna2RI0fq2LFj6tatm1auXKmIiAjzOW+++aZGjx6tnj17KigoSIMGDdKLL75Y6Z/FHbQfAaoO4jWqS8yuSkLCiiq1rSKpDVQTxGt4Ml6T1PYCKrWBqsXeE/jMnsGoPuLj4/26N3RFde/evczlXxaLRZMnT9bkyZNLvSYhIUGLFy/2xvC8hvYjQNVCvIZU9WN2VWK2H7FKeflW0R0VqB6I15A8E69JansBy5mBqsVisahu3bpKTExUfn6+r4eDShYaGkq1VxVFpTZQtRCvQcwOLCHhp5PaOXn5ksJ9OyAAlYJ4DU/Fa5LaXkD7EaBqCg4O5kYJqEKYhAaqJuI1EBjsldqhViknn6Q2UN0Qr1FRrO/xAtqPAADg/2g/AgCA74RE2DaJC7FKuVRrAgDcRFLbC6jUBgDA/9F+BAAA37FvFBlqlXLzSGoDANzj06T2tGnT1KFDB9WoUUOJiYkaMGCAdu7c6XBNTk6ORo0apVq1aikmJkaDBg1SRkaGj0bsmuI3xixnBgDAP1GpDQCA7wSFhpl/zsvL9eFIAACByKdJ7a+++kqjRo3S+vXrtWrVKuXn5+vKK69Udna2ec19992nDz74QO+8846++uorHTx4UNddd50PR312xRPZVH4BAOCf6KkNAIAPhZze4isv95QPBwIACEQ+3Shy5cqVDj8vWLBAiYmJ2rx5sy677DJlZmZq3rx5Wrx4sa644gpJ0vz589WqVSutX79enTt39sWwz4r2IwAA+D/ajwAA4EPFktr5uTk+HAgAIBD5VU/tzMxMSVJCQoIkafPmzcrPz1evXr3Ma1q2bKmGDRtq3bp1Tl8jNzdXWVlZDo/KxkaRAAD4P9qPAADgQyS1AQAV4DdJbavVqrFjx6pr165q3bq1JCk9PV1hYWGKj493uDYpKUnp6elOX2fatGmKi4szHw0aNPD20EsofmPMcmYAAPwTldoAAPhQ8aR2HkltAIB7/CapPWrUKG3btk1Lly6t0OtMmDBBmZmZ5mP//v0eGqHr6KkNAID/o6c2AAA+FHQ6HVFIpTYAwE0+7altN3r0aH344Ydau3at6tevbx5PTk5WXl6ejh075lCtnZGRoeTkZKevFR4ervDwcG8PuUwsZwYAwP8RrwEA8CGLRflBUqhVKsjL9fVoAAABxqeV2oZhaPTo0Vq+fLm++OILNWnSxOH8xRdfrNDQUK1evdo8tnPnTu3bt09dunSp7OG6jOXMAAD4P+I1AAC+VRBkkSQV5pPUBgC4x6eV2qNGjdLixYv13nvvqUaNGmaf7Li4OEVGRiouLk4jRozQuHHjlJCQoNjYWN17773q0qWLOnfu7Muhl4nKLwAA/N+ZiWyrYVWQxW86swEAUOXZktqGCumpDQBwk0+T2rNnz5Ykde/e3eH4/PnzNWzYMEnS888/r6CgIA0aNEi5ubnq06ePZs2aVckjdQ89OgEA8H9nxmiS2gAAVC57pbY1n6Q2AMA9Pk1qG4Zx1msiIiI0c+ZMzZw5sxJG5BksZwYAwP+duZqq0FqokCC/2G4EAIBqgfYjAIDyohzJC2g/AgCA/ztz4pmJaAAAKlehmdTO8/FIAACBhqS2F1CpDQCA/yuR1GYiGgCASlUQZEtJGAVUagMA3ENS28MMw5Ch021V6KkNAIB/ctZTGwAAVJ5Ci61S2yigUhsA4B6S2h525g0xVV8AAPinEj21WV0FAEClKiyq1LbSfgQA4CaS2h5Gf04AAAID7UcAAP5s7dq16t+/v1JSUmSxWLRixYoyr1+2bJl69+6tOnXqKDY2Vl26dNGnn35a6vVPPfWULBaLxo4d69mBu8Ge1DYKaT8CAHAPSW0PK1H1xQ0yAAB+iUptAIA/y87OVps2bTRz5kyXrl+7dq169+6tjz/+WJs3b1aPHj3Uv39/fffddyWu/fbbb/XKK6/owgsv9PSw3WJPaov2IwAAN4X4egBVDf05AQAIDLQMAwD4s9TUVKWmprp8/YwZMxx+njp1qt577z198MEHateunXn8xIkTGjJkiF599VVNmTLFU8MtF5LaAIDyolLbw2g/AgBAYDgzRjMRDQCoSqxWq44fP66EhASH46NGjVK/fv3Uq1cvl14nNzdXWVlZDg+PjTHY3n4k32OvCQCoHqjU9jDajwAAEBhoPwIAqMqeffZZnThxQoMHDzaPLV26VFu2bNG3337r8utMmzZNkyZN8sYQVRAUbPtDIZXaAAD3UKntYVRqAwAQGNgoEgBQVS1evFiTJk3S22+/rcTEREnS/v37NWbMGL355puKiIhw+bUmTJigzMxM87F//36PjdMabEtqW6jUBgC4iUptD6OnNgAAgaFET20mogEAVcDSpUt155136p133nFoMbJ582YdPnxYF110kXmssLBQa9eu1csvv6zc3FwFFyWZiwsPD1d4eLhXxmoNsie1qdQGALiHpLaH0X4EAIDAcGaMZiIaABDolixZouHDh2vp0qXq16+fw7mePXvqxx9/dDh2xx13qGXLlnrwwQedJrS9jUptAEB5kdT2MNqPAAAQGGg/AgDwZydOnNCePXvMn9PS0rR161YlJCSoYcOGmjBhgg4cOKBFixZJsrUcGTp0qF544QV16tRJ6enpkqTIyEjFxcWpRo0aat26tcN7REdHq1atWiWOVxar2VObpDYAwD301PYwKrUBAAgMbBQJAPBnmzZtUrt27dSuXTtJ0rhx49SuXTs9+uijkqRDhw5p37595vVz585VQUGBRo0apbp165qPMWPG+GT8rrBXagdZSWoDANxDpbaH0VMbAIDAQKU2AMCfde/eXYZhlHp+wYIFDj+vWbPG7fcoz3M8yQi2pSQs1gKfjgMAEHio1PYw2o8AABAYmIgGAMC3DCq1AQDlRFLbw2g/AgBAYKD9CAAAvkWlNgCgvEhqexiV2gAABAbajwAA4Fv2pHZQIUltAIB7SGp7GEuZAQAIDFRqAwDgW2ZS2yCpDQBwD0ltD6P9CAAAgYGJaAAAfMsIsSW1g2k/AgBwE0ltD6P9CAAAgYH2IwAA+FhIqCSS2gAA95HU9jAqtQEACAy0HwEAwMeo1AYAlBNJbQ9jKTMAAIGBSm0AAHzLYq/UZmIZAOAmktoeRvsRAAACAxPRAAD4mNl+hPtmAIB7SGp7mL3KK8gS5PAzAADwLyViNhPRAABUKkuoLakdRAwGALiJpLaH2W+IQ4NCHX4GAAD+pUTMZiIaAIBKRfsRAEB5kdT2MPvS5bDgMIefAQCAf7Ense0xm4loAAAqV1BRpXYIE8sAADeR1PawEjfIBGcAAPwSE9EAAPiWJdQWg6nUBgC4i6S2h5lLmYNpPwIAgD8rEbOZiAYAoFKZldpMLAMA3ERS28PsN8T2/pxWwyrDMHw5JAAA4MSZMZuJaAAAKldwWFGlNhPLAAA3kdT2sDOXMkuSIZLaAAD4G3sSm5ZhAAD4RlBYuCQphEIwAICbSGp72Jk3yBI3yQAA+CN6agMA4Fv2Sm02igQAuIuktoeZS5mL+nNKLGcGAMAfnRmzidcAAFQuM6nNxDIAwE0ktT3M3HQqqFhSm1lnAAD8zpkxm3gNAEDlOp3Upv0IAMA9JLU9zH5DXLz9CMuZAQDwP2fGbCq1AQCoXKHhRT21rdwzAwDcQ1Lbw5xtFMlNMgAA/sUwDHMjZzaKBADAN0LCqdQGAJQPSW0PM5cyB9N+BAAAf1V8FZU9ZrOyCgCAyhVir9Q2DBUWktgGALiOpLaH2RPYIUEhp49RqQ0AgF8pHpvNntrEawAAKlWY2X5EystnchkA4DqS2h5mvyEOCQqRRRZJVH4BAAJbYWGhJk6cqCZNmigyMlLNmjXTE088IaPYUmHDMPToo4+qbt26ioyMVK9evbR7924fjrpsxVdR0X4EAADfCI2IkGRLap/Ky/fxaAAAgYSktofZE9jBlmAFBwVL4iYZABDYnn76ac2ePVsvv/yyduzYoaefflrTp0/XSy+9ZF4zffp0vfjii5ozZ442bNig6Oho9enTRzk5OT4ceemKTzizUSQAAL4RFnG6UpukNgDAHSFnvwTusCewg4OCFWwJVoEKuEkGAAS0b775Rtdee6369esnSWrcuLGWLFmijRs3SrJVac+YMUOPPPKIrr32WknSokWLlJSUpBUrVuimm27y2dhL49B+hJ7aAAD4RFjk6UrtHJLaAAA3UKntYfab5CBLkIIstl8vldoAgEB2ySWXaPXq1dq1a5ck6fvvv9d///tfpaamSpLS0tKUnp6uXr16mc+Ji4tTp06dtG7dOqevmZubq6ysLIdHZSoem82e2sRrAAAqVSiV2gCAcqJS28PMSu1i7Ueo/AIABLKHHnpIWVlZatmypYKDg1VYWKgnn3xSQ4YMkSSlp6dLkpKSkhyel5SUZJ4707Rp0zRp0iTvDrwMxSu1aT8CAIBvBIXZJpZDC6VsktoAADdQqe1hZk/tovYjEjfJAIDA9vbbb+vNN9/U4sWLtWXLFi1cuFDPPvusFi5cWO7XnDBhgjIzM83H/v37PTjisys+4RwSZJvjp1IbAIBKFmKLwSFWKSefpDYAwHVUanuYPYHNRpEAgKrin//8px566CGzN/YFF1ygvXv3atq0aRo6dKiSk5MlSRkZGapbt675vIyMDLVt29bpa4aHhys8PNzrYy+Nw8oqCyurAADwiWJJ7dz8Ah8PBgAQSKjU9jD7TbJDT20qtQEAAezkyZMKCnL8yhAcHCyr1ZYEbtKkiZKTk7V69WrzfFZWljZs2KAuXbpU6lhd5XQPDOI1AACVyyGpTaU2AMB1VGp7mEOlNpVfAIAqoH///nryySfVsGFDnX/++fruu+/073//W8OHD5ckWSwWjR07VlOmTFGLFi3UpEkTTZw4USkpKRowYIBvB18Ks1I7iJVVAAD4DO1HAADlRFLbwxx6anOTDACoAl566SVNnDhR99xzjw4fPqyUlBT97W9/06OPPmpeM378eGVnZ2vkyJE6duyYunXrppUrVyoiIsKHIy+dGa8t7IEBAIDPUKkNACgnktoe5qxHJzfJAIBAVqNGDc2YMUMzZswo9RqLxaLJkydr8uTJlTewCjBXVhWbhGZlFQAAlYykNgCgnOip7WHObpKp1AYAwL843QODeA0AQOUqntQuIKkNAHAdSW0PY6NIAAD8n7M9MIjXAABUMiq1AQDlRFLbw5z16GQ5MwAA/oWNIgEA8AOhoZKkEEPKI6kNAHADSW0Po/0IAAD+j0loAAD8QMjpbb7y8nJ9OBAAQKAhqe1hbBQJAID/czoJTbwGAKByFUtqF+ae8uFAAACBhqS2h9lviNl4CgAA/8UeGAAA+IFiSe0CKrUBAG4gqe1h5nLmYpVfLGcGAMC/ON0okkloAAAqF0ltAEA5kdT2MNqPAADg/5iEBgDADwQHm38szM/x4UAAAIGGpLaHsVEkAAD+j0loAAD8QFCQCi22PxZSqQ0AcANJbQ+jRycAAP6PPTAAAPAPBUVx2Jqf5+ORAAACCUltDzOXMxer/GI5MwAA/sWs1C6+sopJaAAAKl1hkK1Um6Q2AMAdJLU9jPYjAAD4PyahAQDwDwVB9kpt2o8AAFxHUtvDzKQ2PToBAPBbTEIDAOAfCu1J7QIqtQEAriOp7WFOe2pzkwwAgF9hDwwAAPyDvVLboP0IAMANJLU9zFzOXKzyi+XMAAD4F6crq5iEBgCg0tkrtY0C2o8AAFxHUtvDaD8CAID/YxIaAAD/UGixJ7Wp1AYAuI6ktofZq7zo0QkAgP8y4zWT0AAA+FRh0X2zCvN9OxAAQEAhqe1h9htienQCAOC/nMZrJqEBAKh09vYjKiCpDQBwXYWS2jk5OZ4aR5VhLmcuVvnFcmYAgDcRj93ndGUVk9AAAA8hNrvOaq/Upv0IAMANbie1rVarnnjiCdWrV08xMTH69ddfJUkTJ07UvHnzPD7AQEP7EQBAZSAeV4yzSWjiNQCgIojN5UP7EQBAebid1J4yZYoWLFig6dOnKywszDzeunVrvfbaax4dXCBio0gAQGUgHleMGa/ZKBIA4CHE5vKxBpPUBgC4z+2k9qJFizR37lwNGTJEwfbgI6lNmzb6+eef3XqttWvXqn///kpJSZHFYtGKFSsczg8bNkwWi8Xh0bdvX3eHXKnsVV706AQAeJMn43F15DReMwkNAKgAYnP52NuPWKwktQEArnM7qX3gwAE1b968xHGr1ar8fPeCUHZ2ttq0aaOZM2eWek3fvn116NAh87FkyRJ3h1ypzOXMVH4BALzIk/G4OnK6sopJaABABRCby8dMarNRJADADSHuPuG8887T119/rUaNGjkcf/fdd9WuXTu3Xis1NVWpqallXhMeHq7k5GR3h+kztB8BAFQGT8bj6sjZJDTxGgBQEcTm8ikMtqUlgqwFPh4JACCQuJ3UfvTRRzV06FAdOHBAVqtVy5Yt086dO7Vo0SJ9+OGHHh/gmjVrlJiYqJo1a+qKK67QlClTVKtWrVKvz83NVW5urvlzVlaWx8dUFoeNIqn8AgB4SWXH46rGjNfFJqFZWQUAqAhic/kYRUlt0X4EAOAGt9uPXHvttfrggw/0+eefKzo6Wo8++qh27NihDz74QL179/bo4Pr27atFixZp9erVevrpp/XVV18pNTVVhYWlJ4mnTZumuLg489GgQQOPjuls7FVe9OgEAHhTZcbjqshpvGYSGgBQAcTm8rEGFVVqF1KpDQBwnduV2pJ06aWXatWqVZ4eSwk33XST+ecLLrhAF154oZo1a6Y1a9aoZ8+eTp8zYcIEjRs3zvw5KyurUhPb5nJmCz21AQDeVVnxuCpyWFlF+xEAgIcQm91nr9QOJqkNAHCD25XavtS0aVPVrl1be/bsKfWa8PBwxcbGOjwqE+1HAADwfw6T0MRrAAB8xp7Uthi0HwEAuM7tSu2goCBZLJZSz5fVGqSifv/9d/3111+qW7eu196johw2iqTyCwDgJb6Mx1WBGa+DWFkFAPAMYnM52Su1mVwGALjB7aT28uXLHX7Oz8/Xd999p4ULF2rSpEluvdaJEyccqq7T0tK0detWJSQkKCEhQZMmTdKgQYOUnJysX375RePHj1fz5s3Vp08fd4ddaexVXvToBAB4kyfjcXXkNF4zCQ0AqABic/kYIaGSpCAr7UcAAK5zO6l97bXXljh2/fXX6/zzz9dbb72lESNGuPxamzZtUo8ePcyf7b2whw4dqtmzZ+uHH37QwoULdezYMaWkpOjKK6/UE088ofDwcHeHXWnM5czF2o9Q+QUA8DRPxuPqyGFlFe1HAAAeQGwuJyq1AQDlUK6NIp3p3LmzRo4c6dZzunfvLsMwSj3/6aefVnRYlY72IwAAXypPPK6OnG3sTLwGAHgDsfksgqnUBgC4zyMbRZ46dUovvvii6tWr54mXC2hsFAkA8BXiseucxWtWVgEAPI3Y7IKi9iNUagMA3OF2pXbNmjUdNr8wDEPHjx9XVFSU3njjDY8OLhAVr9SmRycAwFuIxxVjj83sgQEA8BRic/lY7Elt7psBAG5wO6n9/PPPOwTqoKAg1alTR506dVLNmjU9OrhAVHzjKftyZiq/AACeRjyuGLNSm/YjAAAPITaXjxEaJolKbQCAe9xOag8bNswLw6g6nG0USeUXAMDTiMcVQ7wGAHgasbl8qNQGAJSHS0ntH374weUXvPDCC8s9mKqAjSIBAN5CPPYcZ/GalVUAAHcRmysuKNTeU5s4DABwnUtJ7bZt28piscgwjDKvs1gsKiys3glcNooEAHgL8dhzircLYw8MAEB5eTM2r127Vs8884w2b96sQ4cOafny5RowYECp1y9btkyzZ8/W1q1blZubq/PPP1+PP/64+vTpY14zbdo0LVu2TD///LMiIyN1ySWX6Omnn9a5557r1tg8yRJiaz8SQlIbAOAGl5LaaWlp3h5HleFs4ymrCM4AgIojHnuOWanNJDQAoAK8GZuzs7PVpk0bDR8+XNddd91Zr1+7dq169+6tqVOnKj4+XvPnz1f//v21YcMGtWvXTpL01VdfadSoUerQoYMKCgr08MMP68orr9T27dsVHR3ttc9SlqCintohTC4DANzgUlK7UaNG3h5HlWH26CzefoSbZACABxCPPcdpvOZmGgDgJm/G5tTUVKWmprp8/YwZMxx+njp1qt577z198MEHZlJ75cqVDtcsWLBAiYmJ2rx5sy677LIKj7k87EntYNqAAQDc4PZGkXbbt2/Xvn37lJeX53D8mmuuqfCgApnT9iPcJAMAvIR4XD7O4jU9tQEAnuAvsdlqter48eNKSEgo9ZrMzExJKvOa3Nxc5ebmmj9nZWV5bpCSgsPs7UcMGYYhi8Xi0dcHAFRNbie1f/31Vw0cOFA//vijQ+8we+Cp7j08nW4USaU2AMDDiMcV4yxeS7bEtr19GAAA7vC32Pzss8/qxIkTGjx4sNPzVqtVY8eOVdeuXdW6detSX2fatGmaNGmSt4Z5uv2IVSqwFig0ONRr7wUAqDrcvmsbM2aMmjRposOHDysqKko//fST1q5dq/bt22vNmjVeGGJgcbbxFJVfAABPIx5XjLN4Xfw4AADu8qfYvHjxYk2aNElvv/22EhMTnV4zatQobdu2TUuXLi3ztSZMmKDMzEzzsX//fo+ONTgsXJItqZ1vzffoawMAqi63K7XXrVunL774QrVr11ZQUJCCgoLUrVs3TZs2Tf/4xz/03XffeWOcAcPs0Un7EQCAFxGPK8bZRpH246GiQgwA4D5/ic1Lly7VnXfeqXfeeUe9evVyes3o0aP14Ycfau3atapfv36ZrxceHq7w8HBvDFVS8fYjUn5hvgjDAABXuF2pXVhYqBo1akiSateurYMHD0qybZCxc+dOz44uANF+BABQGYjHFeNso8jixwEAcJc/xOYlS5bojjvu0JIlS9SvX78S5w3D0OjRo7V8+XJ98cUXatKkSaWMqywh4RG2/xa1HwEAwBVuV2q3bt1a33//vZo0aaJOnTpp+vTpCgsL09y5c9W0aVNvjDGgsFEkAKAyEI8rptRKbSaiAQDl5OnYfOLECe3Zs8f8OS0tTVu3blVCQoIaNmyoCRMm6MCBA1q0aJEkW8uRoUOH6oUXXlCnTp2Unp4uSYqMjFRcXJwkW8uRxYsX67333lONGjXMa+Li4hQZGVnRX0G5hIQXq9Sm/QgAwEVuV2o/8sgjslptVUyTJ09WWlqaLr30Un388cd68cUXPT7AQGIYhgzZNgOhpzYAwJuIxxVTak9tJqIBAOXk6di8adMmtWvXTu3atZMkjRs3Tu3atdOjjz4qSTp06JD27dtnXj937lwVFBRo1KhRqlu3rvkYM2aMec3s2bOVmZmp7t27O1zz1ltvVeSjV4gl1FZrZ7YfAQDABS5Xardv31533nmnbrnlFsXGxkqSmjdvrp9//llHjhxRzZo1zV2dq6viyWvajwAAvIF47BnO2oVJxGwAgPu8FZu7d+8uwzBKPb9gwQKHn13ZjLKs1/OVoLBiSW0qtQEALnK5UrtNmzYaP3686tatq9tvv90hYCYkJHADLcfqLtqPAAC8gXjsGc42di5+HAAAVxGbK8ae1A6lUhsA4AaXk9rz5s1Tenq6Zs6cqX379qlnz55q3ry5pk6dqgMHDnhzjAGjeHUXldoAAG8gHnuGuQeGJVgWi0UW2RIOTEQDANxFbK6YoHAqtQEA7nOrp3ZUVJSGDRumNWvWaNeuXbrpppv0yiuvqHHjxurXr5+WLVvmrXEGhOI3wsV7dHKDDADwJOJxxdljsz1WmzGbiWgAQDkQm8svOIye2gAA97m9UaRds2bNNGXKFP32229asmSJ1q9frxtuuMGTYws4Dj21iy1nZikzAMBbiMflY1ZqF62qMldXMRENAKggYrN7ivfUziss8PFoAACBwuWNIp1Zs2aN5s+fr//85z8KCQnRXXfd5alxBSTajwAAfIF47D6zp3bRBLS5DwYxGwDgAcRm1wUXaz9yIo9KbQCAa9xOav/+++9asGCBFixYoF9//VWXXnqpZs2apRtuuEGRkZHeGGPAYKNIAEBlIR5XjD02n1mpzeoqAEB5EZvLp3hP7Zx8ktoAANe43H7k7bffVt++fdWkSRPNnj1bgwcP1q5du/TVV1/p9ttvJ0jLsbrLIgv9OQEAHuereHzgwAHdeuutqlWrliIjI3XBBRdo06ZN5nnDMPToo4+qbt26ioyMVK9evbR7926vjMUT7LG5RE9tJqIBAG7iXrliivfUzqFSGwDgIpcrtW+99Vb169dPy5cv11VXXaWgoHK3466y7NVdQZYgWSwWqr4AAB7ni3h89OhRde3aVT169NAnn3yiOnXqaPfu3apZs6Z5zfTp0/Xiiy9q4cKFatKkiSZOnKg+ffpo+/btioiI8PoY3WVWatN+BABQQdwrV0xwRKgkW1I7l0ptAICLXE5q//7770pMTPTmWAJeqTfIVH0BADzEF/H46aefVoMGDTR//nzzWJMmTcw/G4ahGTNm6JFHHtG1114rSVq0aJGSkpK0YsUK3XTTTZU6XleYPbXZKBIAUEHcK1dM8Y0iaT8CAHCVy1PIBOmzs1d3lbhBpuoLAOAhvojH77//vtq3b68bbrhBiYmJateunV599VXzfFpamtLT09WrVy/zWFxcnDp16qR169Y5fc3c3FxlZWU5PCqTGbPPmIhmdRUAwF3cK1eMJfR0Uju3gKQ2AMA1rIvyIHt1F/05AQBVya+//qrZs2erRYsW+vTTT3X33XfrH//4hxYuXChJSk9PlyQlJSU5PC8pKck8d6Zp06YpLi7OfDRo0MC7H+IMpcZsJqIBAKhcIaeT2nn5BT4eDAAgUJDU9iBzKTNVXwCAKsRqteqiiy7S1KlT1a5dO40cOVJ33XWX5syZU+7XnDBhgjIzM83H/v37PTjisyt1dRUT0QAAVK6ipHZoIZXaAADXkdT2INqPAACqorp16+q8885zONaqVSvt27dPkpScnCxJysjIcLgmIyPDPHem8PBwxcbGOjwqU2kT0cRsAAAqWQjtRwAA7nM7qb1//379/vvv5s8bN27U2LFjNXfuXI8OLBCxUSQAoLJUZjzu2rWrdu7c6XBs165datSokSTbppHJyclavXq1eT4rK0sbNmxQly5dPD4eTzBj9hkT0ayuAgCUF/fK5URSGwBQDm4ntW+55RZ9+eWXkmw9NHv37q2NGzfqX//6lyZPnuzxAQYSe3UX/TkBAN5WmfH4vvvu0/r16zV16lTt2bNHixcv1ty5czVq1ChJksVi0dixYzVlyhS9//77+vHHH3X77bcrJSVFAwYM8OhYPKXUmM1ENACgnLhXLqfiPbVJagMAXOR2Unvbtm3q2LGjJOntt99W69at9c033+jNN9/UggULPD2+gGIuZabqCwDgZZUZjzt06KDly5dryZIlat26tZ544gnNmDFDQ4YMMa8ZP3687r33Xo0cOVIdOnTQiRMntHLlSkVERHh0LJ5S6uoqJqIBAOXEvXI5kdQGAJRDiLtPyM/PV3h4uCTp888/1zXXXCNJatmypQ4dOuTZ0QUY2o8AACpLZcfjq6++WldffXWp5y0WiyZPnhwwlWilTUQTswEA5cW9cjnRfgQAUA5uV2qff/75mjNnjr7++mutWrVKffv2lSQdPHhQtWrV8vgAAwkbRQIAKgvxuGLMmH3GRDSrqwAA5UVsLqeipHaQpIL8PN+OBQAQMNxOaj/99NN65ZVX1L17d918881q06aNJOn99983l1pVV/bqLvpzAgC8jXhcMaXGbCaiAQDlRGwup9BQ848Febk+HAgAIJC43X6ke/fu+vPPP5WVlaWaNWuax0eOHKmoqCiPDi7QmEuZqfoCAHgZ8bhiSl1dxUQ0AKCciM3lFHI6LWHNo1IbAOAatyu1T506pdzcXDNI7927VzNmzNDOnTuVmJjo8QEGEtqPAAAqC/G4YkqbiCZmAwDKi9hcTsWS2oX5VGoDAFzjdlL72muv1aJFiyRJx44dU6dOnfTcc89pwIABmj17tscHGEjYKBIAUFmIxxVjxuwzJqJZXQUAKC9iczkVr9QmqQ0AcJHbSe0tW7bo0ksvlSS9++67SkpK0t69e7Vo0SK9+OKLHh9gIDmzUpv+nAAAbyEeV4w9NrMPBgDAU4jN5RR0Oi1hZaNIAICL3E5qnzx5UjVq1JAkffbZZ7ruuusUFBSkzp07a+/evR4fYCCxV3fZb4yp+gIAeAvxuGJKXV3FRDQAoJyIzeVksSi/6B6aSm0AgKvcTmo3b95cK1as0P79+/Xpp5/qyiuvlCQdPnxYsbGxHh9gIKH9CACgshCPK8bsqc1GkQAADyE2l19hUbW2UZDv45EAAAKF20ntRx99VA888IAaN26sjh07qkuXLpJsM9Ht2rXz+AADSWkbRVoNqwzD8Nm4AABVD/G4YsyYfcZENKurAADlRWwuv4KiSm2D9iMAABeFnP0SR9dff726deumQ4cOqU2bNubxnj17auDAgR4dXKAprVJbst0kF/8ZAICKIB5XjD1mn9kyjPYjAIDyIjaXX2FRHBaV2gAAF7md1Jak5ORkJScn6/fff5ck1a9fXx07dvTowALRmT217f+1nwsWSW0AgOcQj8uv1M2daT8CAKgAYnP5FBYVgBkFVGoDAFzjdvsRq9WqyZMnKy4uTo0aNVKjRo0UHx+vJ554QlZr9V6yW1r7EYmbZACAZxGPK8bsqc1GkQAADyE2l19BcNG9cyGV2gAA17hdqf2vf/1L8+bN01NPPaWuXbtKkv773//q8ccfV05Ojp588kmPDzJQlNV+hJtkAIAnEY8rxozZTvbBAACgPIjN5We1kNQGALjH7aT2woUL9dprr+maa64xj1144YWqV6+e7rnnnmodqKnUBgBUFuJxxZS2USTxGgBQXsTm8jvdU5v2IwAA17jdfuTIkSNq2bJlieMtW7bUkSNHPDKoQHXmplNn9tQGAMBTiMflZxiGDBmSSsZsVlYBAMqL2Fx+VntSu7DAtwMBAAQMt5Pabdq00csvv1zi+Msvv+yww3N1VFp/TombZACAZxGPy694NfaZq6uo1AYAlBexufzsldoW2o8AAFzkdvuR6dOnq1+/fvr888/VpUsXSdK6deu0f/9+ffzxxx4fYCA5s/1I8UptbpIBAJ5EPC6/4qunzpyIZmUVAKC8iM3lZw2ypSaCqNQGALjI7Urtyy+/XLt27dLAgQN17NgxHTt2TNddd5127typSy+91BtjDBhnbhRpsVhYzgwA8AricfkVj8klKrWJ1wCAciI2l589qU2lNgDAVW5XaktSSkpKiU0ufv/9d40cOVJz5871yMACkf1GuHiFdpAlSFbDSuUXAMDjiMflU3z1VIme2qysAgBUALG5fMyktpVKbQCAa9yu1C7NX3/9pXnz5nnq5QKS2VM76HQvbXvVNjfJAIDKQDw+O4dK7TPaj1CpDQDwNGLz2RnB9kptktoAANd4LKmNku1HJJYzAwDgbxx6agedkdRmEhoAgEpn9tSmUhsA4CKS2h505kaREjfJAAD4m+Ix2azUDmKjSAAAfMVeqR1MUhsA4CKS2h5kv0k+s6e2xE0yAAD+ovjqKYvFIkls7AwAgA9Zg0MlSZZC4jAAwDUubxR53XXXlXn+2LFjFR1LwDN7atN+BADgJcTjinPaLoyVVQCAciI2e0BRUjvYoFIbAOAal5PacXFxZz1/++23V3hAgYz2IwAAbyMeV5zTjZ2ZhAYAlBOxueIMe1KbOAwAcJHLSe358+d7cxxVAhtFAgC8jXhcceYktJNKbdqFAQDcRWz2gKKkdhD3zQAAF9FT24PsN8n01AYAwH+VtQcGK6sAAPABktoAADeR1PYgpz21aT8CAIBfcdoujJVVAAD4TogtqR3CfTMAwEUktT3IbD/CTTIAAH6LSWgAAPyMPaltNVjlDABwCUltDyqrRyc3yQAA+IeyJqG5kQYAoPJZgsMkSSFWqcBa4OPRAAACAUltDyqrRyc3yQAA+Iey9sBgZRUAAJXPYlZqS/mF+T4eDQAgEJDU9iBzOTPtRwAA8FtmpTYrqwAA8AuW0NOV2vlWktoAgLMjqe1BtB8BAMD/MQkNAIB/CQo5ndTOLcj18WgAAIGApLYHsVEkAAD+r6xJaKtoFwYAQKUrVqmdU5Dj48EAAAIBSW0PKqtHJz21AQDwD2XtgcEkNAAAlc8SGiLJltQ+VXDKx6MBAAQCktoeZC5npv0IAAB+y6zUdrayingNAEDlK5bUplIbAOAKktoeRPsRAAD8X5mT0MRrAAAqnSWEpDYAwD0+TWqvXbtW/fv3V0pKiiwWi1asWOFw3jAMPfroo6pbt64iIyPVq1cv7d692zeDdQEbRQIA4P/KmoSmXRgAAJXPQqU2AMBNPk1qZ2dnq02bNpo5c6bT89OnT9eLL76oOXPmaMOGDYqOjlafPn2Uk+OfQc7ZTTI9OgEA8C9l7YHBJDQAAJWPpDYAwF0hvnzz1NRUpaamOj1nGIZmzJihRx55RNdee60kadGiRUpKStKKFSt00003VeZQXWKv7ip+k0zlFwAA/sWchKb9CAAAfsGe1A4tJKkNAHCN3/bUTktLU3p6unr16mUei4uLU6dOnbRu3bpSn5ebm6usrCyHR2Up8yaZyi8AAPyC2VObjSIBAPALlrDTldqn8k/5eDQAgEDgt0nt9PR0SVJSUpLD8aSkJPOcM9OmTVNcXJz5aNCggVfHWZzZU5uNIgEA8Ftl7YHByioAACpfEO1HAABu8tukdnlNmDBBmZmZ5mP//v2V9t5UagMA4P/sMdlpT20moQEAqHT01AYAuMtvk9rJycmSpIyMDIfjGRkZ5jlnwsPDFRsb6/CoLM56atv/TOUXAAD+ocyVVUxCAwBQ6YLCQyWR1AYAuM5vk9pNmjRRcnKyVq9ebR7LysrShg0b1KVLFx+OrHS0HwEAwP+ZPbXZKBIAAL9QvKc2SW0AgCtCfPnmJ06c0J49e8yf09LStHXrViUkJKhhw4YaO3aspkyZohYtWqhJkyaaOHGiUlJSNGDAAN8Nugy0HwEAwP+Z8drJJDQrqwAAqHxBJLUBAG7yaVJ706ZN6tGjh/nzuHHjJElDhw7VggULNH78eGVnZ2vkyJE6duyYunXrppUrVyoiIsJXQy4TldoAAPg/e0wu3i6MSWgAAHyneFL7VMEpH48GABAIfJrU7t69uwzDKPW8xWLR5MmTNXny5EocVfnRUxsAAP/nbGUVG0UCAOA7wVRqAwDc5Lc9tQMR7UcAAPB/Zk9tNooEAMAv0H4EAOAuktoeRPsRAAD8nxmvnUxCs7IKAIDKR1IbAOAuktoeRKU2AAD+r6yNIpmEBgCg8gWH25LaoVbpVD5JbQDA2ZHU9iB6agMA4P+cbRRp9tRmEhoAgEoXFH66UvtkPhtFAgDOjqS2BzltP2Kh8gsAAH9S5soq4jUAAJWu+EaRVGoDAFxBUtuDnN4ks/EUAAB+hY0iAQDwL8HFKrVzSGoDAFxAUtuDqNQGAFQHTz31lCwWi8aOHWsey8nJ0ahRo1SrVi3FxMRo0KBBysjI8N0gy8BGkQCAQLB27Vr1799fKSkpslgsWrFiRZnXL1u2TL1791adOnUUGxurLl266NNPPy1x3cyZM9W4cWNFRESoU6dO2rhxo5c+getCIkNt/6VSGwDgIpLaHkRPbQBAVfftt9/qlVde0YUXXuhw/L777tMHH3ygd955R1999ZUOHjyo6667zkejLJu9GttpT20moQEAfiI7O1tt2rTRzJkzXbp+7dq16t27tz7++GNt3rxZPXr0UP/+/fXdd9+Z17z11lsaN26cHnvsMW3ZskVt2rRRnz59dPjwYW99DJcE0X4EAOCmEF8PoCqh/QgAoCo7ceKEhgwZoldffVVTpkwxj2dmZmrevHlavHixrrjiCknS/Pnz1apVK61fv16dO3f21ZCdcrqyingNAPAzqampSk1Ndfn6GTNmOPw8depUvffee/rggw/Url07SdK///1v3XXXXbrjjjskSXPmzNFHH32k119/XQ899JDHxu62kGJJ7QKS2gCAs6NS24NoPwIAqMpGjRqlfv36qVevXg7HN2/erPz8fIfjLVu2VMOGDbVu3Tqnr5Wbm6usrCyHR2Uxe2qzUSQAoAqzWq06fvy4EhISJEl5eXnavHmzQ7wOCgpSr169So3XUiXF7GJJ7ZyCU55/fQBAlUNS24Oo1AYAVFVLly7Vli1bNG3atBLn0tPTFRYWpvj4eIfjSUlJSk9Pd/p606ZNU1xcnPlo0KCBN4btVFnxmnZhAICq4tlnn9WJEyc0ePBgSdKff/6pwsJCJSUlOVxXVryWKilmF0tq5xZSqQ0AODuS2h5kr+6ipzYAoCrZv3+/xowZozfffFMREREeec0JEyYoMzPTfOzfv98jr+uKsuI1k9AAgKpg8eLFmjRpkt5++20lJiZW6LUqJWaT1AYAuIme2h5kLmem/QgAoArZvHmzDh8+rIsuusg8VlhYqLVr1+rll1/Wp59+qry8PB07dsyhWjsjI0PJyclOXzM8PFzh4eHeHrpTZqU28RoAUAUtXbpUd955p9555x2HViO1a9dWcHCwMjIyHK4vK15LlRSziyW186wktQEAZ0eltgfRfgQAUBX17NlTP/74o7Zu3Wo+2rdvryFDhph/Dg0N1erVq83n7Ny5U/v27VOXLl18OHLnnPbUJl4DAKqAJUuW6I477tCSJUvUr18/h3NhYWG6+OKLHeK11WrV6tWrfR+vi5LaoYWSVYUqsBb4djwAAL9HpbYHsVEkAKAqqlGjhlq3bu1wLDo6WrVq1TKPjxgxQuPGjVNCQoJiY2N17733qkuXLurcubMvhlymsuI17cIAAP7ixIkT2rNnj/lzWlqatm7dqoSEBDVs2FATJkzQgQMHtGjRIkm2liNDhw7VCy+8oE6dOpl9siMjIxUXFydJGjdunIYOHar27durY8eOmjFjhrKzs3XHHXdU/gcszl6pbUgypJyCHMWExfh2TAAAv0ZS24Ps1V301AYAVDfPP/+8goKCNGjQIOXm5qpPnz6aNWuWr4flVFnxmkloAIC/2LRpk3r06GH+PG7cOEnS0KFDtWDBAh06dEj79u0zz8+dO1cFBQUaNWqURo0aZR63Xy9JN954o/744w89+uijSk9PV9u2bbVy5coSm0dWupDTqYlgq3Qq/xRJbQBAmUhqexDLmQEA1cWaNWscfo6IiNDMmTM1c+ZM3wzIDWalNvEaAODHunfvLsMwSj1vT1TbnRmbSzN69GiNHj26AiPzgmJJ7RCrrVIbAICy0FPbg2g/AgCA/2NjZwAA/ExoqPlHktoAAFeQ1PYgNooEAMD/lRWvaRcGAIAPFKvUDiWpDQBwAUltD7JXd9FTGwAA/1VWvGYSGgAAHwgJkbVosjkyn6Q2AODsSGp7UJnLmblJBgDAL5iV2k7itdWwltm/FAAAeIHFovzwaElSdL50quCUjwcEAPB3JLU9qMz2I/ToBADAL5S1sXPx8wAAoPIUhBUltfOo1AYAnB1JbQ8qc6NIKrUBAPALZcVriaQ2AAC+UBgRJclWqU1SGwBwNiS1PcieuKanNgAA/quseF38PAAAqDyFEVRqAwBcR1Lbg8pazkz7EQAA/ENZ7cIkYjYAAL5gRJ7uqU1SGwBwNiS1PYj2IwAA+L+yNnaWiNkAAPiCEU2lNgDAdSS1PcQwDBkyJFGpDQCAPzMnodkoEgAA/1GU1I7Kl07ln/LxYAAA/o6ktocUr+oqfmNMT20AAPzLWXtqMxENAECls8TQfgQA4DqS2h5SPGld/MaY9iMAAPiXstqFScRsAAB8IahG2e1HDMOo7CEBAPwYSW0PKV7VRfsRAAD8l7ONnS0WiyyySCJmAwDgC8E1Sq/UXrt3rZKeTdJb297yxdAAAH6IpLaHlNZ+hEptAAD8iz0mF4/XxX+mZRgAAJUvJDZKkq1S+2S+Y1J71S+r9MfJP/TJnk98MTQAgB8iqe0hZ6vU5gYZAAD/YI/ZxduFFf+ZiWgAACpfSPzpSu3sHMek9vG845Kk7PzsSh8XAMA/kdT2kNJ6aps3yCxlBgDAL5iV2pYzKrUttAwDAMBXQmJP99Q+nnPK4VxWbpYk6UTeiUofFwDAP5HU9hDajwAAEBjMntqltB8hZgMAUPksMacrtU/mlVKpnUelNgDAhqS2hxSv6ipeqc1GkQAA+Bd7TC6tUpuWYQAA+EC0Lakd5SSpba/Upv0IAMCOpLaHnG0pMzfIAAD4B3vMPrOnNhPRAAD4UPTp9iMlKrVzbZXatB8BANiR1PYQe9KaTacAAPBvZqX2Ge1HiNkAAPhQ9On2I6fyS6nUpv0IAKAISW0PKe0GmaovAAD8i9lTm40iAQDwH1FRkmyV2qcKHDeKtPfUplIbAGBHUttDztZ+hKovAAD8gxmz2SgSAAD/UaxSO7fQefuR7PxsGYZR6UMDAPgfktoecrZKbXpqAwDgH9goEgAAP1Ssp3bxpLZhGGb7kQJrgfIK83wyPACAfyGp7SFn7anNUmYAAPxCaRtFErMBAPChoqR2VL6UV6z9SE5BjsMqqux8+moDAEhqewztRwAACAxn3QeDmA0AQOUrSmoHSQoqtlGkvUrbjr7aAACJpLbHsFEkAACBgY0iAQDwQ0UbRUpSWO7pSm37JpF22XlUagMASGp7DJXaAAAEhrNtFElPbQAAfCA4WHnB4ZKksLzSK7VpPwIAkEhqe8zZempzgwwAgH+wV2KX2lObiWgAAHwiL8zWgiQyv8CM18dzHSu1aT8CAJBIansM7UcAAAgMZ11dRcwGAMAnCkJtLUii86XcwlxJTiq1aT8CABBJbY+h/QgAAIHB7KnNRpEAAPiVwogYSVJ0npRTYGtBcmZPbSq1AQASSW2PoVIbAIDAYMbsUiaiaRkGAIBvFEbY2o9E559OatNTGwDgDEltD6GnNgAAgcFeiV1qT20mogEA8AlrpC2pHVUsqU1PbQCAMyS1PYT2IwAABIazrq4iZgMA4BNGUVI7Ok86lX9KEj21AQDOkdT2ENqPAAAQGMye2mwUCQCAXzGiS7Yfoac2AMAZktoeQqU2AACBwYzZpUxE0zIMAADfsESfrtQ+s/2IvU0YPbUBABJJbY+hpzYAAIHBXoldak9tJqIBAPAJS0yUpDM2isyztR9Jik6SRPsRAIANSW0POVv7EathlWEYlT4uAADg6Kyrq2g/AgCATwTXKL1SOzkmWZJ0Ip/2IwAAktoec7YbZIlqbQAA/IHZU5uNIgEA8CvBsad7ap8qcNwosm6NupKo1AYA2JDU9pCzVWpL3CQDAOAPzJhdykQ0k9AAAPhGcJyTSu2ijSLrxtiS2mwUCQCQSGp7zNl6ahe/BgAA+I59krnUntq0HwEAwCdCi5LaUflSdm5RT217pXZRUpuNIgEAEkltj3Gl/Qg3yQAA+N7ZVlexsgoAAN8IjT/dfiTrpGNPbXv7ESq1AQASSW2Pof0IAACBweypzUaRAAD4lZBi7UeOn8qRYRglK7XpqQ0AEEltj6FSGwAA/2cYhgwZkkqfiKZdGAAAvmGJjpJkq9Q+kZOjnIIc817b3CiS9iMAAJHU9pjSKrXpqQ0AgP8ovmqq1J7arKwCAMA3ootVauecMqu0JSkpOkkS7UcAADYktT3ElY0iuUkGAMC3iq+aov0IAAB+Jvp0T+3s3Bwdz7P1044Ji1GN8BqSZKveJlYDQLVHUttDSms/YrFYTld+EXgBAPCp4qum2CgSAAA/U6xS+2RujlmpHRseq5iwGPMyWpAAAEhqe0hp7UekYpVf3CQDAOBTxWNxaZXatAsDAMBHipLaUfnSyfwcHc+1VWrXCKuh8OBws2CMzSIBACS1PaS0Sm3pdAsSbpIBAPCt4qumSu2pzcoqAAB8oyipHVEo5eaeNNuPxIbHymKxKDrUdp6+2gAAktoeUlpPbanYcmZukgEA8CmHSu0z24+wsgoAAN8qSmpLkiU722w/Yu+nbW9BQvsRAECIrwcQKL79Vvr6a+nCC6VevUqep/0IAAD+z6Gn9pntR5iEBgDAt8LDVWixKNgwpJMnzPYjseGxkqToMCq1AQA2VGq76P33pfvvl5Ytc36+rPYj3CQDAFA5jh2T3n5bWrDA+Xl7LLbIIovF4nCOntoAAPiYxaLckHBJUnDOydOV2mFnVGrTUxsAqj2S2i7qt+NZHVKyenw2wel5Vyq1uUkGAMC7DhyQbrxRuu8+5+ftk9BltgtjZRUAAD6TExohSQrJOeXQU1uS2VOb9iMAAJLaLqoZW6hkZSjiyEGn58vqqW1uPMVNMgAAXtXo1M/areZad6yljh8veb6sSWg2igQAwPfyQiMlSaG5p0qt1Kb9CADAr5Pajz/+uCwWi8OjZcuWPhlLdNMkSVLUiQyn52k/AgCA78UkRau5flFT/ar9e0uukLJPQjuN1+yBAQCAz+WF2ZLa4Xk5pfbUpv0IAMCvk9qSdP755+vQoUPm47///a9PxlGzpS2pnZCfoZMnS543K7+4SQYAVDHTpk1Thw4dVKNGDSUmJmrAgAHauXOnwzU5OTkaNWqUatWqpZiYGA0aNEgZGc4ngr0qMVGSFKZ8Hdp+tMRpcxLaWbswJqEBAPC5gvAoSVJoXq6y8ooqtcOp1AYAOPL7pHZISIiSk5PNR+3atX0yDnuldpIytH9/yfOu3CTTUxsAEIi++uorjRo1SuvXr9eqVauUn5+vK6+8UtnZp6uk7rvvPn3wwQd655139NVXX+ngwYO67rrrKn+w4eE6HlpTknRkR8mkuj1h7bSnNntgAADgcwURtmrsiPzckpXa9NQGABQJ8fUAzmb37t1KSUlRRESEunTpomnTpqlhw4alXp+bm6vc3Fzz56ysLM8MJMmW1E7UYW3fa9W55zreDLvUU5vKLwBAAFq5cqXDzwsWLFBiYqI2b96syy67TJmZmZo3b54WL16sK664QpI0f/58tWrVSuvXr1fnzp0rdbzZ0Umqceyoju/JkHSew7my2oWxBwYAAL5njSxKaufl0VMbAFAqv67U7tSpkxYsWKCVK1dq9uzZSktL06WXXqrjznZ+KjJt2jTFxcWZjwYNGnhmMEXLmUNUqMM/HylxmvYjAIDqIjMzU5KUkJAgSdq8ebPy8/PVq1cv85qWLVuqYcOGWrdundPXyM3NVVZWlsPDU3JrJtv+uze9xDmzpzbtRwAA8E+RtsR1ZEG+jufZ7v3t7UfMSm16agNAtefXSe3U1FTdcMMNuvDCC9WnTx99/PHHOnbsmN5+++1SnzNhwgRlZmaaj/3OeoWUR2ioToTbbt4zdzlZzkyPTgBANWC1WjV27Fh17dpVrVu3liSlp6crLCxM8fHxDtcmJSUpPb1kYlny4iS0JKNodZX1UOntR5iEBgDAP1mibUntqIICs1L7zI0iT+RTqQ0A1Z1fJ7XPFB8fr3POOUd79uwp9Zrw8HDFxsY6PDzlVKztJvlkWvlukunRCQAIdKNGjdK2bdu0dOnSCr2O1yahJYXWs8XrkD9LJtTZAwMAAP8WFFOUwM6T/jz5p6SS7Ueo1AYABFRS+8SJE/rll19Ut25dn7x/QS3bTXL+7yWT2i711KbyCwAQwEaPHq0PP/xQX375perXr28eT05OVl5eno4dO+ZwfUZGhpKTk52+ljcnoaOa2t4zIitD1jPy02VtFMkeGAAA+F6IPamdL53MPymJjSIBACX5dVL7gQce0FdffaXffvtN33zzjQYOHKjg4GDdfPPNPhmPJcnWV1sZtB8BAFQfhmFo9OjRWr58ub744gs1adLE4fzFF1+s0NBQrV692jy2c+dO7du3T126dKns4apGC9skdB1rhv74w/FcWRtF0n4EAADfC4m1VWVH5Z8+Zu+pzUaRAAC7EF8PoCy///67br75Zv3111+qU6eOunXrpvXr16tOnTo+GU94A9tNcuhRenQCAKqPUaNGafHixXrvvfdUo0YNs092XFycIiMjFRcXpxEjRmjcuHFKSEhQbGys7r33XnXp0kWdO3eu9PGGpNjidbLStW+fVNRiW5KLG0USrwEA8JngWFviOjrv9DF7MtveU5v2IwAAv05qV7Rfp6dFN7PdFdfMy1BmphQXd/ocPToBAFXV7NmzJUndu3d3OD5//nwNGzZMkvT8888rKChIgwYNUm5urvr06aNZs2ZV8kiLFLU8SVKGNuyXOnQ4fYo9MAAA8G8hcbbEdXRRpXZMWIzZIoxKbQCAnV8ntf1NWH1bUjtJGdq/3zGp7VJPbdqPAAACkGEYZ70mIiJCM2fO1MyZMythRGdRVJqdqMPa95tVxbut2SehidcAAPin0PiipHZRpba9n7ZET20AwGl+3VPb7yQ5JrWLo/0IAAB+ItG2B0aoCvTX7iMOp8x4TfsRAAD8Ulh8lKTTldo1wmqY56jUBgDYkdR2R1lJbTaKBADAP4SF6VRUgiTpxC+O+2CYPbXLmoQmXgMA4DPhCWVUahfrqe3KSjIAQNVFUtsdxZYz79/nGEDNpDY9OgEA8Ln8BFvMzt3vmNRmDwwAAPzbmT21a4SfrtS2tx8xZOhUwalKHxsAwH+Q1HZHUVI7XHn669dMh1Mu9dRmOTMAAJWjaLNIHUp3OGyvwiZeAwDgp6JLr9SOCo0y/5ydR19tAKjOSGq7IyJCeZG2gJr96xmVX6706GQ5MwAAlSKsQdFEdGaGcnNPH3dlZRXxGgAAHypKakc56akdHBSsyJBISWwWCQDVHUltNxXUst0k5x8oZTkzG0UCAOBz4Q1O74Nx4MDp42ZPbTaKBADAP0UXaz9iOFZqS2wWCQCwIantJkuy7SbZkpGh4vtSuFKpTY9OAAAqh6Wurf1IstK1b9/p42a8Zg8MAAD8U1FSO9iQwgscK7Ulx80iAQDVF0ltN9mXM9fMz9Bff50+7lJPbZYzAwBQOZJOV2rv33/6sL0Km3gNAPBna9euVf/+/ZWSkiKLxaIVK1aUef2hQ4d0yy236JxzzlFQUJDGjh3r9LoZM2bo3HPPVWRkpBo0aKD77rtPOTk5nv8AFRF1um92dL7jRpESldoAABuS2m4Krlv2TTLtRwAA8APFktpOK7VpPwIA8GPZ2dlq06aNZs6c6dL1ubm5qlOnjh555BG1adPG6TWLFy/WQw89pMcee0w7duzQvHnz9NZbb+nhhx/25NArLiREuUG2VEV0Xsn2I9GhRZXa9NQGgGotxNcDCDhnVH61a2c7zEaRAAD4keTT7UeKT0KbPbXZKBIA4MdSU1OVmprq8vWNGzfWCy+8IEl6/fXXnV7zzTffqGvXrrrlllvM59x8883asGFDxQfsYadCQxWem2ur1A6jUhsAUBKV2u46y3JmenQCAOAHiuJ1og5r/97T8deM1+yBAQCoZi655BJt3rxZGzdulCT9+uuv+vjjj3XVVVeV+bzc3FxlZWU5PLztVGiYpFIqtempDQAQldruK5bUXuek8qvMHp0sZwYAoHIkJkqSQlSorLS/JNWRdLoKm3gNAKhubrnlFv3555/q1q2bDMNQQUGB/v73v5+1/ci0adM0adKkShqlTU5RUjuqjJ7atB8BgOqNSm13lVapTfsRAAD8R2ioCuJrSZLyf88wD7u0BwbxGgBQBa1Zs0ZTp07VrFmztGXLFi1btkwfffSRnnjiiTKfN2HCBGVmZpqP/cVvhL0kJzRckm2jyNJ6atN+BACqNyq13VVU+ZWkDO3fZ0iySGKjSAAA/E1QcpJ07C9FZ2coM7O14uKK9dRmo0gAQDUzceJE3XbbbbrzzjslSRdccIGys7M1cuRI/etf/1JQkPOat/DwcIWHh1fmUJUXFiHJ1n7kzJ7a5kaRtB8BgGqNSm03/JH9h/6Isf3KonRKR/adnhmmUhsAAP8SVNe2uipZ6dq3z3bMjNfsgQEAqGZOnjxZInEdHGyLfYZh+GJIpcoLL0pqO6nUZqNIAIBEUttl41eNV+KziZq1Y5GskVGSpMKDGbIW3fealV9ObpKDin7N3CQDAOB9BdYC7T22V0pOluTYMsxehV1mT20moQEAPnbixAlt3bpVW7dulSSlpaVp69at2lc0SzthwgTdfvvtDs+xX3/ixAn98ccf2rp1q7Zv326e79+/v2bPnq2lS5cqLS1Nq1at0sSJE9W/f38zue0vCsKLVWqf0VPb3CiSntoAUK3RfsRFLRJaSJJW/bpKjyYnSWlpSijIUEZGc9WtW/ZNMsuZAQCoHOt/X6/UN1OVHJOsHUl9JdmS2iUqtWk/AgDwY5s2bVKPHj3Mn8eNGydJGjp0qBYsWKBDhw6ZCW67du3amX/evHmzFi9erEaNGum3336TJD3yyCOyWCx65JFHdODAAdWpU0f9+/fXk08+6f0P5KbCCFshWXT+6cpsOyq1AQASSW2X9W7WW5LtZrmwzkUKSUszK7/q1j3LTTIbTwEAUCla1m6p47nHdSznmI7EXasE2dqP7Cqq1C5rZRXxGgDgL7p3715mS5AFCxaUOHa2FiIhISF67LHH9Nhjj1V0eF5nT2rXyA0tUThm9tSmUhsAqjXaj7iocXxjtUhooUKjUH/UsG0OmaQMFU16l71RJJVfAABUiviIeHVp0EWS9L1xSJItXu/daztvxusyKrVpFwYAgI9F2aqxY3LDSpyiUhsAIJHUdkvvprZq7V9CbcEzSRn6/HPbObPyy8lNsn1mmZtkAAC8L7V5qiTp69xdkmzxesMGyTBOV2GX2VObSWgAAHzKEm3bHDLaSVLb7KmdR6U2AFRnJLXdYG9B8r3SJdlukt97TyosLPsmmeXMAABUnr7Nbb20P83+XpKt/ciePdK2bWdZWUW8BgDAL9SueY7tv3+0LXHOXqlN+xEAqN5IaruhR+MeCrYEa3vQEUlS/ZAMHT4sbdhA+xEAAPxF2+S2SopOUlrYKUlSouUPWWTV8uVn6alNvAYAwC9E1bZVagdn1NKJM7qM2Htq034EAKo3ktpuiIuIU6f6nZRhi6E6Jz5DkmzV2mwUCQCAXwiyBKlP8z76oyheBxuFqqW/tGwZ8RoAgEDQsKVto8iwgmy98YbjObNSm/YjAFCtkdR2U++mvZVhi6FKCbYltZcvd61Sm57aAABUjr7N+qogWDoSY4vB9YLS9f330h9/ldEujHgNAIBfCKphm5mOVrZeftm2L4advac2ldoAUL2R1HZT76a9zUrtmOwMhYVJu3dLubm2G2A2ngIAwPeubHalLLLoYKQt9va+0DYRvf3n0iehidcAAPiJaNtNdyv9rLt+GqP9N4yTJk6UfvvNbD+Sb81XXmGeL0cJAPAhktpu6livo07VqiFJspw4odTLT0qSjmeznBkAAH9RK6qWOtbraK6uurKNLan9886intrEawAA/FdysiQpUYc1Ri+q4X+el6ZMkTp3VszONPMyWpAAQPVFUttNocGhuvjcHsopuhe+4fLDkqSTp9goEgAAf5LaPFXpRUntjg3TJUkHDhKvAQDwexddJC1YoD/unqgp+pee0kPKO7e1lJGh0Ct6qctBW8zOziepDQDVFUntcujd7Eqz8qtPW1vlV17B2Su16dEJAEDl6du8r9kyrMapQ+rUSZKljJ7axGsAAPyDxSINHao6sybrq15TNEHT9OSVa6XOnaWjR/XpgkJd9ht9tQGgOiOpXQ69mxXrq310R9FNsgs9tVnODABApWmf0l7Ha0ZJkv78dZuuu05SUOmT0MRrAAD8z7332v770hs1dfK9VdIVV6hGnrTyDSnkw499OzgAgM+Q1C6HFgkttLuRrVTbOnqURnT60az8YjkzAAD+ITgoWPGt2kmS4j5arVuSvzAnofNyidcAAASCfv2kxo2lo0el2++J0Yd3f6QvWscoskBqNuKf0syZvh4iAMAHSGqXg8ViUcZj92tjihSVeVI3LOykFpk5kqT/WxSsnBzH60OsFrXOkEJP5jh5NZzV4cNSbq6vRwEACED1brtHnzaTwnMLlHjXlbo+/2dJ0mcrg3XggOO1wZZgpWRJyUcLfDDSKuDECenIEV+PAgBQxQQHSw89ZPvzf/4j9b8hQn26nKNXL5IsVqs0erTmxo9Xm/9v796Do6rv/4+/ztlNNveEiyQBQVBQEJGiKEWcH7bSr6Dfeq396aT+ou3UQcFiO734q6Xa6VCZb2fsr+106Nip9ju/WvmWDlBKay3ipdWfXKRcVRAVq18gXIVcSLKX8/79cZJNNtfdBLK7yfMxcybZc9vPee/Z8zr72T270zzdc4/0k59Ir7wiHTwoRYl0ABi0HDOzdDfiXKqtrVVpaalOnz6tkpKSs7ZeM9OKvy3TNdVL9akj0n8XS//jPunAb9/VuKJJ+sHjptsv2KbAymfl/eHXKv6kTuEcV8HPLZB3681q/Nx1ipUWywu48gKuwoeP64PVb+n43/Yo5623pUBA7tSJGjn3Yo29YaICw4pljiPPcRWJBpRbGFJOYYECOTkKOgEFT59RrOa0wodOyPFiKhhVpJyyQqmwUAoGJTPFYlE1x5qVEypQMK9Ayg3Ji8Skmho5hw/JqTksRSJyykdJo1qGUEiKxSTP84dU5ORIeXn+IEl790pbt/rD++9LF10kTZ8ufepT0qRJUn6+f3+StHu3tGaNtHq1tGuXVFAgffaz0oIF0uc+J7U+lgO9++bm+m0MhaRIRNqzR9q502/jqVPS5MnS1KnSpZf69YvF2gakxnH8wXX9uufl+ftUNCpt2iRt2OAP+/f7PyQzd6503XXStGn+fhGN+sPgPsShv/LzpdLSfq3iXOXMUHMu8/oHL3xXMx5erlv2SWFX+uKd0h9rHlfe5se0eLH0jf91XMXP/176r9+o6J9bJUne9BnS7beq+d/nK3LhBfJcR17AVaypSYdf3Kv//tNbCm/eo7xPjik6YYJKZk/SuAUTVXhxpcx1ZI6rSMyVmxtSqDhfgVBIATeonKaodPS0IodOKna6XnnDCxQaXiinuMjPFjOZ56k51iwFAsrNL5KTmyfPCUgnTsTz2jn1iTR8eFtel5S0ZXWqmeO6/jE2P98/ZzhypC2vt2/31z19uj9MmyaVlfltDQSk48eldev8vN6wQQqH/WPyggX+MGmSfxwe6GNxIOBvUyjkb9MHH/h5vXOnfw5y/vlteX3RRf4yrXlNbqSmNa8dx691a90dR3rvvba8fv11qbLSz+q5c6U5c/x9rjWvUz3PxNDiOFJ5eb9XQ2afHemq44svSmvXSn/+s/ThdddJF7yq//2XSv1o62FJ0t9HjNLevHLVWalqbZjqoyNVHzlPTqhCuUWVCpWWKn9YofJHFKlgeIGCeQEFcgIKhgIK5LgKBPxIDLRczNUaqWaSyc92ua7MceXJH8xxJceR4zoJywddTzlBU8C1lmy3hDg0OYnHz3aDG/D/OoGWzyAmmUudZnOcLm/7f0xOT7O3u5HM3Xe8q84j2q2sZYWO2lYcr0fH++7Qyvarbb98l3fVaQsBJKunp3BvPn1tUJVj+vcZ6mRzhk7tfvrrG/9X42+p1uRjpogrNatQYS9XjkzDdCo+X3NACp2Dfk1PkjlSIMMfxVgKbQy7Ui6va9AFT5LnSMEM39+RPdZdeZVufnNLv9bBC+Sz41zX8cW9z6vhf96mW3Y1y5NUm5uvcLhIUQV1no4pR/5HuWItJ3DnIlcjrpST4fmWSl5HXCngcdkfusb5HM6m07kBlTb3/yO3ZPbZke46mkm3/+d9Wvuv30iSqnZKT/+RYw4AZIJV31quO//jO/1aR7I5E+zXvUDzZ9+jPRsqtG/+TbqkJqIcNUhqkCSdUb7W6Wb9fsxlWn/fUl18Qrptr3TbO9IVNZ3X9d4waUeFtKtCck265Lg05Zh08QkpL9r1i0ZXUusblLW50okCKepKhWGpMCIVhdtenHrqeh1ngtLhYulQsb/seQ3SeWekkWcSX9i2dqAnw7HE+wqYVJ8jbRstbR0tvTtCmnhSmn5E+lSNVN7QNm+uJzUFpBcmSmsmS+svlsbUSQv2Swvek6752O8UaD1nSbZN/dXVi/xjBf5jtrNcOpkvTTkuTT3q/81vd96dSu3Qef9p5cp/bhwrkF68UNpwobRnlHT1QWnuv6TrPvT33fZi1B09OBM70/tMGBTmTV6gw6/s1/O3XK0F/6hRWbhRUmN8+jZdoZWBu/S7r/wfhUsP6fP7pFv3Sv/2vpTX4U3pE/n+sX9HhXS4SJp0wj/uTzne+RjUXmuHdnPAX0ddSMqPtOV165vfrfnW8TgYc6QjhX5ef5IvDWuURjX4Gdr+jfNUM6d9vgXMX/7t86StY6RtlVJJs5/X02v8c5LWdrVuzz8rpNVTpDVTpOMF0g3v+Xn9b+9LIxr71qa+6i4/mgJ+XuyokPaPkMaelqYe8zN7FLnRL93VPNfzO7b/31hpw0XSy+P987nrPpTmfihddixxfs6V0BPP5apHtHEc6T/v/qn+9v5NOhM5o/Dnw1p95wGN3LRL0dOfyDt9Wk5trdTQILf+jHIam1XQ7Kkg4r9GK4j4+Rsw/83ZgPmvMbp7T9dRy7HO+HANAPQmEGrsfaazhE9qnyWRcJPCH76vwkCeFI3KC0cVO/8CBcuK5DjSobpDqm1qULi+QGdqC9R8THJjnj94ptJR+ZpyVXGnj/h7nv+tHbGYVFRoKsz3lJcTVeRMs5pqG9Xc0KhoNCanoljBYskJNiviRXW6LqrTtVGdqo8ox8lVUahARXn5KsjNk6NmWaResUi9ohZRpDBPES+miBdVNBaTF5NinuRF/bYp4MoNBuQ6jsxMnjx55inmeTLPUTTqyIs58jz/+8aDrivHcaRoRAo3Sc2NcsKNah4+XBZs/WEuk+TEzxycpmYFIlG5zWE54YiaS8sUC/lfW+KZ+fM7JjP/ryOn7dItS7w0wp/dkee1timxpu2X67SszL/qS/LvJ+ESLJMTjcoNR+SGw/LMFC4uleO4fnvkX+ZuMsmLyY3FZK7rD3L8mnSnpRwJI1pe2XV8gnpe56dsxyu1OtYj8crv7i/h6ulwYGZdXFbWcaa2+/HM5Lrt29TymLWru7VsnfkValmlk7huLyY3FlUwHFYgHJaiUZ0ZPlJy3Q7z+isONDXJCwQUCwRkjtt2/+rQDnW+dKb18e+uFj09hl3Xzmlro9ptr1kf1tXW5u4e547T29e147T2Ley0jg7taL0aMvF+LF7Uthp3/7xM2AaZXKe3/dCRzJGZk/DcV7tlu6phx8e2tZ2tz+vWZVqnTxkzRp+74pKuG5qkdH9aabAYqDqamY7s26YKFcW/9qApVKqciycoEJAawg068MmH8poL1VhbqDPHg3IaYwrETK7nKRh0NG3ueSoo7Lz/HTzo/wxEYYH5Q15MFg6rqb5RzbWNCjc2SSMKFRiWo0AoIs8Jq7a+JbProopE5Od1qEBFoXwFXE+KNsiLNCgWbVBzYZ6ijinsRRSJRiVzWvLaZJGonEBATiAg13XjzxtPnmJeTOY5isUc/xs2oo5cx88l13HkmCcn0iw1N8ptblQ4P0+xwoK2Y2tLXptJikQVCIcVCEfkhiOKBYNqLhuecOz3n+VePK/bat/VccvPavOcLo8bnTOsZXxrXrc8t9vfliRFoy2ZHZYbDquppFQWCLa0x5Ha5Y8bbpZcV7HWy8pNnY5RbffbLvri2eO0O24m7msd9ZTXnbe3px7eXi657nDfXW1O+8es/ZXv/kSn5XL7tvpKicdvp2P7zOTEogpEIi37SJOai0sVzcuL50P77HfD/u+leIFAu3OlxFW23kenx79dBnVXj47H+4Rlu5m3Y2abWeftTLjX3h6Hns9D2+8HrZndcXr7jO20jg7blPAYqnVf6kdetztf6frco11eS/65a4fMdhynyxp2lddOy9OzfWa3Tg+6AT1409yuG5oCMvvsyLY6mpnf+R0LK2YxxbyYYhaTZ17CEHACch03PrTuv07r62Dz/ITzYjLPk3kxWSzm35Y/3fM8mXmKyRSTp5g8meP4x4uW9SQc7E1y2gddy1cxOWYyL9btV4i0bpfU7hjdsmzr8cufp+UdaK/LFwLdFKz1T9trl8TJHY6NPb526eIFuePE38S0luX9Q4a1tbcbfT0mn81ur7aziN7b0d/3avuTQedKNnUhdveaO5Vt6LS/o1dTJ1ytkaWV/VoHn9QeYDm5ecq5eGr8tqvET62MLh6t0cWSzkttva7rf92jz5EUaBlCkno4gRjZ01qLe5sBAIBByXEcVUyemTAur93/hbmFuqx8qvpizBh/ULzj1JWUI6mw+4WG97bWYX1qCwAA8DmOo8LcQhX2lMcAgKzD1yACAAAAAAAAALIGndoAAAAAAAAAgKxBpzYAAAAAAAAAIGvQqQ0AAAAAAAAAyBp0agMAAAAAAAAAsgad2gAAAAAAAACArEGnNgAAAAAAAAAga9CpDQAAAAAAAADIGnRqAwAAAAAAAACyBp3aAAAAAAAAAICsQac2AAAAAAAAACBr0KkNAAAAAAAAAMgadGoDAAAAAAAAALIGndoAAAAAAAAAgKwRTHcDzjUzkyTV1tamuSUAgMGoNV9a8wZ9Q14DAM41MvvsILMBAOdSsnk96Du16+rqJEljx45Nc0sAAINZXV2dSktL092MrEVeAwAGCpndP2Q2AGAg9JbXjg3yt6k9z9OhQ4dUXFwsx3FSWra2tlZjx47Vxx9/rJKSknPUwsGBWiWPWqWGeiWPWqXmbNXLzFRXV6fRo0fLdflWr74irwcGtUoN9UoetUoN9Ure2awVmX129DWz2e9TQ72SR61SQ72SR62Sl468HvSf1HZdV+eff36/1lFSUsLOmyRqlTxqlRrqlTxqlZqzUS8+7dV/5PXAolapoV7Jo1apoV7JO1u1IrP7r7+ZzX6fGuqVPGqVGuqVPGqVvIHMa96eBgAAAAAAAABkDTq1AQAAAAAAAABZg07tHoRCIT322GMKhULpbkrGo1bJo1apoV7Jo1apoV6DB49l8qhVaqhX8qhVaqhX8qjV4MFjmRrqlTxqlRrqlTxqlbx01GrQ/1AkAAAAAAAAAGDw4JPaAAAAAAAAAICsQac2AAAAAAAAACBr0KkNAAAAAAAAAMgadGoDAAAAAAAAALIGndrd+MUvfqHx48crLy9Ps2bN0pYtW9LdpLR74okndNVVV6m4uFijRo3Srbfeqn379iXM09TUpEWLFmnEiBEqKirSHXfcoSNHjqSpxZlj+fLlchxHDz/8cHwctUp08OBBfelLX9KIESOUn5+vadOm6c0334xPNzN9//vfV2VlpfLz8zVv3jzt378/jS1Oj1gspqVLl2rChAnKz8/XRRddpB/+8Idq/5u/Q7lWf//73/X5z39eo0ePluM4Wrt2bcL0ZGpz8uRJVVVVqaSkRGVlZfrKV76i+vr6AdwKpIrM7ozM7jsyu2fkdfLI7O6R10MTed0Zed135HXvyOzkkNc9y+jMNnSycuVKy83Ntaefftreeust++pXv2plZWV25MiRdDctrW644QZ75plnbM+ePbZjxw678cYbbdy4cVZfXx+fZ+HChTZ27FjbuHGjvfnmm/bpT3/arrnmmjS2Ov22bNli48ePt8svv9yWLFkSH0+t2pw8edIuuOACu/fee23z5s32wQcf2AsvvGDvvfdefJ7ly5dbaWmprV271nbu3Gk333yzTZgwwRobG9PY8oG3bNkyGzFihK1fv94OHDhgq1atsqKiIvvpT38an2co1+ovf/mLPfroo7Z69WqTZGvWrEmYnkxt5s+fb9OnT7dNmzbZP/7xD5s4caLdfffdA7wlSBaZ3TUyu2/I7J6R16khs7tHXg895HXXyOu+Ia97R2Ynj7zuWSZnNp3aXbj66qtt0aJF8duxWMxGjx5tTzzxRBpblXmOHj1qkuzVV181M7NTp05ZTk6OrVq1Kj7PO++8Y5LsjTfeSFcz06qurs4mTZpkGzZssLlz58YDl1ol+s53vmPXXnttt9M9z7OKigr78Y9/HB936tQpC4VC9txzzw1EEzPGTTfdZF/+8pcTxt1+++1WVVVlZtSqvY6Bm0xt3n77bZNkW7dujc/z/PPPm+M4dvDgwQFrO5JHZieHzO4dmd078jo1ZHZyyOuhgbxODnndO/I6OWR28sjr5GVaZvP1Ix2Ew2Ft27ZN8+bNi49zXVfz5s3TG2+8kcaWZZ7Tp09LkoYPHy5J2rZtmyKRSELtJk+erHHjxg3Z2i1atEg33XRTQk0katXRunXrNHPmTN15550aNWqUZsyYoV/96lfx6QcOHFBNTU1CvUpLSzVr1qwhV69rrrlGGzdu1LvvvitJ2rlzp1577TUtWLBAErXqSTK1eeONN1RWVqaZM2fG55k3b55c19XmzZsHvM3oGZmdPDK7d2R278jr1JDZfUNeDz7kdfLI696R18khs5NHXvddujM72K+lB6Hjx48rFoupvLw8YXx5ebn27t2bplZlHs/z9PDDD2vOnDm67LLLJEk1NTXKzc1VWVlZwrzl5eWqqalJQyvTa+XKlfrnP/+prVu3dppGrRJ98MEHWrFihb7xjW/ou9/9rrZu3aqvfe1rys3NVXV1dbwmXT0vh1q9HnnkEdXW1mry5MkKBAKKxWJatmyZqqqqJIla9SCZ2tTU1GjUqFEJ04PBoIYPHz7k65eJyOzkkNm9I7OTQ16nhszuG/J68CGvk0Ne9468Th6ZnTzyuu/Sndl0aqNPFi1apD179ui1115Ld1My0scff6wlS5Zow4YNysvLS3dzMp7neZo5c6Z+9KMfSZJmzJihPXv26Je//KWqq6vT3LrM8vvf/17PPvusfve732nq1KnasWOHHn74YY0ePZpaAegSmd0zMjt55HVqyGwAqSCve0Zep4bMTh55nb34+pEORo4cqUAg0OkXco8cOaKKioo0tSqzLF68WOvXr9fLL7+s888/Pz6+oqJC4XBYp06dSph/KNZu27ZtOnr0qK644goFg0EFg0G9+uqr+tnPfqZgMKjy8nJq1U5lZaUuvfTShHFTpkzRRx99JEnxmvC8lL71rW/pkUce0V133aVp06bpnnvu0de//nU98cQTkqhVT5KpTUVFhY4ePZowPRqN6uTJk0O+fpmIzO4dmd07Mjt55HVqyOy+Ia8HH/K6d+R178jr1JDZySOv+y7dmU2ndge5ubm68sortXHjxvg4z/O0ceNGzZ49O40tSz8z0+LFi7VmzRq99NJLmjBhQsL0K6+8Ujk5OQm127dvnz766KMhV7vrr79eu3fv1o4dO+LDzJkzVVVVFf+fWrWZM2eO9u3blzDu3Xff1QUXXCBJmjBhgioqKhLqVVtbq82bNw+5ep05c0aum3joDgQC8jxPErXqSTK1mT17tk6dOqVt27bF53nppZfkeZ5mzZo14G1Gz8js7pHZySOzk0dep4bM7hvyevAhr7tHXiePvE4NmZ088rrv0p7Z/fqZyUFq5cqVFgqF7De/+Y29/fbbdv/991tZWZnV1NSku2lp9cADD1hpaam98sordvjw4fhw5syZ+DwLFy60cePG2UsvvWRvvvmmzZ4922bPnp3GVmeO9r/MbEat2tuyZYsFg0FbtmyZ7d+/35599lkrKCiw3/72t/F5li9fbmVlZfbHP/7Rdu3aZbfccotNmDDBGhsb09jygVddXW1jxoyx9evX24EDB2z16tU2cuRI+/a3vx2fZyjXqq6uzrZv327bt283Sfbkk0/a9u3b7V//+peZJVeb+fPn24wZM2zz5s322muv2aRJk+zuu+9O1yahF2R218js/iGzu0Zep4bM7h55PfSQ110jr/uHvO4emZ088rpnmZzZdGp34+c//7mNGzfOcnNz7eqrr7ZNmzalu0lpJ6nL4ZlnnonP09jYaA8++KANGzbMCgoK7LbbbrPDhw+nr9EZpGPgUqtEf/rTn+yyyy6zUChkkydPtqeeeiphuud5tnTpUisvL7dQKGTXX3+97du3L02tTZ/a2lpbsmSJjRs3zvLy8uzCCy+0Rx991Jqbm+PzDOVavfzyy10ep6qrq80sudqcOHHC7r77bisqKrKSkhK77777rK6uLg1bg2SR2Z2R2f1DZnePvE4emd098npoIq87I6/7h7zuGZmdHPK6Z5mc2Y6ZWf8+6w0AAAAAAAAAwMDgO7UBAAAAAAAAAFmDTm0AAAAAAAAAQNagUxsAAAAAAAAAkDXo1AYAAAAAAAAAZA06tQEAAAAAAAAAWYNObQAAAAAAAABA1qBTGwAAAAAAAACQNejUBgAAAAAAAABkDTq1AZwVjuNo7dq16W4GAADoBZkNAEDmI6+BntGpDQwC9957rxzH6TTMnz8/3U0DAADtkNkAAGQ+8hrIfMF0NwDA2TF//nw988wzCeNCoVCaWgMAALpDZgMAkPnIayCz8UltYJAIhUKqqKhIGIYNGybJv2xpxYoVWrBggfLz83XhhRfqD3/4Q8Lyu3fv1mc/+1nl5+drxIgRuv/++1VfX58wz9NPP62pU6cqFAqpsrJSixcvTph+/Phx3XbbbSooKNCkSZO0bt26c7vRAABkITIbAIDMR14DmY1ObWCIWLp0qe644w7t3LlTVVVVuuuuu/TOO+9IkhoaGnTDDTdo2LBh2rp1q1atWqUXX3wxIVBXrFihRYsW6f7779fu3bu1bt06TZw4MeE+fvCDH+iLX/yidu3apRtvvFFVVVU6efLkgG4nAADZjswGACDzkddAmhmArFddXW2BQMAKCwsThmXLlpmZmSRbuHBhwjKzZs2yBx54wMzMnnrqKRs2bJjV19fHp//5z38213WtpqbGzMxGjx5tjz76aLdtkGTf+9734rfr6+tNkj3//PNnbTsBAMh2ZDYAAJmPvAYyH9+pDQwSn/nMZ7RixYqEccOHD4//P3v27IRps2fP1o4dOyRJ77zzjqZPn67CwsL49Dlz5sjzPO3bt0+O4+jQoUO6/vrre2zD5ZdfHv+/sLBQJSUlOnr0aF83CQCAQYnMBgAg85HXQGajUxsYJAoLCztdqnS25OfnJzVfTk5Owm3HceR53rloEgAAWYvMBgAg85HXQGbjO7WBIWLTpk2dbk+ZMkWSNGXKFO3cuVMNDQ3x6a+//rpc19Ull1yi4uJijR8/Xhs3bhzQNgMAMBSR2QAAZD7yGkgvPqkNDBLNzc2qqalJGBcMBjVy5EhJ0qpVqzRz5kxde+21evbZZ7Vlyxb9+te/liRVVVXpscceU3V1tR5//HEdO3ZMDz30kO655x6Vl5dLkh5//HEtXLhQo0aN0oIFC1RXV6fXX39dDz300MBuKAAAWY7MBgAg85HXQGajUxsYJP7617+qsrIyYdwll1yivXv3SvJ/NXnlypV68MEHVVlZqeeee06XXnqpJKmgoEAvvPCClixZoquuukoFBQW644479OSTT8bXVV1draamJv3kJz/RN7/5TY0cOVJf+MIXBm4DAQAYJMhsAAAyH3kNZDbHzCzdjQBwbjmOozVr1ujWW29Nd1MAAEAPyGwAADIfeQ2kH9+pDQAAAAAAAADIGnRqAwAAAAAAAACyBl8/AgAAAAAAAADIGnxSGwAAAAAAAACQNejUBgAAAAAAAABkDTq1AQAAAAAAAABZg05tAAAAAAAAAEDWoFMbAAAAAAAAAJA16NQGAAAAAAAAAGQNOrUBAAAAAAAAAFmDTm0AAAAAAAAAQNb4/9yd0vS7G2b0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1800x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1480564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.52915358543396,\n",
       " 2.6699368953704834,\n",
       " 2.47354793548584,\n",
       " 24.051544189453125,\n",
       " 2.471971273422241,\n",
       " 2.481118679046631,\n",
       " 2.4829139709472656,\n",
       " 2.4834539890289307,\n",
       " 2.4835879802703857,\n",
       " 2.4831602573394775,\n",
       " 2.4808285236358643,\n",
       " 2.4755706787109375,\n",
       " 2.4718868732452393,\n",
       " 2.4805285930633545,\n",
       " 2.474663734436035,\n",
       " 2.4706850051879883,\n",
       " 2.470874309539795,\n",
       " 2.4715564250946045,\n",
       " 2.469951629638672,\n",
       " 2.4705374240875244,\n",
       " 2.4714772701263428,\n",
       " 2.4691684246063232,\n",
       " 2.4689483642578125,\n",
       " 2.469027519226074,\n",
       " 2.469649076461792,\n",
       " 2.4692981243133545,\n",
       " 2.468510627746582,\n",
       " 2.468695640563965,\n",
       " 2.468609094619751,\n",
       " 2.469146490097046,\n",
       " 2.469041347503662,\n",
       " 2.4691054821014404,\n",
       " 2.4688165187835693,\n",
       " 2.468752145767212,\n",
       " 2.4683568477630615,\n",
       " 2.468144178390503,\n",
       " 2.46884822845459,\n",
       " 2.46816349029541,\n",
       " 2.4682648181915283,\n",
       " 2.4684689044952393,\n",
       " 2.468280076980591,\n",
       " 2.4682114124298096,\n",
       " 2.4684877395629883,\n",
       " 2.4681806564331055,\n",
       " 2.4682083129882812,\n",
       " 2.468263864517212,\n",
       " 2.468032121658325,\n",
       " 2.4681508541107178,\n",
       " 2.4682843685150146,\n",
       " 2.4682259559631348,\n",
       " 2.4681196212768555,\n",
       " 2.4678714275360107,\n",
       " 2.467898368835449,\n",
       " 2.467970848083496,\n",
       " 2.4676060676574707,\n",
       " 2.4675703048706055,\n",
       " 2.4678874015808105,\n",
       " 2.4675090312957764,\n",
       " 2.4680023193359375,\n",
       " 2.4677674770355225,\n",
       " 2.4674315452575684,\n",
       " 2.4678878784179688,\n",
       " 2.467369794845581,\n",
       " 2.4674346446990967,\n",
       " 2.467540740966797,\n",
       " 2.4675920009613037,\n",
       " 2.467515707015991,\n",
       " 2.467275381088257,\n",
       " 2.467164993286133,\n",
       " 2.4674460887908936,\n",
       " 2.4673032760620117,\n",
       " 2.46718430519104,\n",
       " 2.467564344406128,\n",
       " 2.4671683311462402,\n",
       " 2.4673449993133545,\n",
       " 2.467233180999756,\n",
       " 2.467222213745117,\n",
       " 2.4671692848205566,\n",
       " 2.4671006202697754,\n",
       " 2.4672975540161133,\n",
       " 2.4670467376708984,\n",
       " 2.4671497344970703,\n",
       " 2.467115640640259,\n",
       " 2.4674007892608643,\n",
       " 2.467235803604126,\n",
       " 2.4672343730926514,\n",
       " 2.4669559001922607,\n",
       " 2.4669559001922607,\n",
       " 2.4672043323516846,\n",
       " 2.4672036170959473,\n",
       " 2.466874599456787,\n",
       " 2.467198610305786,\n",
       " 2.466935873031616,\n",
       " 2.467633008956909,\n",
       " 2.4668126106262207,\n",
       " 2.4671883583068848,\n",
       " 2.467252731323242,\n",
       " 2.4669625759124756,\n",
       " 2.4668374061584473,\n",
       " 2.467071771621704]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyGAT_response[\"loss_dict\"][\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b2872d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5196"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(OG_response[\"loss_dict\"][\"loss\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAT_response = train_dominant(encoder=\"GAT\", dataset=\"BlogCatalog\", hidden_dim=32, max_epoch=100, lr=5e-3, dropout=0.3, alpha=0.8, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b6f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0582c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dominant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m DenseGAT_response \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dominant\u001b[49m(encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDenseGAT\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlogCatalog\u001b[39m\u001b[38;5;124m\"\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, max_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-3\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dominant' is not defined"
     ]
    }
   ],
   "source": [
    "DenseGAT_response = train_dominant(encoder=\"DenseGAT\", dataset=\"BlogCatalog\", hidden_dim=64, max_epoch=100, lr=5e-3, dropout=0.3, alpha=0.8, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6da33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DenseSAGE_response = train_dominant(encoder=\"DenseSage\", dataset=\"BlogCatalog\", hidden_dim=64, max_epoch=100, lr=5e-3, dropout=0.3, alpha=0.8, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0277f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa530dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca422f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a641f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6efa67d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba3e131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d829c509",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ce29cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train_loss= 3.80194 train/struct_loss= 13.96719 train/feat_loss= 1.26063\n",
      "Epoch: 0000 Auc 0.8076820836336431\n",
      "Epoch: 0001 train_loss= 3.63766 train/struct_loss= 13.25442 train/feat_loss= 1.23347\n",
      "Epoch: 0002 train_loss= 3.53396 train/struct_loss= 12.81713 train/feat_loss= 1.21317\n",
      "Epoch: 0003 train_loss= 3.41595 train/struct_loss= 12.28494 train/feat_loss= 1.19870\n",
      "Epoch: 0004 train_loss= 3.30913 train/struct_loss= 11.79079 train/feat_loss= 1.18872\n",
      "Epoch: 0005 train_loss= 3.22164 train/struct_loss= 11.37924 train/feat_loss= 1.18224\n",
      "Epoch: 0006 train_loss= 3.14188 train/struct_loss= 10.99608 train/feat_loss= 1.17834\n",
      "Epoch: 0007 train_loss= 3.07212 train/struct_loss= 10.65597 train/feat_loss= 1.17615\n",
      "Epoch: 0008 train_loss= 2.99504 train/struct_loss= 10.27493 train/feat_loss= 1.17507\n",
      "Epoch: 0009 train_loss= 2.93523 train/struct_loss= 9.97784 train/feat_loss= 1.17458\n",
      "Epoch: 0010 train_loss= 2.87493 train/struct_loss= 9.67710 train/feat_loss= 1.17438\n",
      "Epoch: 0010 Auc 0.808315817166848\n",
      "Epoch: 0011 train_loss= 2.81950 train/struct_loss= 9.40025 train/feat_loss= 1.17431\n",
      "Epoch: 0012 train_loss= 2.77723 train/struct_loss= 9.18897 train/feat_loss= 1.17429\n",
      "Epoch: 0013 train_loss= 2.73005 train/struct_loss= 8.95313 train/feat_loss= 1.17428\n",
      "Epoch: 0014 train_loss= 2.69099 train/struct_loss= 8.75789 train/feat_loss= 1.17427\n",
      "Epoch: 0015 train_loss= 2.65765 train/struct_loss= 8.59119 train/feat_loss= 1.17426\n",
      "Epoch: 0016 train_loss= 2.62433 train/struct_loss= 8.42463 train/feat_loss= 1.17426\n",
      "Epoch: 0017 train_loss= 2.60178 train/struct_loss= 8.31192 train/feat_loss= 1.17425\n",
      "Epoch: 0018 train_loss= 2.58050 train/struct_loss= 8.20556 train/feat_loss= 1.17423\n",
      "Epoch: 0019 train_loss= 2.56091 train/struct_loss= 8.10767 train/feat_loss= 1.17422\n",
      "Epoch: 0020 train_loss= 2.54422 train/struct_loss= 8.02425 train/feat_loss= 1.17422\n",
      "Epoch: 0020 Auc 0.8122018026807271\n",
      "Epoch: 0021 train_loss= 2.53058 train/struct_loss= 7.95605 train/feat_loss= 1.17421\n",
      "Epoch: 0022 train_loss= 2.52005 train/struct_loss= 7.90347 train/feat_loss= 1.17420\n",
      "Epoch: 0023 train_loss= 2.51026 train/struct_loss= 7.85449 train/feat_loss= 1.17420\n",
      "Epoch: 0024 train_loss= 2.50265 train/struct_loss= 7.81646 train/feat_loss= 1.17420\n",
      "Epoch: 0025 train_loss= 2.49594 train/struct_loss= 7.78290 train/feat_loss= 1.17419\n",
      "Epoch: 0026 train_loss= 2.49088 train/struct_loss= 7.75765 train/feat_loss= 1.17419\n",
      "Epoch: 0027 train_loss= 2.48585 train/struct_loss= 7.73248 train/feat_loss= 1.17419\n",
      "Epoch: 0028 train_loss= 2.48234 train/struct_loss= 7.71498 train/feat_loss= 1.17418\n",
      "Epoch: 0029 train_loss= 2.47894 train/struct_loss= 7.69799 train/feat_loss= 1.17418\n",
      "Epoch: 0030 train_loss= 2.47648 train/struct_loss= 7.68570 train/feat_loss= 1.17417\n",
      "Epoch: 0030 Auc 0.8138063474750685\n",
      "Epoch: 0031 train_loss= 2.47418 train/struct_loss= 7.67421 train/feat_loss= 1.17417\n",
      "Epoch: 0032 train_loss= 2.47279 train/struct_loss= 7.66729 train/feat_loss= 1.17416\n",
      "Epoch: 0033 train_loss= 2.47103 train/struct_loss= 7.65849 train/feat_loss= 1.17416\n",
      "Epoch: 0034 train_loss= 2.47011 train/struct_loss= 7.65392 train/feat_loss= 1.17416\n",
      "Epoch: 0035 train_loss= 2.46913 train/struct_loss= 7.64903 train/feat_loss= 1.17416\n",
      "Epoch: 0036 train_loss= 2.46817 train/struct_loss= 7.64425 train/feat_loss= 1.17415\n",
      "Epoch: 0037 train_loss= 2.46781 train/struct_loss= 7.64242 train/feat_loss= 1.17415\n",
      "Epoch: 0038 train_loss= 2.46761 train/struct_loss= 7.64146 train/feat_loss= 1.17415\n",
      "Epoch: 0039 train_loss= 2.46714 train/struct_loss= 7.63910 train/feat_loss= 1.17415\n",
      "Epoch: 0040 train_loss= 2.46699 train/struct_loss= 7.63838 train/feat_loss= 1.17414\n",
      "Epoch: 0040 Auc 0.814089300933678\n",
      "Epoch: 0041 train_loss= 2.46655 train/struct_loss= 7.63620 train/feat_loss= 1.17414\n",
      "Epoch: 0042 train_loss= 2.46623 train/struct_loss= 7.63461 train/feat_loss= 1.17414\n",
      "Epoch: 0043 train_loss= 2.46614 train/struct_loss= 7.63414 train/feat_loss= 1.17414\n",
      "Epoch: 0044 train_loss= 2.46603 train/struct_loss= 7.63362 train/feat_loss= 1.17414\n",
      "Epoch: 0045 train_loss= 2.46580 train/struct_loss= 7.63245 train/feat_loss= 1.17413\n",
      "Epoch: 0046 train_loss= 2.46581 train/struct_loss= 7.63255 train/feat_loss= 1.17413\n",
      "Epoch: 0047 train_loss= 2.46581 train/struct_loss= 7.63251 train/feat_loss= 1.17413\n",
      "Epoch: 0048 train_loss= 2.46584 train/struct_loss= 7.63268 train/feat_loss= 1.17413\n",
      "Epoch: 0049 train_loss= 2.46539 train/struct_loss= 7.63046 train/feat_loss= 1.17413\n",
      "Epoch: 0050 train_loss= 2.46563 train/struct_loss= 7.63164 train/feat_loss= 1.17413\n",
      "Epoch: 0050 Auc 0.8141612382536635\n",
      "Epoch: 0051 train_loss= 2.46541 train/struct_loss= 7.63055 train/feat_loss= 1.17413\n",
      "Epoch: 0052 train_loss= 2.46539 train/struct_loss= 7.63047 train/feat_loss= 1.17412\n",
      "Epoch: 0053 train_loss= 2.46546 train/struct_loss= 7.63083 train/feat_loss= 1.17412\n",
      "Epoch: 0054 train_loss= 2.46539 train/struct_loss= 7.63049 train/feat_loss= 1.17412\n",
      "Epoch: 0055 train_loss= 2.46551 train/struct_loss= 7.63109 train/feat_loss= 1.17412\n",
      "Epoch: 0056 train_loss= 2.46526 train/struct_loss= 7.62984 train/feat_loss= 1.17412\n",
      "Epoch: 0057 train_loss= 2.46531 train/struct_loss= 7.63007 train/feat_loss= 1.17412\n",
      "Epoch: 0058 train_loss= 2.46532 train/struct_loss= 7.63012 train/feat_loss= 1.17412\n",
      "Epoch: 0059 train_loss= 2.46538 train/struct_loss= 7.63046 train/feat_loss= 1.17411\n",
      "Epoch: 0060 train_loss= 2.46527 train/struct_loss= 7.62991 train/feat_loss= 1.17411\n",
      "Epoch: 0060 Auc 0.8142297499869828\n",
      "Epoch: 0061 train_loss= 2.46524 train/struct_loss= 7.62973 train/feat_loss= 1.17411\n",
      "Epoch: 0062 train_loss= 2.46522 train/struct_loss= 7.62963 train/feat_loss= 1.17411\n",
      "Epoch: 0063 train_loss= 2.46510 train/struct_loss= 7.62909 train/feat_loss= 1.17411\n",
      "Epoch: 0064 train_loss= 2.46512 train/struct_loss= 7.62922 train/feat_loss= 1.17410\n",
      "Epoch: 0065 train_loss= 2.46500 train/struct_loss= 7.62862 train/feat_loss= 1.17409\n",
      "Epoch: 0066 train_loss= 2.46488 train/struct_loss= 7.62803 train/feat_loss= 1.17409\n",
      "Epoch: 0067 train_loss= 2.46504 train/struct_loss= 7.62888 train/feat_loss= 1.17408\n",
      "Epoch: 0068 train_loss= 2.46486 train/struct_loss= 7.62795 train/feat_loss= 1.17409\n",
      "Epoch: 0069 train_loss= 2.46511 train/struct_loss= 7.62917 train/feat_loss= 1.17409\n",
      "Epoch: 0070 train_loss= 2.46488 train/struct_loss= 7.62804 train/feat_loss= 1.17409\n",
      "Epoch: 0070 Auc 0.8142324904563154\n",
      "Epoch: 0071 train_loss= 2.46481 train/struct_loss= 7.62770 train/feat_loss= 1.17408\n",
      "Epoch: 0072 train_loss= 2.46515 train/struct_loss= 7.62944 train/feat_loss= 1.17408\n",
      "Epoch: 0073 train_loss= 2.46477 train/struct_loss= 7.62757 train/feat_loss= 1.17408\n",
      "Epoch: 0074 train_loss= 2.46500 train/struct_loss= 7.62873 train/feat_loss= 1.17406\n",
      "Epoch: 0075 train_loss= 2.46473 train/struct_loss= 7.62746 train/feat_loss= 1.17405\n",
      "Epoch: 0076 train_loss= 2.46472 train/struct_loss= 7.62748 train/feat_loss= 1.17403\n",
      "Epoch: 0077 train_loss= 2.46472 train/struct_loss= 7.62753 train/feat_loss= 1.17402\n",
      "Epoch: 0078 train_loss= 2.46481 train/struct_loss= 7.62799 train/feat_loss= 1.17401\n",
      "Epoch: 0079 train_loss= 2.46464 train/struct_loss= 7.62715 train/feat_loss= 1.17401\n",
      "Epoch: 0080 train_loss= 2.46462 train/struct_loss= 7.62703 train/feat_loss= 1.17401\n",
      "Epoch: 0080 Auc 0.8142468779203127\n",
      "Epoch: 0081 train_loss= 2.46460 train/struct_loss= 7.62694 train/feat_loss= 1.17402\n",
      "Epoch: 0082 train_loss= 2.46459 train/struct_loss= 7.62685 train/feat_loss= 1.17402\n",
      "Epoch: 0083 train_loss= 2.46459 train/struct_loss= 7.62686 train/feat_loss= 1.17402\n",
      "Epoch: 0084 train_loss= 2.46456 train/struct_loss= 7.62672 train/feat_loss= 1.17402\n",
      "Epoch: 0085 train_loss= 2.46444 train/struct_loss= 7.62614 train/feat_loss= 1.17401\n",
      "Epoch: 0086 train_loss= 2.46456 train/struct_loss= 7.62676 train/feat_loss= 1.17401\n",
      "Epoch: 0087 train_loss= 2.46456 train/struct_loss= 7.62677 train/feat_loss= 1.17400\n",
      "Epoch: 0088 train_loss= 2.46448 train/struct_loss= 7.62638 train/feat_loss= 1.17400\n",
      "Epoch: 0089 train_loss= 2.46446 train/struct_loss= 7.62629 train/feat_loss= 1.17400\n",
      "Epoch: 0090 train_loss= 2.46418 train/struct_loss= 7.62492 train/feat_loss= 1.17400\n",
      "Epoch: 0090 Auc 0.8142276946349831\n",
      "Epoch: 0091 train_loss= 2.46445 train/struct_loss= 7.62623 train/feat_loss= 1.17400\n",
      "Epoch: 0092 train_loss= 2.46432 train/struct_loss= 7.62560 train/feat_loss= 1.17400\n",
      "Epoch: 0093 train_loss= 2.46426 train/struct_loss= 7.62532 train/feat_loss= 1.17400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0094 train_loss= 2.46428 train/struct_loss= 7.62543 train/feat_loss= 1.17400\n",
      "Epoch: 0095 train_loss= 2.46446 train/struct_loss= 7.62631 train/feat_loss= 1.17400\n",
      "Epoch: 0096 train_loss= 2.46435 train/struct_loss= 7.62578 train/feat_loss= 1.17399\n",
      "Epoch: 0097 train_loss= 2.46421 train/struct_loss= 7.62509 train/feat_loss= 1.17399\n",
      "Epoch: 0098 train_loss= 2.46424 train/struct_loss= 7.62525 train/feat_loss= 1.17399\n",
      "Epoch: 0099 train_loss= 2.46420 train/struct_loss= 7.62506 train/feat_loss= 1.17399\n",
      "Epoch: 0100 train_loss= 2.46437 train/struct_loss= 7.62591 train/feat_loss= 1.17399\n",
      "Epoch: 0100 Auc 0.81424310977498\n",
      "Epoch: 0101 train_loss= 2.46432 train/struct_loss= 7.62566 train/feat_loss= 1.17399\n",
      "Epoch: 0102 train_loss= 2.46430 train/struct_loss= 7.62555 train/feat_loss= 1.17399\n",
      "Epoch: 0103 train_loss= 2.46438 train/struct_loss= 7.62595 train/feat_loss= 1.17399\n",
      "Epoch: 0104 train_loss= 2.46415 train/struct_loss= 7.62481 train/feat_loss= 1.17399\n",
      "Epoch: 0105 train_loss= 2.46423 train/struct_loss= 7.62522 train/feat_loss= 1.17398\n",
      "Epoch: 0106 train_loss= 2.46413 train/struct_loss= 7.62470 train/feat_loss= 1.17398\n",
      "Epoch: 0107 train_loss= 2.46418 train/struct_loss= 7.62499 train/feat_loss= 1.17398\n",
      "Epoch: 0108 train_loss= 2.46405 train/struct_loss= 7.62433 train/feat_loss= 1.17398\n",
      "Epoch: 0109 train_loss= 2.46436 train/struct_loss= 7.62590 train/feat_loss= 1.17398\n",
      "Epoch: 0110 train_loss= 2.46421 train/struct_loss= 7.62514 train/feat_loss= 1.17398\n",
      "Epoch: 0110 Auc 0.8142838742563051\n",
      "Epoch: 0111 train_loss= 2.46406 train/struct_loss= 7.62441 train/feat_loss= 1.17398\n",
      "Epoch: 0112 train_loss= 2.46402 train/struct_loss= 7.62421 train/feat_loss= 1.17397\n",
      "Epoch: 0113 train_loss= 2.46405 train/struct_loss= 7.62435 train/feat_loss= 1.17397\n",
      "Epoch: 0114 train_loss= 2.46395 train/struct_loss= 7.62389 train/feat_loss= 1.17397\n",
      "Epoch: 0115 train_loss= 2.46406 train/struct_loss= 7.62441 train/feat_loss= 1.17397\n",
      "Epoch: 0116 train_loss= 2.46422 train/struct_loss= 7.62520 train/feat_loss= 1.17397\n",
      "Epoch: 0117 train_loss= 2.46395 train/struct_loss= 7.62386 train/feat_loss= 1.17397\n",
      "Epoch: 0118 train_loss= 2.46407 train/struct_loss= 7.62450 train/feat_loss= 1.17397\n",
      "Epoch: 0119 train_loss= 2.46397 train/struct_loss= 7.62399 train/feat_loss= 1.17397\n",
      "Epoch: 0120 train_loss= 2.46393 train/struct_loss= 7.62381 train/feat_loss= 1.17396\n",
      "Epoch: 0120 Auc 0.814278050758973\n",
      "Epoch: 0121 train_loss= 2.46393 train/struct_loss= 7.62381 train/feat_loss= 1.17396\n",
      "Epoch: 0122 train_loss= 2.46386 train/struct_loss= 7.62345 train/feat_loss= 1.17397\n",
      "Epoch: 0123 train_loss= 2.46384 train/struct_loss= 7.62336 train/feat_loss= 1.17396\n",
      "Epoch: 0124 train_loss= 2.46393 train/struct_loss= 7.62380 train/feat_loss= 1.17396\n",
      "Epoch: 0125 train_loss= 2.46384 train/struct_loss= 7.62333 train/feat_loss= 1.17396\n",
      "Epoch: 0126 train_loss= 2.46390 train/struct_loss= 7.62365 train/feat_loss= 1.17396\n",
      "Epoch: 0127 train_loss= 2.46388 train/struct_loss= 7.62353 train/feat_loss= 1.17396\n",
      "Epoch: 0128 train_loss= 2.46387 train/struct_loss= 7.62351 train/feat_loss= 1.17396\n",
      "Epoch: 0129 train_loss= 2.46360 train/struct_loss= 7.62216 train/feat_loss= 1.17396\n",
      "Epoch: 0130 train_loss= 2.46390 train/struct_loss= 7.62368 train/feat_loss= 1.17396\n",
      "Epoch: 0130 Auc 0.814313676860299\n",
      "Epoch: 0131 train_loss= 2.46401 train/struct_loss= 7.62420 train/feat_loss= 1.17396\n",
      "Epoch: 0132 train_loss= 2.46406 train/struct_loss= 7.62445 train/feat_loss= 1.17396\n",
      "Epoch: 0133 train_loss= 2.46372 train/struct_loss= 7.62276 train/feat_loss= 1.17396\n",
      "Epoch: 0134 train_loss= 2.46373 train/struct_loss= 7.62283 train/feat_loss= 1.17396\n",
      "Epoch: 0135 train_loss= 2.46380 train/struct_loss= 7.62317 train/feat_loss= 1.17396\n",
      "Epoch: 0136 train_loss= 2.46391 train/struct_loss= 7.62374 train/feat_loss= 1.17396\n",
      "Epoch: 0137 train_loss= 2.46379 train/struct_loss= 7.62312 train/feat_loss= 1.17396\n",
      "Epoch: 0138 train_loss= 2.46387 train/struct_loss= 7.62354 train/feat_loss= 1.17396\n",
      "Epoch: 0139 train_loss= 2.46390 train/struct_loss= 7.62367 train/feat_loss= 1.17396\n",
      "Epoch: 0140 train_loss= 2.46372 train/struct_loss= 7.62276 train/feat_loss= 1.17396\n",
      "Epoch: 0140 Auc 0.8143140194189656\n",
      "Epoch: 0141 train_loss= 2.46363 train/struct_loss= 7.62232 train/feat_loss= 1.17396\n",
      "Epoch: 0142 train_loss= 2.46375 train/struct_loss= 7.62291 train/feat_loss= 1.17395\n",
      "Epoch: 0143 train_loss= 2.46376 train/struct_loss= 7.62295 train/feat_loss= 1.17396\n",
      "Epoch: 0144 train_loss= 2.46367 train/struct_loss= 7.62254 train/feat_loss= 1.17395\n",
      "Epoch: 0145 train_loss= 2.46372 train/struct_loss= 7.62281 train/feat_loss= 1.17395\n",
      "Epoch: 0146 train_loss= 2.46370 train/struct_loss= 7.62270 train/feat_loss= 1.17395\n",
      "Epoch: 0147 train_loss= 2.46375 train/struct_loss= 7.62293 train/feat_loss= 1.17395\n",
      "Epoch: 0148 train_loss= 2.46372 train/struct_loss= 7.62282 train/feat_loss= 1.17395\n",
      "Epoch: 0149 train_loss= 2.46366 train/struct_loss= 7.62251 train/feat_loss= 1.17395\n",
      "Epoch: 0150 train_loss= 2.46371 train/struct_loss= 7.62277 train/feat_loss= 1.17395\n",
      "Epoch: 0150 Auc 0.814304427776301\n",
      "Epoch: 0151 train_loss= 2.46363 train/struct_loss= 7.62235 train/feat_loss= 1.17395\n",
      "Epoch: 0152 train_loss= 2.46369 train/struct_loss= 7.62264 train/feat_loss= 1.17395\n",
      "Epoch: 0153 train_loss= 2.46365 train/struct_loss= 7.62245 train/feat_loss= 1.17395\n",
      "Epoch: 0154 train_loss= 2.46346 train/struct_loss= 7.62151 train/feat_loss= 1.17395\n",
      "Epoch: 0155 train_loss= 2.46362 train/struct_loss= 7.62232 train/feat_loss= 1.17395\n",
      "Epoch: 0156 train_loss= 2.46372 train/struct_loss= 7.62282 train/feat_loss= 1.17395\n",
      "Epoch: 0157 train_loss= 2.46352 train/struct_loss= 7.62180 train/feat_loss= 1.17395\n",
      "Epoch: 0158 train_loss= 2.46353 train/struct_loss= 7.62185 train/feat_loss= 1.17395\n",
      "Epoch: 0159 train_loss= 2.46364 train/struct_loss= 7.62241 train/feat_loss= 1.17395\n",
      "Epoch: 0160 train_loss= 2.46355 train/struct_loss= 7.62196 train/feat_loss= 1.17395\n",
      "Epoch: 0160 Auc 0.8143284068829628\n",
      "Epoch: 0161 train_loss= 2.46347 train/struct_loss= 7.62156 train/feat_loss= 1.17395\n",
      "Epoch: 0162 train_loss= 2.46378 train/struct_loss= 7.62310 train/feat_loss= 1.17395\n",
      "Epoch: 0163 train_loss= 2.46348 train/struct_loss= 7.62163 train/feat_loss= 1.17394\n",
      "Epoch: 0164 train_loss= 2.46358 train/struct_loss= 7.62213 train/feat_loss= 1.17395\n",
      "Epoch: 0165 train_loss= 2.46346 train/struct_loss= 7.62153 train/feat_loss= 1.17394\n",
      "Epoch: 0166 train_loss= 2.46347 train/struct_loss= 7.62158 train/feat_loss= 1.17394\n",
      "Epoch: 0167 train_loss= 2.46351 train/struct_loss= 7.62178 train/feat_loss= 1.17394\n",
      "Epoch: 0168 train_loss= 2.46345 train/struct_loss= 7.62147 train/feat_loss= 1.17394\n",
      "Epoch: 0169 train_loss= 2.46372 train/struct_loss= 7.62283 train/feat_loss= 1.17394\n",
      "Epoch: 0170 train_loss= 2.46347 train/struct_loss= 7.62158 train/feat_loss= 1.17394\n",
      "Epoch: 0170 Auc 0.8143277217656296\n",
      "Epoch: 0171 train_loss= 2.46343 train/struct_loss= 7.62137 train/feat_loss= 1.17394\n",
      "Epoch: 0172 train_loss= 2.46347 train/struct_loss= 7.62158 train/feat_loss= 1.17394\n",
      "Epoch: 0173 train_loss= 2.46332 train/struct_loss= 7.62085 train/feat_loss= 1.17394\n",
      "Epoch: 0174 train_loss= 2.46341 train/struct_loss= 7.62130 train/feat_loss= 1.17394\n",
      "Epoch: 0175 train_loss= 2.46354 train/struct_loss= 7.62196 train/feat_loss= 1.17394\n",
      "Epoch: 0176 train_loss= 2.46350 train/struct_loss= 7.62174 train/feat_loss= 1.17394\n",
      "Epoch: 0177 train_loss= 2.46344 train/struct_loss= 7.62143 train/feat_loss= 1.17394\n",
      "Epoch: 0178 train_loss= 2.46348 train/struct_loss= 7.62167 train/feat_loss= 1.17393\n",
      "Epoch: 0179 train_loss= 2.46350 train/struct_loss= 7.62176 train/feat_loss= 1.17393\n",
      "Epoch: 0180 train_loss= 2.46356 train/struct_loss= 7.62204 train/feat_loss= 1.17394\n",
      "Epoch: 0180 Auc 0.8143897248842836\n",
      "Epoch: 0181 train_loss= 2.46345 train/struct_loss= 7.62150 train/feat_loss= 1.17394\n",
      "Epoch: 0182 train_loss= 2.46347 train/struct_loss= 7.62163 train/feat_loss= 1.17394\n",
      "Epoch: 0183 train_loss= 2.46323 train/struct_loss= 7.62043 train/feat_loss= 1.17393\n",
      "Epoch: 0184 train_loss= 2.46344 train/struct_loss= 7.62150 train/feat_loss= 1.17393\n",
      "Epoch: 0185 train_loss= 2.46342 train/struct_loss= 7.62138 train/feat_loss= 1.17393\n",
      "Epoch: 0186 train_loss= 2.46326 train/struct_loss= 7.62060 train/feat_loss= 1.17393\n",
      "Epoch: 0187 train_loss= 2.46347 train/struct_loss= 7.62164 train/feat_loss= 1.17393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0188 train_loss= 2.46348 train/struct_loss= 7.62169 train/feat_loss= 1.17393\n",
      "Epoch: 0189 train_loss= 2.46333 train/struct_loss= 7.62091 train/feat_loss= 1.17393\n",
      "Epoch: 0190 train_loss= 2.46346 train/struct_loss= 7.62160 train/feat_loss= 1.17393\n",
      "Epoch: 0190 Auc 0.8143475901682923\n",
      "Epoch: 0191 train_loss= 2.46331 train/struct_loss= 7.62085 train/feat_loss= 1.17393\n",
      "Epoch: 0192 train_loss= 2.46333 train/struct_loss= 7.62093 train/feat_loss= 1.17393\n",
      "Epoch: 0193 train_loss= 2.46315 train/struct_loss= 7.62006 train/feat_loss= 1.17393\n",
      "Epoch: 0194 train_loss= 2.46318 train/struct_loss= 7.62020 train/feat_loss= 1.17393\n",
      "Epoch: 0195 train_loss= 2.46322 train/struct_loss= 7.62041 train/feat_loss= 1.17393\n",
      "Epoch: 0196 train_loss= 2.46329 train/struct_loss= 7.62076 train/feat_loss= 1.17393\n",
      "Epoch: 0197 train_loss= 2.46322 train/struct_loss= 7.62041 train/feat_loss= 1.17393\n",
      "Epoch: 0198 train_loss= 2.46355 train/struct_loss= 7.62207 train/feat_loss= 1.17393\n",
      "Epoch: 0199 train_loss= 2.46353 train/struct_loss= 7.62194 train/feat_loss= 1.17392\n",
      "Epoch: 0200 train_loss= 2.46333 train/struct_loss= 7.62092 train/feat_loss= 1.17393\n",
      "Epoch: 0200 Auc 0.8144058251416139\n",
      "Epoch: 0201 train_loss= 2.46332 train/struct_loss= 7.62089 train/feat_loss= 1.17392\n",
      "Epoch: 0202 train_loss= 2.46351 train/struct_loss= 7.62189 train/feat_loss= 1.17392\n",
      "Epoch: 0203 train_loss= 2.46338 train/struct_loss= 7.62122 train/feat_loss= 1.17392\n",
      "Epoch: 0204 train_loss= 2.46321 train/struct_loss= 7.62036 train/feat_loss= 1.17392\n",
      "Epoch: 0205 train_loss= 2.46344 train/struct_loss= 7.62154 train/feat_loss= 1.17392\n",
      "Epoch: 0206 train_loss= 2.46337 train/struct_loss= 7.62114 train/feat_loss= 1.17392\n",
      "Epoch: 0207 train_loss= 2.46307 train/struct_loss= 7.61964 train/feat_loss= 1.17392\n",
      "Epoch: 0208 train_loss= 2.46332 train/struct_loss= 7.62090 train/feat_loss= 1.17392\n",
      "Epoch: 0209 train_loss= 2.46337 train/struct_loss= 7.62118 train/feat_loss= 1.17392\n",
      "Epoch: 0210 train_loss= 2.46324 train/struct_loss= 7.62050 train/feat_loss= 1.17392\n",
      "Epoch: 0210 Auc 0.8144161019016116\n",
      "Epoch: 0211 train_loss= 2.46317 train/struct_loss= 7.62018 train/feat_loss= 1.17392\n",
      "Epoch: 0212 train_loss= 2.46306 train/struct_loss= 7.61963 train/feat_loss= 1.17392\n",
      "Epoch: 0213 train_loss= 2.46328 train/struct_loss= 7.62072 train/feat_loss= 1.17392\n",
      "Epoch: 0214 train_loss= 2.46310 train/struct_loss= 7.61983 train/feat_loss= 1.17392\n",
      "Epoch: 0215 train_loss= 2.46310 train/struct_loss= 7.61981 train/feat_loss= 1.17392\n",
      "Epoch: 0216 train_loss= 2.46311 train/struct_loss= 7.61991 train/feat_loss= 1.17392\n",
      "Epoch: 0217 train_loss= 2.46307 train/struct_loss= 7.61967 train/feat_loss= 1.17392\n",
      "Epoch: 0218 train_loss= 2.46304 train/struct_loss= 7.61956 train/feat_loss= 1.17392\n",
      "Epoch: 0219 train_loss= 2.46321 train/struct_loss= 7.62037 train/feat_loss= 1.17392\n",
      "Epoch: 0220 train_loss= 2.46302 train/struct_loss= 7.61943 train/feat_loss= 1.17391\n",
      "Epoch: 0220 Auc 0.8144051400242805\n",
      "Epoch: 0221 train_loss= 2.46304 train/struct_loss= 7.61955 train/feat_loss= 1.17391\n",
      "Epoch: 0222 train_loss= 2.46309 train/struct_loss= 7.61978 train/feat_loss= 1.17391\n",
      "Epoch: 0223 train_loss= 2.46309 train/struct_loss= 7.61978 train/feat_loss= 1.17391\n",
      "Epoch: 0224 train_loss= 2.46296 train/struct_loss= 7.61917 train/feat_loss= 1.17391\n",
      "Epoch: 0225 train_loss= 2.46301 train/struct_loss= 7.61938 train/feat_loss= 1.17391\n",
      "Epoch: 0226 train_loss= 2.46283 train/struct_loss= 7.61853 train/feat_loss= 1.17391\n",
      "Epoch: 0227 train_loss= 2.46298 train/struct_loss= 7.61928 train/feat_loss= 1.17391\n",
      "Epoch: 0228 train_loss= 2.46295 train/struct_loss= 7.61912 train/feat_loss= 1.17391\n",
      "Epoch: 0229 train_loss= 2.46314 train/struct_loss= 7.62008 train/feat_loss= 1.17391\n",
      "Epoch: 0230 train_loss= 2.46294 train/struct_loss= 7.61906 train/feat_loss= 1.17391\n",
      "Epoch: 0230 Auc 0.8144393958909402\n",
      "Epoch: 0231 train_loss= 2.46276 train/struct_loss= 7.61818 train/feat_loss= 1.17391\n",
      "Epoch: 0232 train_loss= 2.46308 train/struct_loss= 7.61973 train/feat_loss= 1.17391\n",
      "Epoch: 0233 train_loss= 2.46286 train/struct_loss= 7.61868 train/feat_loss= 1.17391\n",
      "Epoch: 0234 train_loss= 2.46286 train/struct_loss= 7.61866 train/feat_loss= 1.17391\n",
      "Epoch: 0235 train_loss= 2.46294 train/struct_loss= 7.61908 train/feat_loss= 1.17391\n",
      "Epoch: 0236 train_loss= 2.46299 train/struct_loss= 7.61933 train/feat_loss= 1.17391\n",
      "Epoch: 0237 train_loss= 2.46293 train/struct_loss= 7.61899 train/feat_loss= 1.17391\n",
      "Epoch: 0238 train_loss= 2.46293 train/struct_loss= 7.61901 train/feat_loss= 1.17391\n",
      "Epoch: 0239 train_loss= 2.46287 train/struct_loss= 7.61871 train/feat_loss= 1.17391\n",
      "Epoch: 0240 train_loss= 2.46280 train/struct_loss= 7.61835 train/feat_loss= 1.17391\n",
      "Epoch: 0240 Auc 0.8144736517575999\n",
      "Epoch: 0241 train_loss= 2.46311 train/struct_loss= 7.61995 train/feat_loss= 1.17391\n",
      "Epoch: 0242 train_loss= 2.46288 train/struct_loss= 7.61878 train/feat_loss= 1.17391\n",
      "Epoch: 0243 train_loss= 2.46272 train/struct_loss= 7.61799 train/feat_loss= 1.17391\n",
      "Epoch: 0244 train_loss= 2.46265 train/struct_loss= 7.61766 train/feat_loss= 1.17390\n",
      "Epoch: 0245 train_loss= 2.46295 train/struct_loss= 7.61915 train/feat_loss= 1.17390\n",
      "Epoch: 0246 train_loss= 2.46281 train/struct_loss= 7.61841 train/feat_loss= 1.17390\n",
      "Epoch: 0247 train_loss= 2.46282 train/struct_loss= 7.61851 train/feat_loss= 1.17390\n",
      "Epoch: 0248 train_loss= 2.46267 train/struct_loss= 7.61775 train/feat_loss= 1.17390\n",
      "Epoch: 0249 train_loss= 2.46278 train/struct_loss= 7.61831 train/feat_loss= 1.17390\n",
      "Epoch: 0250 train_loss= 2.46282 train/struct_loss= 7.61846 train/feat_loss= 1.17390\n",
      "Epoch: 0250 Auc 0.8144681708189345\n",
      "Epoch: 0251 train_loss= 2.46273 train/struct_loss= 7.61802 train/feat_loss= 1.17390\n",
      "Epoch: 0252 train_loss= 2.46274 train/struct_loss= 7.61812 train/feat_loss= 1.17390\n",
      "Epoch: 0253 train_loss= 2.46273 train/struct_loss= 7.61804 train/feat_loss= 1.17390\n",
      "Epoch: 0254 train_loss= 2.46261 train/struct_loss= 7.61746 train/feat_loss= 1.17390\n",
      "Epoch: 0255 train_loss= 2.46261 train/struct_loss= 7.61748 train/feat_loss= 1.17390\n",
      "Epoch: 0256 train_loss= 2.46278 train/struct_loss= 7.61832 train/feat_loss= 1.17390\n",
      "Epoch: 0257 train_loss= 2.46271 train/struct_loss= 7.61799 train/feat_loss= 1.17389\n",
      "Epoch: 0258 train_loss= 2.46276 train/struct_loss= 7.61822 train/feat_loss= 1.17389\n",
      "Epoch: 0259 train_loss= 2.46272 train/struct_loss= 7.61804 train/feat_loss= 1.17389\n",
      "Epoch: 0260 train_loss= 2.46258 train/struct_loss= 7.61732 train/feat_loss= 1.17389\n",
      "Epoch: 0260 Auc 0.8144729666402668\n",
      "Epoch: 0261 train_loss= 2.46273 train/struct_loss= 7.61805 train/feat_loss= 1.17389\n",
      "Epoch: 0262 train_loss= 2.46272 train/struct_loss= 7.61805 train/feat_loss= 1.17389\n",
      "Epoch: 0263 train_loss= 2.46277 train/struct_loss= 7.61830 train/feat_loss= 1.17389\n",
      "Epoch: 0264 train_loss= 2.46270 train/struct_loss= 7.61793 train/feat_loss= 1.17389\n",
      "Epoch: 0265 train_loss= 2.46263 train/struct_loss= 7.61758 train/feat_loss= 1.17389\n",
      "Epoch: 0266 train_loss= 2.46259 train/struct_loss= 7.61742 train/feat_loss= 1.17389\n",
      "Epoch: 0267 train_loss= 2.46267 train/struct_loss= 7.61783 train/feat_loss= 1.17389\n",
      "Epoch: 0268 train_loss= 2.46269 train/struct_loss= 7.61789 train/feat_loss= 1.17388\n",
      "Epoch: 0269 train_loss= 2.46263 train/struct_loss= 7.61759 train/feat_loss= 1.17388\n",
      "Epoch: 0270 train_loss= 2.46261 train/struct_loss= 7.61752 train/feat_loss= 1.17388\n",
      "Epoch: 0270 Auc 0.8144805029309319\n",
      "Epoch: 0271 train_loss= 2.46262 train/struct_loss= 7.61758 train/feat_loss= 1.17388\n",
      "Epoch: 0272 train_loss= 2.46272 train/struct_loss= 7.61806 train/feat_loss= 1.17388\n",
      "Epoch: 0273 train_loss= 2.46277 train/struct_loss= 7.61832 train/feat_loss= 1.17388\n",
      "Epoch: 0274 train_loss= 2.46267 train/struct_loss= 7.61782 train/feat_loss= 1.17388\n",
      "Epoch: 0275 train_loss= 2.46256 train/struct_loss= 7.61728 train/feat_loss= 1.17388\n",
      "Epoch: 0276 train_loss= 2.46246 train/struct_loss= 7.61675 train/feat_loss= 1.17388\n",
      "Epoch: 0277 train_loss= 2.46246 train/struct_loss= 7.61678 train/feat_loss= 1.17388\n",
      "Epoch: 0278 train_loss= 2.46262 train/struct_loss= 7.61757 train/feat_loss= 1.17388\n",
      "Epoch: 0279 train_loss= 2.46245 train/struct_loss= 7.61673 train/feat_loss= 1.17388\n",
      "Epoch: 0280 train_loss= 2.46250 train/struct_loss= 7.61696 train/feat_loss= 1.17388\n",
      "Epoch: 0280 Auc 0.8144969457469287\n",
      "Epoch: 0281 train_loss= 2.46244 train/struct_loss= 7.61668 train/feat_loss= 1.17388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0282 train_loss= 2.46259 train/struct_loss= 7.61745 train/feat_loss= 1.17388\n",
      "Epoch: 0283 train_loss= 2.46259 train/struct_loss= 7.61744 train/feat_loss= 1.17387\n",
      "Epoch: 0284 train_loss= 2.46251 train/struct_loss= 7.61707 train/feat_loss= 1.17387\n",
      "Epoch: 0285 train_loss= 2.46260 train/struct_loss= 7.61750 train/feat_loss= 1.17387\n",
      "Epoch: 0286 train_loss= 2.46252 train/struct_loss= 7.61713 train/feat_loss= 1.17387\n",
      "Epoch: 0287 train_loss= 2.46256 train/struct_loss= 7.61729 train/feat_loss= 1.17387\n",
      "Epoch: 0288 train_loss= 2.46261 train/struct_loss= 7.61758 train/feat_loss= 1.17387\n",
      "Epoch: 0289 train_loss= 2.46255 train/struct_loss= 7.61726 train/feat_loss= 1.17387\n",
      "Epoch: 0290 train_loss= 2.46237 train/struct_loss= 7.61640 train/feat_loss= 1.17387\n",
      "Epoch: 0290 Auc 0.8145113332109257\n",
      "Epoch: 0291 train_loss= 2.46237 train/struct_loss= 7.61639 train/feat_loss= 1.17387\n",
      "Epoch: 0292 train_loss= 2.46248 train/struct_loss= 7.61695 train/feat_loss= 1.17387\n",
      "Epoch: 0293 train_loss= 2.46237 train/struct_loss= 7.61638 train/feat_loss= 1.17387\n",
      "Epoch: 0294 train_loss= 2.46248 train/struct_loss= 7.61692 train/feat_loss= 1.17387\n",
      "Epoch: 0295 train_loss= 2.46237 train/struct_loss= 7.61639 train/feat_loss= 1.17387\n",
      "Epoch: 0296 train_loss= 2.46253 train/struct_loss= 7.61720 train/feat_loss= 1.17386\n",
      "Epoch: 0297 train_loss= 2.46246 train/struct_loss= 7.61682 train/feat_loss= 1.17386\n",
      "Epoch: 0298 train_loss= 2.46250 train/struct_loss= 7.61702 train/feat_loss= 1.17386\n",
      "Epoch: 0299 train_loss= 2.46249 train/struct_loss= 7.61701 train/feat_loss= 1.17386\n",
      "Epoch: 0300 train_loss= 2.46241 train/struct_loss= 7.61661 train/feat_loss= 1.17386\n",
      "Epoch: 0300 Auc 0.8145099629762593\n",
      "Epoch: 0301 train_loss= 2.46250 train/struct_loss= 7.61707 train/feat_loss= 1.17386\n",
      "Epoch: 0302 train_loss= 2.46241 train/struct_loss= 7.61663 train/feat_loss= 1.17386\n",
      "Epoch: 0303 train_loss= 2.46239 train/struct_loss= 7.61650 train/feat_loss= 1.17386\n",
      "Epoch: 0304 train_loss= 2.46250 train/struct_loss= 7.61707 train/feat_loss= 1.17386\n",
      "Epoch: 0305 train_loss= 2.46235 train/struct_loss= 7.61630 train/feat_loss= 1.17386\n",
      "Epoch: 0306 train_loss= 2.46259 train/struct_loss= 7.61753 train/feat_loss= 1.17386\n",
      "Epoch: 0307 train_loss= 2.46228 train/struct_loss= 7.61598 train/feat_loss= 1.17386\n",
      "Epoch: 0308 train_loss= 2.46238 train/struct_loss= 7.61646 train/feat_loss= 1.17386\n",
      "Epoch: 0309 train_loss= 2.46250 train/struct_loss= 7.61706 train/feat_loss= 1.17386\n",
      "Epoch: 0310 train_loss= 2.46238 train/struct_loss= 7.61647 train/feat_loss= 1.17386\n",
      "Epoch: 0310 Auc 0.8145250355575895\n",
      "Epoch: 0311 train_loss= 2.46228 train/struct_loss= 7.61599 train/feat_loss= 1.17386\n",
      "Epoch: 0312 train_loss= 2.46235 train/struct_loss= 7.61635 train/feat_loss= 1.17385\n",
      "Epoch: 0313 train_loss= 2.46245 train/struct_loss= 7.61686 train/feat_loss= 1.17385\n",
      "Epoch: 0314 train_loss= 2.46230 train/struct_loss= 7.61609 train/feat_loss= 1.17385\n",
      "Epoch: 0315 train_loss= 2.46230 train/struct_loss= 7.61610 train/feat_loss= 1.17385\n",
      "Epoch: 0316 train_loss= 2.46238 train/struct_loss= 7.61652 train/feat_loss= 1.17385\n",
      "Epoch: 0317 train_loss= 2.46228 train/struct_loss= 7.61600 train/feat_loss= 1.17385\n",
      "Epoch: 0318 train_loss= 2.46232 train/struct_loss= 7.61623 train/feat_loss= 1.17385\n",
      "Epoch: 0319 train_loss= 2.46224 train/struct_loss= 7.61580 train/feat_loss= 1.17385\n",
      "Epoch: 0320 train_loss= 2.46225 train/struct_loss= 7.61585 train/feat_loss= 1.17384\n",
      "Epoch: 0320 Auc 0.8145568935135832\n",
      "Epoch: 0321 train_loss= 2.46221 train/struct_loss= 7.61565 train/feat_loss= 1.17384\n",
      "Epoch: 0322 train_loss= 2.46234 train/struct_loss= 7.61634 train/feat_loss= 1.17384\n",
      "Epoch: 0323 train_loss= 2.46234 train/struct_loss= 7.61633 train/feat_loss= 1.17385\n",
      "Epoch: 0324 train_loss= 2.46225 train/struct_loss= 7.61587 train/feat_loss= 1.17384\n",
      "Epoch: 0325 train_loss= 2.46244 train/struct_loss= 7.61684 train/feat_loss= 1.17385\n",
      "Epoch: 0326 train_loss= 2.46216 train/struct_loss= 7.61542 train/feat_loss= 1.17384\n",
      "Epoch: 0327 train_loss= 2.46216 train/struct_loss= 7.61545 train/feat_loss= 1.17384\n",
      "Epoch: 0328 train_loss= 2.46236 train/struct_loss= 7.61643 train/feat_loss= 1.17384\n",
      "Epoch: 0329 train_loss= 2.46218 train/struct_loss= 7.61556 train/feat_loss= 1.17384\n",
      "Epoch: 0330 train_loss= 2.46232 train/struct_loss= 7.61622 train/feat_loss= 1.17384\n",
      "Epoch: 0330 Auc 0.8145544956029169\n",
      "Epoch: 0331 train_loss= 2.46226 train/struct_loss= 7.61596 train/feat_loss= 1.17384\n",
      "Epoch: 0332 train_loss= 2.46234 train/struct_loss= 7.61635 train/feat_loss= 1.17384\n",
      "Epoch: 0333 train_loss= 2.46232 train/struct_loss= 7.61628 train/feat_loss= 1.17383\n",
      "Epoch: 0334 train_loss= 2.46218 train/struct_loss= 7.61559 train/feat_loss= 1.17383\n",
      "Epoch: 0335 train_loss= 2.46221 train/struct_loss= 7.61575 train/feat_loss= 1.17383\n",
      "Epoch: 0336 train_loss= 2.46217 train/struct_loss= 7.61556 train/feat_loss= 1.17383\n",
      "Epoch: 0337 train_loss= 2.46224 train/struct_loss= 7.61592 train/feat_loss= 1.17382\n",
      "Epoch: 0338 train_loss= 2.46214 train/struct_loss= 7.61537 train/feat_loss= 1.17383\n",
      "Epoch: 0339 train_loss= 2.46220 train/struct_loss= 7.61570 train/feat_loss= 1.17383\n",
      "Epoch: 0340 train_loss= 2.46208 train/struct_loss= 7.61511 train/feat_loss= 1.17383\n",
      "Epoch: 0340 Auc 0.8145812151789116\n",
      "Epoch: 0341 train_loss= 2.46238 train/struct_loss= 7.61660 train/feat_loss= 1.17382\n",
      "Epoch: 0342 train_loss= 2.46205 train/struct_loss= 7.61494 train/feat_loss= 1.17382\n",
      "Epoch: 0343 train_loss= 2.46218 train/struct_loss= 7.61563 train/feat_loss= 1.17382\n",
      "Epoch: 0344 train_loss= 2.46212 train/struct_loss= 7.61532 train/feat_loss= 1.17382\n",
      "Epoch: 0345 train_loss= 2.46215 train/struct_loss= 7.61548 train/feat_loss= 1.17382\n",
      "Epoch: 0346 train_loss= 2.46207 train/struct_loss= 7.61506 train/feat_loss= 1.17382\n",
      "Epoch: 0347 train_loss= 2.46203 train/struct_loss= 7.61486 train/feat_loss= 1.17382\n",
      "Epoch: 0348 train_loss= 2.46206 train/struct_loss= 7.61502 train/feat_loss= 1.17382\n",
      "Epoch: 0349 train_loss= 2.46224 train/struct_loss= 7.61593 train/feat_loss= 1.17382\n",
      "Epoch: 0350 train_loss= 2.46213 train/struct_loss= 7.61538 train/feat_loss= 1.17382\n",
      "Epoch: 0350 Auc 0.814592862173576\n",
      "Epoch: 0351 train_loss= 2.46229 train/struct_loss= 7.61618 train/feat_loss= 1.17382\n",
      "Epoch: 0352 train_loss= 2.46202 train/struct_loss= 7.61482 train/feat_loss= 1.17381\n",
      "Epoch: 0353 train_loss= 2.46230 train/struct_loss= 7.61625 train/feat_loss= 1.17381\n",
      "Epoch: 0354 train_loss= 2.46218 train/struct_loss= 7.61564 train/feat_loss= 1.17381\n",
      "Epoch: 0355 train_loss= 2.46217 train/struct_loss= 7.61561 train/feat_loss= 1.17381\n",
      "Epoch: 0356 train_loss= 2.46228 train/struct_loss= 7.61617 train/feat_loss= 1.17381\n",
      "Epoch: 0357 train_loss= 2.46222 train/struct_loss= 7.61586 train/feat_loss= 1.17381\n",
      "Epoch: 0358 train_loss= 2.46225 train/struct_loss= 7.61600 train/feat_loss= 1.17381\n",
      "Epoch: 0359 train_loss= 2.46217 train/struct_loss= 7.61559 train/feat_loss= 1.17381\n",
      "Epoch: 0360 train_loss= 2.46203 train/struct_loss= 7.61495 train/feat_loss= 1.17381\n",
      "Epoch: 0360 Auc 0.8145887514695765\n",
      "Epoch: 0361 train_loss= 2.46199 train/struct_loss= 7.61475 train/feat_loss= 1.17380\n",
      "Epoch: 0362 train_loss= 2.46215 train/struct_loss= 7.61556 train/feat_loss= 1.17380\n",
      "Epoch: 0363 train_loss= 2.46208 train/struct_loss= 7.61516 train/feat_loss= 1.17380\n",
      "Epoch: 0364 train_loss= 2.46198 train/struct_loss= 7.61470 train/feat_loss= 1.17380\n",
      "Epoch: 0365 train_loss= 2.46200 train/struct_loss= 7.61478 train/feat_loss= 1.17380\n",
      "Epoch: 0366 train_loss= 2.46209 train/struct_loss= 7.61522 train/feat_loss= 1.17380\n",
      "Epoch: 0367 train_loss= 2.46214 train/struct_loss= 7.61552 train/feat_loss= 1.17380\n",
      "Epoch: 0368 train_loss= 2.46215 train/struct_loss= 7.61557 train/feat_loss= 1.17380\n",
      "Epoch: 0369 train_loss= 2.46201 train/struct_loss= 7.61487 train/feat_loss= 1.17380\n",
      "Epoch: 0370 train_loss= 2.46210 train/struct_loss= 7.61532 train/feat_loss= 1.17379\n",
      "Epoch: 0370 Auc 0.8145914919389095\n",
      "Epoch: 0371 train_loss= 2.46196 train/struct_loss= 7.61461 train/feat_loss= 1.17380\n",
      "Epoch: 0372 train_loss= 2.46196 train/struct_loss= 7.61463 train/feat_loss= 1.17379\n",
      "Epoch: 0373 train_loss= 2.46204 train/struct_loss= 7.61506 train/feat_loss= 1.17379\n",
      "Epoch: 0374 train_loss= 2.46206 train/struct_loss= 7.61513 train/feat_loss= 1.17379\n",
      "Epoch: 0375 train_loss= 2.46203 train/struct_loss= 7.61500 train/feat_loss= 1.17379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0376 train_loss= 2.46198 train/struct_loss= 7.61477 train/feat_loss= 1.17379\n",
      "Epoch: 0377 train_loss= 2.46193 train/struct_loss= 7.61450 train/feat_loss= 1.17379\n",
      "Epoch: 0378 train_loss= 2.46205 train/struct_loss= 7.61510 train/feat_loss= 1.17379\n",
      "Epoch: 0379 train_loss= 2.46192 train/struct_loss= 7.61446 train/feat_loss= 1.17378\n",
      "Epoch: 0380 train_loss= 2.46209 train/struct_loss= 7.61534 train/feat_loss= 1.17378\n",
      "Epoch: 0380 Auc 0.8146168412802377\n",
      "Epoch: 0381 train_loss= 2.46189 train/struct_loss= 7.61433 train/feat_loss= 1.17378\n",
      "Epoch: 0382 train_loss= 2.46211 train/struct_loss= 7.61544 train/feat_loss= 1.17378\n",
      "Epoch: 0383 train_loss= 2.46219 train/struct_loss= 7.61583 train/feat_loss= 1.17378\n",
      "Epoch: 0384 train_loss= 2.46219 train/struct_loss= 7.61582 train/feat_loss= 1.17378\n",
      "Epoch: 0385 train_loss= 2.46207 train/struct_loss= 7.61522 train/feat_loss= 1.17378\n",
      "Epoch: 0386 train_loss= 2.46199 train/struct_loss= 7.61483 train/feat_loss= 1.17378\n",
      "Epoch: 0387 train_loss= 2.46194 train/struct_loss= 7.61461 train/feat_loss= 1.17378\n",
      "Epoch: 0388 train_loss= 2.46203 train/struct_loss= 7.61507 train/feat_loss= 1.17377\n",
      "Epoch: 0389 train_loss= 2.46192 train/struct_loss= 7.61450 train/feat_loss= 1.17377\n",
      "Epoch: 0390 train_loss= 2.46192 train/struct_loss= 7.61452 train/feat_loss= 1.17378\n",
      "Epoch: 0390 Auc 0.8145956026429085\n",
      "Epoch: 0391 train_loss= 2.46218 train/struct_loss= 7.61579 train/feat_loss= 1.17377\n",
      "Epoch: 0392 train_loss= 2.46189 train/struct_loss= 7.61438 train/feat_loss= 1.17377\n",
      "Epoch: 0393 train_loss= 2.46201 train/struct_loss= 7.61495 train/feat_loss= 1.17378\n",
      "Epoch: 0394 train_loss= 2.46209 train/struct_loss= 7.61535 train/feat_loss= 1.17377\n",
      "Epoch: 0395 train_loss= 2.46202 train/struct_loss= 7.61501 train/feat_loss= 1.17377\n",
      "Epoch: 0396 train_loss= 2.46207 train/struct_loss= 7.61524 train/feat_loss= 1.17377\n",
      "Epoch: 0397 train_loss= 2.46212 train/struct_loss= 7.61551 train/feat_loss= 1.17377\n",
      "Epoch: 0398 train_loss= 2.46174 train/struct_loss= 7.61362 train/feat_loss= 1.17377\n",
      "Epoch: 0399 train_loss= 2.46204 train/struct_loss= 7.61512 train/feat_loss= 1.17377\n",
      "Epoch: 0400 train_loss= 2.46221 train/struct_loss= 7.61600 train/feat_loss= 1.17377\n",
      "Epoch: 0400 Auc 0.8146223222189031\n",
      "Epoch: 0401 train_loss= 2.46205 train/struct_loss= 7.61520 train/feat_loss= 1.17377\n",
      "Epoch: 0402 train_loss= 2.46204 train/struct_loss= 7.61511 train/feat_loss= 1.17377\n",
      "Epoch: 0403 train_loss= 2.46214 train/struct_loss= 7.61565 train/feat_loss= 1.17376\n",
      "Epoch: 0404 train_loss= 2.46181 train/struct_loss= 7.61401 train/feat_loss= 1.17376\n",
      "Epoch: 0405 train_loss= 2.46183 train/struct_loss= 7.61410 train/feat_loss= 1.17377\n",
      "Epoch: 0406 train_loss= 2.46202 train/struct_loss= 7.61507 train/feat_loss= 1.17376\n",
      "Epoch: 0407 train_loss= 2.46197 train/struct_loss= 7.61480 train/feat_loss= 1.17376\n",
      "Epoch: 0408 train_loss= 2.46184 train/struct_loss= 7.61418 train/feat_loss= 1.17376\n",
      "Epoch: 0409 train_loss= 2.46188 train/struct_loss= 7.61437 train/feat_loss= 1.17375\n",
      "Epoch: 0410 train_loss= 2.46204 train/struct_loss= 7.61516 train/feat_loss= 1.17376\n",
      "Epoch: 0410 Auc 0.8146490417948978\n",
      "Epoch: 0411 train_loss= 2.46194 train/struct_loss= 7.61469 train/feat_loss= 1.17376\n",
      "Epoch: 0412 train_loss= 2.46197 train/struct_loss= 7.61484 train/feat_loss= 1.17376\n",
      "Epoch: 0413 train_loss= 2.46180 train/struct_loss= 7.61397 train/feat_loss= 1.17375\n",
      "Epoch: 0414 train_loss= 2.46196 train/struct_loss= 7.61478 train/feat_loss= 1.17375\n",
      "Epoch: 0415 train_loss= 2.46186 train/struct_loss= 7.61429 train/feat_loss= 1.17375\n",
      "Epoch: 0416 train_loss= 2.46187 train/struct_loss= 7.61437 train/feat_loss= 1.17375\n",
      "Epoch: 0417 train_loss= 2.46179 train/struct_loss= 7.61398 train/feat_loss= 1.17375\n",
      "Epoch: 0418 train_loss= 2.46209 train/struct_loss= 7.61545 train/feat_loss= 1.17375\n",
      "Epoch: 0419 train_loss= 2.46175 train/struct_loss= 7.61378 train/feat_loss= 1.17375\n",
      "Epoch: 0420 train_loss= 2.46185 train/struct_loss= 7.61425 train/feat_loss= 1.17375\n",
      "Epoch: 0420 Auc 0.8145962877602417\n",
      "Epoch: 0421 train_loss= 2.46204 train/struct_loss= 7.61520 train/feat_loss= 1.17375\n",
      "Epoch: 0422 train_loss= 2.46174 train/struct_loss= 7.61373 train/feat_loss= 1.17374\n",
      "Epoch: 0423 train_loss= 2.46200 train/struct_loss= 7.61503 train/feat_loss= 1.17374\n",
      "Epoch: 0424 train_loss= 2.46183 train/struct_loss= 7.61418 train/feat_loss= 1.17374\n",
      "Epoch: 0425 train_loss= 2.46173 train/struct_loss= 7.61368 train/feat_loss= 1.17374\n",
      "Epoch: 0426 train_loss= 2.46168 train/struct_loss= 7.61342 train/feat_loss= 1.17374\n",
      "Epoch: 0427 train_loss= 2.46187 train/struct_loss= 7.61440 train/feat_loss= 1.17374\n",
      "Epoch: 0428 train_loss= 2.46185 train/struct_loss= 7.61431 train/feat_loss= 1.17374\n",
      "Epoch: 0429 train_loss= 2.46182 train/struct_loss= 7.61414 train/feat_loss= 1.17374\n",
      "Epoch: 0430 train_loss= 2.46172 train/struct_loss= 7.61369 train/feat_loss= 1.17373\n",
      "Epoch: 0430 Auc 0.8146558929682297\n",
      "Epoch: 0431 train_loss= 2.46177 train/struct_loss= 7.61394 train/feat_loss= 1.17373\n",
      "Epoch: 0432 train_loss= 2.46203 train/struct_loss= 7.61522 train/feat_loss= 1.17373\n",
      "Epoch: 0433 train_loss= 2.46181 train/struct_loss= 7.61413 train/feat_loss= 1.17372\n",
      "Epoch: 0434 train_loss= 2.46165 train/struct_loss= 7.61336 train/feat_loss= 1.17372\n",
      "Epoch: 0435 train_loss= 2.46175 train/struct_loss= 7.61388 train/feat_loss= 1.17372\n",
      "Epoch: 0436 train_loss= 2.46175 train/struct_loss= 7.61388 train/feat_loss= 1.17372\n",
      "Epoch: 0437 train_loss= 2.46168 train/struct_loss= 7.61352 train/feat_loss= 1.17372\n",
      "Epoch: 0438 train_loss= 2.46184 train/struct_loss= 7.61434 train/feat_loss= 1.17372\n",
      "Epoch: 0439 train_loss= 2.46161 train/struct_loss= 7.61315 train/feat_loss= 1.17372\n",
      "Epoch: 0440 train_loss= 2.46169 train/struct_loss= 7.61358 train/feat_loss= 1.17372\n",
      "Epoch: 0440 Auc 0.8146668548455608\n",
      "Epoch: 0441 train_loss= 2.46169 train/struct_loss= 7.61360 train/feat_loss= 1.17371\n",
      "Epoch: 0442 train_loss= 2.46179 train/struct_loss= 7.61410 train/feat_loss= 1.17371\n",
      "Epoch: 0443 train_loss= 2.46179 train/struct_loss= 7.61409 train/feat_loss= 1.17371\n",
      "Epoch: 0444 train_loss= 2.46165 train/struct_loss= 7.61337 train/feat_loss= 1.17371\n",
      "Epoch: 0445 train_loss= 2.46173 train/struct_loss= 7.61380 train/feat_loss= 1.17371\n",
      "Epoch: 0446 train_loss= 2.46176 train/struct_loss= 7.61394 train/feat_loss= 1.17371\n",
      "Epoch: 0447 train_loss= 2.46175 train/struct_loss= 7.61390 train/feat_loss= 1.17371\n",
      "Epoch: 0448 train_loss= 2.46162 train/struct_loss= 7.61327 train/feat_loss= 1.17371\n",
      "Epoch: 0449 train_loss= 2.46167 train/struct_loss= 7.61353 train/feat_loss= 1.17371\n",
      "Epoch: 0450 train_loss= 2.46173 train/struct_loss= 7.61385 train/feat_loss= 1.17371\n",
      "Epoch: 0450 Auc 0.8146497269122309\n",
      "Epoch: 0451 train_loss= 2.46156 train/struct_loss= 7.61301 train/feat_loss= 1.17370\n",
      "Epoch: 0452 train_loss= 2.46174 train/struct_loss= 7.61387 train/feat_loss= 1.17370\n",
      "Epoch: 0453 train_loss= 2.46169 train/struct_loss= 7.61365 train/feat_loss= 1.17370\n",
      "Epoch: 0454 train_loss= 2.46164 train/struct_loss= 7.61340 train/feat_loss= 1.17370\n",
      "Epoch: 0455 train_loss= 2.46166 train/struct_loss= 7.61349 train/feat_loss= 1.17370\n",
      "Epoch: 0456 train_loss= 2.46172 train/struct_loss= 7.61379 train/feat_loss= 1.17370\n",
      "Epoch: 0457 train_loss= 2.46178 train/struct_loss= 7.61409 train/feat_loss= 1.17370\n",
      "Epoch: 0458 train_loss= 2.46175 train/struct_loss= 7.61395 train/feat_loss= 1.17370\n",
      "Epoch: 0459 train_loss= 2.46155 train/struct_loss= 7.61297 train/feat_loss= 1.17370\n",
      "Epoch: 0460 train_loss= 2.46171 train/struct_loss= 7.61377 train/feat_loss= 1.17370\n",
      "Epoch: 0460 Auc 0.8146709655495601\n",
      "Epoch: 0461 train_loss= 2.46166 train/struct_loss= 7.61353 train/feat_loss= 1.17369\n",
      "Epoch: 0462 train_loss= 2.46142 train/struct_loss= 7.61232 train/feat_loss= 1.17369\n",
      "Epoch: 0463 train_loss= 2.46167 train/struct_loss= 7.61360 train/feat_loss= 1.17369\n",
      "Epoch: 0464 train_loss= 2.46144 train/struct_loss= 7.61245 train/feat_loss= 1.17368\n",
      "Epoch: 0465 train_loss= 2.46157 train/struct_loss= 7.61309 train/feat_loss= 1.17369\n",
      "Epoch: 0466 train_loss= 2.46149 train/struct_loss= 7.61273 train/feat_loss= 1.17369\n",
      "Epoch: 0467 train_loss= 2.46160 train/struct_loss= 7.61328 train/feat_loss= 1.17368\n",
      "Epoch: 0468 train_loss= 2.46158 train/struct_loss= 7.61314 train/feat_loss= 1.17369\n",
      "Epoch: 0469 train_loss= 2.46153 train/struct_loss= 7.61294 train/feat_loss= 1.17368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0470 train_loss= 2.46149 train/struct_loss= 7.61271 train/feat_loss= 1.17368\n",
      "Epoch: 0470 Auc 0.8146600036722289\n",
      "Epoch: 0471 train_loss= 2.46166 train/struct_loss= 7.61359 train/feat_loss= 1.17368\n",
      "Epoch: 0472 train_loss= 2.46147 train/struct_loss= 7.61262 train/feat_loss= 1.17368\n",
      "Epoch: 0473 train_loss= 2.46149 train/struct_loss= 7.61276 train/feat_loss= 1.17368\n",
      "Epoch: 0474 train_loss= 2.46156 train/struct_loss= 7.61307 train/feat_loss= 1.17368\n",
      "Epoch: 0475 train_loss= 2.46140 train/struct_loss= 7.61232 train/feat_loss= 1.17368\n",
      "Epoch: 0476 train_loss= 2.46159 train/struct_loss= 7.61321 train/feat_loss= 1.17368\n",
      "Epoch: 0477 train_loss= 2.46152 train/struct_loss= 7.61292 train/feat_loss= 1.17368\n",
      "Epoch: 0478 train_loss= 2.46150 train/struct_loss= 7.61282 train/feat_loss= 1.17367\n",
      "Epoch: 0479 train_loss= 2.46142 train/struct_loss= 7.61237 train/feat_loss= 1.17368\n",
      "Epoch: 0480 train_loss= 2.46165 train/struct_loss= 7.61354 train/feat_loss= 1.17368\n",
      "Epoch: 0480 Auc 0.8146702804322268\n",
      "Epoch: 0481 train_loss= 2.46142 train/struct_loss= 7.61237 train/feat_loss= 1.17368\n",
      "Epoch: 0482 train_loss= 2.46135 train/struct_loss= 7.61204 train/feat_loss= 1.17367\n",
      "Epoch: 0483 train_loss= 2.46142 train/struct_loss= 7.61241 train/feat_loss= 1.17367\n",
      "Epoch: 0484 train_loss= 2.46156 train/struct_loss= 7.61312 train/feat_loss= 1.17367\n",
      "Epoch: 0485 train_loss= 2.46144 train/struct_loss= 7.61253 train/feat_loss= 1.17367\n",
      "Epoch: 0486 train_loss= 2.46160 train/struct_loss= 7.61332 train/feat_loss= 1.17367\n",
      "Epoch: 0487 train_loss= 2.46136 train/struct_loss= 7.61212 train/feat_loss= 1.17367\n",
      "Epoch: 0488 train_loss= 2.46147 train/struct_loss= 7.61269 train/feat_loss= 1.17367\n",
      "Epoch: 0489 train_loss= 2.46154 train/struct_loss= 7.61302 train/feat_loss= 1.17367\n",
      "Epoch: 0490 train_loss= 2.46142 train/struct_loss= 7.61246 train/feat_loss= 1.17366\n",
      "Epoch: 0490 Auc 0.8146908339522226\n",
      "Epoch: 0491 train_loss= 2.46128 train/struct_loss= 7.61176 train/feat_loss= 1.17366\n",
      "Epoch: 0492 train_loss= 2.46158 train/struct_loss= 7.61323 train/feat_loss= 1.17366\n",
      "Epoch: 0493 train_loss= 2.46133 train/struct_loss= 7.61200 train/feat_loss= 1.17366\n",
      "Epoch: 0494 train_loss= 2.46136 train/struct_loss= 7.61216 train/feat_loss= 1.17366\n",
      "Epoch: 0495 train_loss= 2.46141 train/struct_loss= 7.61245 train/feat_loss= 1.17365\n",
      "Epoch: 0496 train_loss= 2.46132 train/struct_loss= 7.61196 train/feat_loss= 1.17366\n",
      "Epoch: 0497 train_loss= 2.46134 train/struct_loss= 7.61209 train/feat_loss= 1.17365\n",
      "Epoch: 0498 train_loss= 2.46126 train/struct_loss= 7.61167 train/feat_loss= 1.17365\n",
      "Epoch: 0499 train_loss= 2.46126 train/struct_loss= 7.61168 train/feat_loss= 1.17366\n",
      "Epoch: 0499 Auc 0.81468809348289\n"
     ]
    }
   ],
   "source": [
    "train_dominant(dataset=\"BlogCatalog\", hidden_dim=64, max_epoch=500, lr=5e-4, dropout=0.3, alpha=0.8, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "782bc7c6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train_loss= 4.34323 train/struct_loss= 16.68764 train/feat_loss= 1.25712\n",
      "Epoch: 0000 Auc 0.6479319048180192\n",
      "Epoch: 0001 train_loss= 14.08281 train/struct_loss= 52.20726 train/feat_loss= 4.55171\n",
      "Epoch: 0002 train_loss= 2.67803 train/struct_loss= 7.71949 train/feat_loss= 1.41767\n",
      "Epoch: 0003 train_loss= 2.50623 train/struct_loss= 7.68096 train/feat_loss= 1.21255\n",
      "Epoch: 0004 train_loss= 2.47950 train/struct_loss= 7.67807 train/feat_loss= 1.17986\n",
      "Epoch: 0005 train_loss= 2.51940 train/struct_loss= 7.88895 train/feat_loss= 1.17702\n",
      "Epoch: 0006 train_loss= 2.47858 train/struct_loss= 7.68228 train/feat_loss= 1.17765\n",
      "Epoch: 0007 train_loss= 2.48236 train/struct_loss= 7.71183 train/feat_loss= 1.17500\n",
      "Epoch: 0008 train_loss= 2.47954 train/struct_loss= 7.69773 train/feat_loss= 1.17500\n",
      "Epoch: 0009 train_loss= 2.47969 train/struct_loss= 7.69844 train/feat_loss= 1.17500\n",
      "Epoch: 0010 train_loss= 2.47787 train/struct_loss= 7.68937 train/feat_loss= 1.17500\n",
      "Epoch: 0010 Auc 0.8133027862351705\n",
      "Epoch: 0011 train_loss= 2.47739 train/struct_loss= 7.68696 train/feat_loss= 1.17500\n",
      "Epoch: 0012 train_loss= 2.47684 train/struct_loss= 7.68419 train/feat_loss= 1.17500\n",
      "Epoch: 0013 train_loss= 2.48229 train/struct_loss= 7.71146 train/feat_loss= 1.17500\n",
      "Epoch: 0014 train_loss= 2.47642 train/struct_loss= 7.68213 train/feat_loss= 1.17500\n",
      "Epoch: 0015 train_loss= 2.47766 train/struct_loss= 7.68831 train/feat_loss= 1.17500\n",
      "Epoch: 0016 train_loss= 2.47698 train/struct_loss= 7.68492 train/feat_loss= 1.17500\n",
      "Epoch: 0017 train_loss= 2.47655 train/struct_loss= 7.68277 train/feat_loss= 1.17500\n",
      "Epoch: 0018 train_loss= 2.47813 train/struct_loss= 7.69068 train/feat_loss= 1.17500\n",
      "Epoch: 0019 train_loss= 2.47551 train/struct_loss= 7.67755 train/feat_loss= 1.17500\n",
      "Epoch: 0020 train_loss= 2.47676 train/struct_loss= 7.68380 train/feat_loss= 1.17500\n",
      "Epoch: 0020 Auc 0.8134086368631492\n",
      "Epoch: 0021 train_loss= 2.47697 train/struct_loss= 7.68487 train/feat_loss= 1.17500\n",
      "Epoch: 0022 train_loss= 2.47371 train/struct_loss= 7.66854 train/feat_loss= 1.17500\n",
      "Epoch: 0023 train_loss= 2.47697 train/struct_loss= 7.68488 train/feat_loss= 1.17500\n",
      "Epoch: 0024 train_loss= 2.47312 train/struct_loss= 7.66560 train/feat_loss= 1.17500\n",
      "Epoch: 0025 train_loss= 2.47496 train/struct_loss= 7.67481 train/feat_loss= 1.17500\n",
      "Epoch: 0026 train_loss= 2.47519 train/struct_loss= 7.67596 train/feat_loss= 1.17500\n",
      "Epoch: 0027 train_loss= 2.47392 train/struct_loss= 7.66962 train/feat_loss= 1.17500\n",
      "Epoch: 0028 train_loss= 2.47360 train/struct_loss= 7.66802 train/feat_loss= 1.17500\n",
      "Epoch: 0029 train_loss= 2.47382 train/struct_loss= 7.66909 train/feat_loss= 1.17500\n",
      "Epoch: 0030 train_loss= 2.47177 train/struct_loss= 7.65884 train/feat_loss= 1.17500\n",
      "Epoch: 0030 Auc 0.8136535663097663\n",
      "Epoch: 0031 train_loss= 2.47335 train/struct_loss= 7.66678 train/feat_loss= 1.17500\n",
      "Epoch: 0032 train_loss= 2.47323 train/struct_loss= 7.66617 train/feat_loss= 1.17500\n",
      "Epoch: 0033 train_loss= 2.47404 train/struct_loss= 7.67020 train/feat_loss= 1.17500\n",
      "Epoch: 0034 train_loss= 2.47302 train/struct_loss= 7.66510 train/feat_loss= 1.17500\n",
      "Epoch: 0035 train_loss= 2.47262 train/struct_loss= 7.66309 train/feat_loss= 1.17500\n",
      "Epoch: 0036 train_loss= 2.47176 train/struct_loss= 7.65882 train/feat_loss= 1.17500\n",
      "Epoch: 0037 train_loss= 2.47184 train/struct_loss= 7.65923 train/feat_loss= 1.17500\n",
      "Epoch: 0038 train_loss= 2.47245 train/struct_loss= 7.66225 train/feat_loss= 1.17500\n",
      "Epoch: 0039 train_loss= 2.47259 train/struct_loss= 7.66294 train/feat_loss= 1.17500\n",
      "Epoch: 0040 train_loss= 2.47300 train/struct_loss= 7.66501 train/feat_loss= 1.17500\n",
      "Epoch: 0040 Auc 0.8138316968163969\n",
      "Epoch: 0041 train_loss= 2.47249 train/struct_loss= 7.66245 train/feat_loss= 1.17500\n",
      "Epoch: 0042 train_loss= 2.47182 train/struct_loss= 7.65912 train/feat_loss= 1.17500\n",
      "Epoch: 0043 train_loss= 2.47273 train/struct_loss= 7.66366 train/feat_loss= 1.17500\n",
      "Epoch: 0044 train_loss= 2.47228 train/struct_loss= 7.66142 train/feat_loss= 1.17500\n",
      "Epoch: 0045 train_loss= 2.47208 train/struct_loss= 7.66042 train/feat_loss= 1.17500\n",
      "Epoch: 0046 train_loss= 2.47258 train/struct_loss= 7.66292 train/feat_loss= 1.17500\n",
      "Epoch: 0047 train_loss= 2.47171 train/struct_loss= 7.65858 train/feat_loss= 1.17500\n",
      "Epoch: 0048 train_loss= 2.47202 train/struct_loss= 7.66012 train/feat_loss= 1.17500\n",
      "Epoch: 0049 train_loss= 2.47193 train/struct_loss= 7.65968 train/feat_loss= 1.17500\n",
      "Epoch: 0050 train_loss= 2.47173 train/struct_loss= 7.65863 train/feat_loss= 1.17500\n",
      "Epoch: 0050 Auc 0.8138570461577248\n",
      "Epoch: 0051 train_loss= 2.47230 train/struct_loss= 7.66148 train/feat_loss= 1.17500\n",
      "Epoch: 0052 train_loss= 2.47173 train/struct_loss= 7.65868 train/feat_loss= 1.17500\n",
      "Epoch: 0053 train_loss= 2.47189 train/struct_loss= 7.65944 train/feat_loss= 1.17500\n",
      "Epoch: 0054 train_loss= 2.47182 train/struct_loss= 7.65912 train/feat_loss= 1.17500\n",
      "Epoch: 0055 train_loss= 2.47203 train/struct_loss= 7.66016 train/feat_loss= 1.17500\n",
      "Epoch: 0056 train_loss= 2.47183 train/struct_loss= 7.65914 train/feat_loss= 1.17500\n",
      "Epoch: 0057 train_loss= 2.47182 train/struct_loss= 7.65908 train/feat_loss= 1.17500\n",
      "Epoch: 0058 train_loss= 2.47171 train/struct_loss= 7.65854 train/feat_loss= 1.17500\n",
      "Epoch: 0059 train_loss= 2.47219 train/struct_loss= 7.66098 train/feat_loss= 1.17500\n",
      "Epoch: 0060 train_loss= 2.47175 train/struct_loss= 7.65878 train/feat_loss= 1.17500\n",
      "Epoch: 0060 Auc 0.8138632122137237\n",
      "Epoch: 0061 train_loss= 2.47214 train/struct_loss= 7.66069 train/feat_loss= 1.17500\n",
      "Epoch: 0062 train_loss= 2.47203 train/struct_loss= 7.66013 train/feat_loss= 1.17500\n",
      "Epoch: 0063 train_loss= 2.47146 train/struct_loss= 7.65730 train/feat_loss= 1.17500\n",
      "Epoch: 0064 train_loss= 2.47222 train/struct_loss= 7.66109 train/feat_loss= 1.17500\n",
      "Epoch: 0065 train_loss= 2.47192 train/struct_loss= 7.65960 train/feat_loss= 1.17500\n",
      "Epoch: 0066 train_loss= 2.47220 train/struct_loss= 7.66100 train/feat_loss= 1.17500\n",
      "Epoch: 0067 train_loss= 2.47188 train/struct_loss= 7.65941 train/feat_loss= 1.17500\n",
      "Epoch: 0068 train_loss= 2.47181 train/struct_loss= 7.65907 train/feat_loss= 1.17500\n",
      "Epoch: 0069 train_loss= 2.47150 train/struct_loss= 7.65753 train/feat_loss= 1.17500\n",
      "Epoch: 0070 train_loss= 2.47178 train/struct_loss= 7.65889 train/feat_loss= 1.17500\n",
      "Epoch: 0070 Auc 0.8138193647043992\n",
      "Epoch: 0071 train_loss= 2.47184 train/struct_loss= 7.65918 train/feat_loss= 1.17500\n",
      "Epoch: 0072 train_loss= 2.47210 train/struct_loss= 7.66049 train/feat_loss= 1.17500\n",
      "Epoch: 0073 train_loss= 2.47154 train/struct_loss= 7.65772 train/feat_loss= 1.17500\n",
      "Epoch: 0074 train_loss= 2.47182 train/struct_loss= 7.65913 train/feat_loss= 1.17500\n",
      "Epoch: 0075 train_loss= 2.47219 train/struct_loss= 7.66094 train/feat_loss= 1.17500\n",
      "Epoch: 0076 train_loss= 2.47199 train/struct_loss= 7.65996 train/feat_loss= 1.17500\n",
      "Epoch: 0077 train_loss= 2.47198 train/struct_loss= 7.65989 train/feat_loss= 1.17500\n",
      "Epoch: 0078 train_loss= 2.47188 train/struct_loss= 7.65939 train/feat_loss= 1.17500\n",
      "Epoch: 0079 train_loss= 2.47212 train/struct_loss= 7.66062 train/feat_loss= 1.17500\n",
      "Epoch: 0080 train_loss= 2.47195 train/struct_loss= 7.65975 train/feat_loss= 1.17500\n",
      "Epoch: 0080 Auc 0.813835807520396\n",
      "Epoch: 0081 train_loss= 2.47158 train/struct_loss= 7.65790 train/feat_loss= 1.17500\n",
      "Epoch: 0082 train_loss= 2.47180 train/struct_loss= 7.65900 train/feat_loss= 1.17500\n",
      "Epoch: 0083 train_loss= 2.47169 train/struct_loss= 7.65845 train/feat_loss= 1.17500\n",
      "Epoch: 0084 train_loss= 2.47168 train/struct_loss= 7.65838 train/feat_loss= 1.17500\n",
      "Epoch: 0085 train_loss= 2.47215 train/struct_loss= 7.66078 train/feat_loss= 1.17500\n",
      "Epoch: 0086 train_loss= 2.47207 train/struct_loss= 7.66036 train/feat_loss= 1.17500\n",
      "Epoch: 0087 train_loss= 2.47145 train/struct_loss= 7.65728 train/feat_loss= 1.17500\n",
      "Epoch: 0088 train_loss= 2.47189 train/struct_loss= 7.65945 train/feat_loss= 1.17500\n",
      "Epoch: 0089 train_loss= 2.47176 train/struct_loss= 7.65879 train/feat_loss= 1.17500\n",
      "Epoch: 0090 train_loss= 2.47146 train/struct_loss= 7.65731 train/feat_loss= 1.17500\n",
      "Epoch: 0090 Auc 0.8138426586937278\n",
      "Epoch: 0091 train_loss= 2.47180 train/struct_loss= 7.65902 train/feat_loss= 1.17500\n",
      "Epoch: 0092 train_loss= 2.47159 train/struct_loss= 7.65794 train/feat_loss= 1.17500\n",
      "Epoch: 0093 train_loss= 2.47139 train/struct_loss= 7.65697 train/feat_loss= 1.17500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0094 train_loss= 2.47205 train/struct_loss= 7.66024 train/feat_loss= 1.17500\n",
      "Epoch: 0095 train_loss= 2.47147 train/struct_loss= 7.65734 train/feat_loss= 1.17500\n",
      "Epoch: 0096 train_loss= 2.47184 train/struct_loss= 7.65922 train/feat_loss= 1.17500\n",
      "Epoch: 0097 train_loss= 2.47201 train/struct_loss= 7.66006 train/feat_loss= 1.17500\n",
      "Epoch: 0098 train_loss= 2.47163 train/struct_loss= 7.65815 train/feat_loss= 1.17500\n",
      "Epoch: 0099 train_loss= 2.47213 train/struct_loss= 7.66067 train/feat_loss= 1.17500\n",
      "Epoch: 0100 train_loss= 2.47139 train/struct_loss= 7.65697 train/feat_loss= 1.17500\n",
      "Epoch: 0100 Auc 0.8138474545150602\n",
      "Epoch: 0101 train_loss= 2.47151 train/struct_loss= 7.65755 train/feat_loss= 1.17500\n",
      "Epoch: 0102 train_loss= 2.47147 train/struct_loss= 7.65737 train/feat_loss= 1.17500\n",
      "Epoch: 0103 train_loss= 2.47169 train/struct_loss= 7.65843 train/feat_loss= 1.17500\n",
      "Epoch: 0104 train_loss= 2.47185 train/struct_loss= 7.65927 train/feat_loss= 1.17500\n",
      "Epoch: 0105 train_loss= 2.47138 train/struct_loss= 7.65689 train/feat_loss= 1.17500\n",
      "Epoch: 0106 train_loss= 2.47165 train/struct_loss= 7.65826 train/feat_loss= 1.17500\n",
      "Epoch: 0107 train_loss= 2.47164 train/struct_loss= 7.65820 train/feat_loss= 1.17500\n",
      "Epoch: 0108 train_loss= 2.47185 train/struct_loss= 7.65924 train/feat_loss= 1.17500\n",
      "Epoch: 0109 train_loss= 2.47196 train/struct_loss= 7.65980 train/feat_loss= 1.17500\n",
      "Epoch: 0110 train_loss= 2.47170 train/struct_loss= 7.65852 train/feat_loss= 1.17500\n",
      "Epoch: 0110 Auc 0.8138275861123976\n",
      "Epoch: 0111 train_loss= 2.47198 train/struct_loss= 7.65991 train/feat_loss= 1.17500\n",
      "Epoch: 0112 train_loss= 2.47219 train/struct_loss= 7.66095 train/feat_loss= 1.17500\n",
      "Epoch: 0113 train_loss= 2.47214 train/struct_loss= 7.66070 train/feat_loss= 1.17500\n",
      "Epoch: 0114 train_loss= 2.47158 train/struct_loss= 7.65792 train/feat_loss= 1.17500\n",
      "Epoch: 0115 train_loss= 2.47171 train/struct_loss= 7.65855 train/feat_loss= 1.17500\n",
      "Epoch: 0116 train_loss= 2.47185 train/struct_loss= 7.65923 train/feat_loss= 1.17500\n",
      "Epoch: 0117 train_loss= 2.47196 train/struct_loss= 7.65982 train/feat_loss= 1.17500\n",
      "Epoch: 0118 train_loss= 2.47171 train/struct_loss= 7.65857 train/feat_loss= 1.17500\n",
      "Epoch: 0119 train_loss= 2.47173 train/struct_loss= 7.65865 train/feat_loss= 1.17500\n",
      "Epoch: 0120 train_loss= 2.47181 train/struct_loss= 7.65907 train/feat_loss= 1.17500\n",
      "Epoch: 0120 Auc 0.8138337521683963\n",
      "Epoch: 0121 train_loss= 2.47177 train/struct_loss= 7.65888 train/feat_loss= 1.17500\n",
      "Epoch: 0122 train_loss= 2.47167 train/struct_loss= 7.65835 train/feat_loss= 1.17500\n",
      "Epoch: 0123 train_loss= 2.47153 train/struct_loss= 7.65768 train/feat_loss= 1.17500\n",
      "Epoch: 0124 train_loss= 2.47239 train/struct_loss= 7.66195 train/feat_loss= 1.17500\n",
      "Epoch: 0125 train_loss= 2.47199 train/struct_loss= 7.65997 train/feat_loss= 1.17500\n",
      "Epoch: 0126 train_loss= 2.47195 train/struct_loss= 7.65978 train/feat_loss= 1.17500\n",
      "Epoch: 0127 train_loss= 2.47173 train/struct_loss= 7.65864 train/feat_loss= 1.17500\n",
      "Epoch: 0128 train_loss= 2.47252 train/struct_loss= 7.66259 train/feat_loss= 1.17500\n",
      "Epoch: 0129 train_loss= 2.47138 train/struct_loss= 7.65688 train/feat_loss= 1.17500\n",
      "Epoch: 0130 train_loss= 2.47186 train/struct_loss= 7.65933 train/feat_loss= 1.17500\n",
      "Epoch: 0130 Auc 0.8138296414643972\n",
      "Epoch: 0131 train_loss= 2.47163 train/struct_loss= 7.65814 train/feat_loss= 1.17500\n",
      "Epoch: 0132 train_loss= 2.47184 train/struct_loss= 7.65919 train/feat_loss= 1.17500\n",
      "Epoch: 0133 train_loss= 2.47183 train/struct_loss= 7.65918 train/feat_loss= 1.17500\n",
      "Epoch: 0134 train_loss= 2.47201 train/struct_loss= 7.66008 train/feat_loss= 1.17500\n",
      "Epoch: 0135 train_loss= 2.47146 train/struct_loss= 7.65733 train/feat_loss= 1.17500\n",
      "Epoch: 0136 train_loss= 2.47181 train/struct_loss= 7.65904 train/feat_loss= 1.17500\n",
      "Epoch: 0137 train_loss= 2.47146 train/struct_loss= 7.65731 train/feat_loss= 1.17500\n",
      "Epoch: 0138 train_loss= 2.47169 train/struct_loss= 7.65846 train/feat_loss= 1.17500\n",
      "Epoch: 0139 train_loss= 2.47217 train/struct_loss= 7.66087 train/feat_loss= 1.17500\n",
      "Epoch: 0140 train_loss= 2.47155 train/struct_loss= 7.65778 train/feat_loss= 1.17500\n",
      "Epoch: 0140 Auc 0.8138412884590613\n",
      "Epoch: 0141 train_loss= 2.47200 train/struct_loss= 7.66001 train/feat_loss= 1.17500\n",
      "Epoch: 0142 train_loss= 2.47181 train/struct_loss= 7.65906 train/feat_loss= 1.17500\n",
      "Epoch: 0143 train_loss= 2.47186 train/struct_loss= 7.65929 train/feat_loss= 1.17500\n",
      "Epoch: 0144 train_loss= 2.47152 train/struct_loss= 7.65763 train/feat_loss= 1.17500\n",
      "Epoch: 0145 train_loss= 2.47223 train/struct_loss= 7.66118 train/feat_loss= 1.17500\n",
      "Epoch: 0146 train_loss= 2.47174 train/struct_loss= 7.65869 train/feat_loss= 1.17500\n",
      "Epoch: 0147 train_loss= 2.47124 train/struct_loss= 7.65618 train/feat_loss= 1.17500\n",
      "Epoch: 0148 train_loss= 2.47187 train/struct_loss= 7.65936 train/feat_loss= 1.17500\n",
      "Epoch: 0149 train_loss= 2.47189 train/struct_loss= 7.65945 train/feat_loss= 1.17500\n",
      "Epoch: 0150 train_loss= 2.47145 train/struct_loss= 7.65727 train/feat_loss= 1.17500\n",
      "Epoch: 0150 Auc 0.8138255307603979\n",
      "Epoch: 0151 train_loss= 2.47156 train/struct_loss= 7.65783 train/feat_loss= 1.17500\n",
      "Epoch: 0152 train_loss= 2.47163 train/struct_loss= 7.65815 train/feat_loss= 1.17500\n",
      "Epoch: 0153 train_loss= 2.47224 train/struct_loss= 7.66120 train/feat_loss= 1.17500\n",
      "Epoch: 0154 train_loss= 2.47180 train/struct_loss= 7.65901 train/feat_loss= 1.17500\n",
      "Epoch: 0155 train_loss= 2.47174 train/struct_loss= 7.65871 train/feat_loss= 1.17500\n",
      "Epoch: 0156 train_loss= 2.47164 train/struct_loss= 7.65822 train/feat_loss= 1.17500\n",
      "Epoch: 0157 train_loss= 2.47164 train/struct_loss= 7.65820 train/feat_loss= 1.17500\n",
      "Epoch: 0158 train_loss= 2.47194 train/struct_loss= 7.65972 train/feat_loss= 1.17500\n",
      "Epoch: 0159 train_loss= 2.47189 train/struct_loss= 7.65946 train/feat_loss= 1.17500\n",
      "Epoch: 0160 train_loss= 2.47201 train/struct_loss= 7.66008 train/feat_loss= 1.17500\n",
      "Epoch: 0160 Auc 0.8138042921230689\n",
      "Epoch: 0161 train_loss= 2.47174 train/struct_loss= 7.65870 train/feat_loss= 1.17500\n",
      "Epoch: 0162 train_loss= 2.47184 train/struct_loss= 7.65922 train/feat_loss= 1.17500\n",
      "Epoch: 0163 train_loss= 2.47160 train/struct_loss= 7.65799 train/feat_loss= 1.17500\n",
      "Epoch: 0164 train_loss= 2.47195 train/struct_loss= 7.65978 train/feat_loss= 1.17500\n",
      "Epoch: 0165 train_loss= 2.47219 train/struct_loss= 7.66093 train/feat_loss= 1.17500\n",
      "Epoch: 0166 train_loss= 2.47180 train/struct_loss= 7.65902 train/feat_loss= 1.17500\n",
      "Epoch: 0167 train_loss= 2.47140 train/struct_loss= 7.65698 train/feat_loss= 1.17500\n",
      "Epoch: 0168 train_loss= 2.47160 train/struct_loss= 7.65800 train/feat_loss= 1.17500\n",
      "Epoch: 0169 train_loss= 2.47147 train/struct_loss= 7.65733 train/feat_loss= 1.17500\n",
      "Epoch: 0170 train_loss= 2.47167 train/struct_loss= 7.65835 train/feat_loss= 1.17500\n",
      "Epoch: 0170 Auc 0.8138419735763948\n",
      "Epoch: 0171 train_loss= 2.47134 train/struct_loss= 7.65669 train/feat_loss= 1.17500\n",
      "Epoch: 0172 train_loss= 2.47159 train/struct_loss= 7.65795 train/feat_loss= 1.17500\n",
      "Epoch: 0173 train_loss= 2.47161 train/struct_loss= 7.65807 train/feat_loss= 1.17500\n",
      "Epoch: 0174 train_loss= 2.47176 train/struct_loss= 7.65880 train/feat_loss= 1.17500\n",
      "Epoch: 0175 train_loss= 2.47170 train/struct_loss= 7.65853 train/feat_loss= 1.17500\n",
      "Epoch: 0176 train_loss= 2.47137 train/struct_loss= 7.65688 train/feat_loss= 1.17500\n",
      "Epoch: 0177 train_loss= 2.47157 train/struct_loss= 7.65786 train/feat_loss= 1.17500\n",
      "Epoch: 0178 train_loss= 2.47194 train/struct_loss= 7.65968 train/feat_loss= 1.17500\n",
      "Epoch: 0179 train_loss= 2.47204 train/struct_loss= 7.66022 train/feat_loss= 1.17500\n",
      "Epoch: 0180 train_loss= 2.47200 train/struct_loss= 7.65999 train/feat_loss= 1.17500\n",
      "Epoch: 0180 Auc 0.8138495098670598\n",
      "Epoch: 0181 train_loss= 2.47197 train/struct_loss= 7.65988 train/feat_loss= 1.17500\n",
      "Epoch: 0182 train_loss= 2.47172 train/struct_loss= 7.65859 train/feat_loss= 1.17500\n",
      "Epoch: 0183 train_loss= 2.47200 train/struct_loss= 7.65999 train/feat_loss= 1.17500\n",
      "Epoch: 0184 train_loss= 2.47140 train/struct_loss= 7.65702 train/feat_loss= 1.17500\n",
      "Epoch: 0185 train_loss= 2.47176 train/struct_loss= 7.65879 train/feat_loss= 1.17500\n",
      "Epoch: 0186 train_loss= 2.47217 train/struct_loss= 7.66084 train/feat_loss= 1.17500\n",
      "Epoch: 0187 train_loss= 2.47208 train/struct_loss= 7.66040 train/feat_loss= 1.17500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0188 train_loss= 2.47217 train/struct_loss= 7.66084 train/feat_loss= 1.17500\n",
      "Epoch: 0189 train_loss= 2.47180 train/struct_loss= 7.65902 train/feat_loss= 1.17500\n",
      "Epoch: 0190 train_loss= 2.47183 train/struct_loss= 7.65915 train/feat_loss= 1.17500\n",
      "Epoch: 0190 Auc 0.8138597866270577\n",
      "Epoch: 0191 train_loss= 2.47165 train/struct_loss= 7.65827 train/feat_loss= 1.17500\n",
      "Epoch: 0192 train_loss= 2.47161 train/struct_loss= 7.65805 train/feat_loss= 1.17500\n",
      "Epoch: 0193 train_loss= 2.47175 train/struct_loss= 7.65877 train/feat_loss= 1.17500\n",
      "Epoch: 0194 train_loss= 2.47164 train/struct_loss= 7.65820 train/feat_loss= 1.17500\n",
      "Epoch: 0195 train_loss= 2.47195 train/struct_loss= 7.65976 train/feat_loss= 1.17500\n",
      "Epoch: 0196 train_loss= 2.47151 train/struct_loss= 7.65757 train/feat_loss= 1.17500\n",
      "Epoch: 0197 train_loss= 2.47179 train/struct_loss= 7.65893 train/feat_loss= 1.17500\n",
      "Epoch: 0198 train_loss= 2.47197 train/struct_loss= 7.65988 train/feat_loss= 1.17500\n",
      "Epoch: 0199 train_loss= 2.47204 train/struct_loss= 7.66020 train/feat_loss= 1.17500\n",
      "Epoch: 0200 train_loss= 2.47166 train/struct_loss= 7.65830 train/feat_loss= 1.17500\n",
      "Epoch: 0200 Auc 0.8138474545150602\n",
      "Epoch: 0201 train_loss= 2.47179 train/struct_loss= 7.65894 train/feat_loss= 1.17500\n",
      "Epoch: 0202 train_loss= 2.47176 train/struct_loss= 7.65881 train/feat_loss= 1.17500\n",
      "Epoch: 0203 train_loss= 2.47172 train/struct_loss= 7.65863 train/feat_loss= 1.17500\n",
      "Epoch: 0204 train_loss= 2.47155 train/struct_loss= 7.65775 train/feat_loss= 1.17500\n",
      "Epoch: 0205 train_loss= 2.47187 train/struct_loss= 7.65937 train/feat_loss= 1.17500\n",
      "Epoch: 0206 train_loss= 2.47173 train/struct_loss= 7.65866 train/feat_loss= 1.17500\n",
      "Epoch: 0207 train_loss= 2.47171 train/struct_loss= 7.65854 train/feat_loss= 1.17500\n",
      "Epoch: 0208 train_loss= 2.47168 train/struct_loss= 7.65841 train/feat_loss= 1.17500\n",
      "Epoch: 0209 train_loss= 2.47183 train/struct_loss= 7.65914 train/feat_loss= 1.17500\n",
      "Epoch: 0210 train_loss= 2.47229 train/struct_loss= 7.66145 train/feat_loss= 1.17500\n",
      "Epoch: 0210 Auc 0.8138481396323934\n",
      "Epoch: 0211 train_loss= 2.47158 train/struct_loss= 7.65792 train/feat_loss= 1.17500\n",
      "Epoch: 0212 train_loss= 2.47162 train/struct_loss= 7.65812 train/feat_loss= 1.17500\n",
      "Epoch: 0213 train_loss= 2.47176 train/struct_loss= 7.65880 train/feat_loss= 1.17500\n",
      "Epoch: 0214 train_loss= 2.47137 train/struct_loss= 7.65686 train/feat_loss= 1.17500\n",
      "Epoch: 0215 train_loss= 2.47168 train/struct_loss= 7.65843 train/feat_loss= 1.17500\n",
      "Epoch: 0216 train_loss= 2.47192 train/struct_loss= 7.65961 train/feat_loss= 1.17500\n",
      "Epoch: 0217 train_loss= 2.47134 train/struct_loss= 7.65673 train/feat_loss= 1.17500\n",
      "Epoch: 0218 train_loss= 2.47161 train/struct_loss= 7.65808 train/feat_loss= 1.17500\n",
      "Epoch: 0219 train_loss= 2.47205 train/struct_loss= 7.66028 train/feat_loss= 1.17500\n",
      "Epoch: 0220 train_loss= 2.47143 train/struct_loss= 7.65715 train/feat_loss= 1.17500\n",
      "Epoch: 0220 Auc 0.8138179944697328\n",
      "Epoch: 0221 train_loss= 2.47172 train/struct_loss= 7.65859 train/feat_loss= 1.17500\n",
      "Epoch: 0222 train_loss= 2.47212 train/struct_loss= 7.66058 train/feat_loss= 1.17500\n",
      "Epoch: 0223 train_loss= 2.47192 train/struct_loss= 7.65959 train/feat_loss= 1.17500\n",
      "Epoch: 0224 train_loss= 2.47170 train/struct_loss= 7.65852 train/feat_loss= 1.17500\n",
      "Epoch: 0225 train_loss= 2.47157 train/struct_loss= 7.65785 train/feat_loss= 1.17500\n",
      "Epoch: 0226 train_loss= 2.47136 train/struct_loss= 7.65681 train/feat_loss= 1.17500\n",
      "Epoch: 0227 train_loss= 2.47196 train/struct_loss= 7.65979 train/feat_loss= 1.17500\n",
      "Epoch: 0228 train_loss= 2.47166 train/struct_loss= 7.65829 train/feat_loss= 1.17500\n",
      "Epoch: 0229 train_loss= 2.47179 train/struct_loss= 7.65895 train/feat_loss= 1.17500\n",
      "Epoch: 0230 train_loss= 2.47156 train/struct_loss= 7.65781 train/feat_loss= 1.17500\n",
      "Epoch: 0230 Auc 0.8138316968163968\n",
      "Epoch: 0231 train_loss= 2.47176 train/struct_loss= 7.65880 train/feat_loss= 1.17500\n",
      "Epoch: 0232 train_loss= 2.47185 train/struct_loss= 7.65926 train/feat_loss= 1.17500\n",
      "Epoch: 0233 train_loss= 2.47153 train/struct_loss= 7.65768 train/feat_loss= 1.17500\n",
      "Epoch: 0234 train_loss= 2.47188 train/struct_loss= 7.65942 train/feat_loss= 1.17500\n",
      "Epoch: 0235 train_loss= 2.47174 train/struct_loss= 7.65872 train/feat_loss= 1.17500\n",
      "Epoch: 0236 train_loss= 2.47151 train/struct_loss= 7.65758 train/feat_loss= 1.17500\n",
      "Epoch: 0237 train_loss= 2.47140 train/struct_loss= 7.65699 train/feat_loss= 1.17500\n",
      "Epoch: 0238 train_loss= 2.47148 train/struct_loss= 7.65742 train/feat_loss= 1.17500\n",
      "Epoch: 0239 train_loss= 2.47158 train/struct_loss= 7.65791 train/feat_loss= 1.17500\n",
      "Epoch: 0240 train_loss= 2.47173 train/struct_loss= 7.65863 train/feat_loss= 1.17500\n",
      "Epoch: 0240 Auc 0.8137977835084036\n",
      "Epoch: 0241 train_loss= 2.47196 train/struct_loss= 7.65979 train/feat_loss= 1.17500\n",
      "Epoch: 0242 train_loss= 2.47159 train/struct_loss= 7.65796 train/feat_loss= 1.17500\n",
      "Epoch: 0243 train_loss= 2.47127 train/struct_loss= 7.65635 train/feat_loss= 1.17500\n",
      "Epoch: 0244 train_loss= 2.47139 train/struct_loss= 7.65695 train/feat_loss= 1.17500\n",
      "Epoch: 0245 train_loss= 2.47171 train/struct_loss= 7.65855 train/feat_loss= 1.17500\n",
      "Epoch: 0246 train_loss= 2.47149 train/struct_loss= 7.65745 train/feat_loss= 1.17500\n",
      "Epoch: 0247 train_loss= 2.47207 train/struct_loss= 7.66035 train/feat_loss= 1.17500\n",
      "Epoch: 0248 train_loss= 2.47178 train/struct_loss= 7.65892 train/feat_loss= 1.17500\n",
      "Epoch: 0249 train_loss= 2.47155 train/struct_loss= 7.65777 train/feat_loss= 1.17500\n",
      "Epoch: 0250 train_loss= 2.47182 train/struct_loss= 7.65910 train/feat_loss= 1.17500\n",
      "Epoch: 0250 Auc 0.8138474545150601\n",
      "Epoch: 0251 train_loss= 2.47157 train/struct_loss= 7.65787 train/feat_loss= 1.17500\n",
      "Epoch: 0252 train_loss= 2.47187 train/struct_loss= 7.65938 train/feat_loss= 1.17500\n",
      "Epoch: 0253 train_loss= 2.47228 train/struct_loss= 7.66139 train/feat_loss= 1.17500\n",
      "Epoch: 0254 train_loss= 2.47205 train/struct_loss= 7.66025 train/feat_loss= 1.17500\n",
      "Epoch: 0255 train_loss= 2.47152 train/struct_loss= 7.65760 train/feat_loss= 1.17500\n",
      "Epoch: 0256 train_loss= 2.47156 train/struct_loss= 7.65781 train/feat_loss= 1.17500\n",
      "Epoch: 0257 train_loss= 2.47168 train/struct_loss= 7.65842 train/feat_loss= 1.17500\n",
      "Epoch: 0258 train_loss= 2.47209 train/struct_loss= 7.66044 train/feat_loss= 1.17500\n",
      "Epoch: 0259 train_loss= 2.47199 train/struct_loss= 7.65997 train/feat_loss= 1.17500\n",
      "Epoch: 0260 train_loss= 2.47177 train/struct_loss= 7.65886 train/feat_loss= 1.17500\n",
      "Epoch: 0260 Auc 0.8139330941817096\n",
      "Epoch: 0261 train_loss= 2.47224 train/struct_loss= 7.66119 train/feat_loss= 1.17500\n",
      "Epoch: 0262 train_loss= 2.47239 train/struct_loss= 7.66195 train/feat_loss= 1.17500\n",
      "Epoch: 0263 train_loss= 2.47204 train/struct_loss= 7.66020 train/feat_loss= 1.17500\n",
      "Epoch: 0264 train_loss= 2.47325 train/struct_loss= 7.66626 train/feat_loss= 1.17500\n",
      "Epoch: 0265 train_loss= 2.47290 train/struct_loss= 7.66449 train/feat_loss= 1.17500\n",
      "Epoch: 0266 train_loss= 2.47266 train/struct_loss= 7.66330 train/feat_loss= 1.17500\n",
      "Epoch: 0267 train_loss= 2.47302 train/struct_loss= 7.66508 train/feat_loss= 1.17500\n",
      "Epoch: 0268 train_loss= 2.47149 train/struct_loss= 7.65747 train/feat_loss= 1.17500\n",
      "Epoch: 0269 train_loss= 2.47268 train/struct_loss= 7.66339 train/feat_loss= 1.17500\n",
      "Epoch: 0270 train_loss= 2.47166 train/struct_loss= 7.65830 train/feat_loss= 1.17500\n",
      "Epoch: 0270 Auc 0.8139303537123769\n",
      "Epoch: 0271 train_loss= 2.47272 train/struct_loss= 7.66362 train/feat_loss= 1.17500\n",
      "Epoch: 0272 train_loss= 2.47212 train/struct_loss= 7.66060 train/feat_loss= 1.17500\n",
      "Epoch: 0273 train_loss= 2.47198 train/struct_loss= 7.65991 train/feat_loss= 1.17500\n",
      "Epoch: 0274 train_loss= 2.47192 train/struct_loss= 7.65960 train/feat_loss= 1.17500\n",
      "Epoch: 0275 train_loss= 2.47192 train/struct_loss= 7.65962 train/feat_loss= 1.17500\n",
      "Epoch: 0276 train_loss= 2.47173 train/struct_loss= 7.65866 train/feat_loss= 1.17500\n",
      "Epoch: 0277 train_loss= 2.47164 train/struct_loss= 7.65820 train/feat_loss= 1.17500\n",
      "Epoch: 0278 train_loss= 2.47187 train/struct_loss= 7.65938 train/feat_loss= 1.17500\n",
      "Epoch: 0279 train_loss= 2.47196 train/struct_loss= 7.65982 train/feat_loss= 1.17500\n",
      "Epoch: 0280 train_loss= 2.47195 train/struct_loss= 7.65973 train/feat_loss= 1.17500\n",
      "Epoch: 0280 Auc 0.8138940424937176\n",
      "Epoch: 0281 train_loss= 2.47214 train/struct_loss= 7.66069 train/feat_loss= 1.17500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0282 train_loss= 2.47213 train/struct_loss= 7.66068 train/feat_loss= 1.17500\n",
      "Epoch: 0283 train_loss= 2.47224 train/struct_loss= 7.66119 train/feat_loss= 1.17500\n",
      "Epoch: 0284 train_loss= 2.47227 train/struct_loss= 7.66137 train/feat_loss= 1.17500\n",
      "Epoch: 0285 train_loss= 2.47183 train/struct_loss= 7.65918 train/feat_loss= 1.17500\n",
      "Epoch: 0286 train_loss= 2.47224 train/struct_loss= 7.66123 train/feat_loss= 1.17500\n",
      "Epoch: 0287 train_loss= 2.47175 train/struct_loss= 7.65877 train/feat_loss= 1.17500\n",
      "Epoch: 0288 train_loss= 2.47148 train/struct_loss= 7.65742 train/feat_loss= 1.17500\n",
      "Epoch: 0289 train_loss= 2.47158 train/struct_loss= 7.65793 train/feat_loss= 1.17500\n",
      "Epoch: 0290 train_loss= 2.47206 train/struct_loss= 7.66031 train/feat_loss= 1.17500\n",
      "Epoch: 0290 Auc 0.8138412884590616\n",
      "Epoch: 0291 train_loss= 2.47155 train/struct_loss= 7.65777 train/feat_loss= 1.17500\n",
      "Epoch: 0292 train_loss= 2.47176 train/struct_loss= 7.65882 train/feat_loss= 1.17500\n",
      "Epoch: 0293 train_loss= 2.47141 train/struct_loss= 7.65706 train/feat_loss= 1.17500\n",
      "Epoch: 0294 train_loss= 2.47180 train/struct_loss= 7.65900 train/feat_loss= 1.17500\n",
      "Epoch: 0295 train_loss= 2.47249 train/struct_loss= 7.66248 train/feat_loss= 1.17500\n",
      "Epoch: 0296 train_loss= 2.47191 train/struct_loss= 7.65958 train/feat_loss= 1.17500\n",
      "Epoch: 0297 train_loss= 2.47155 train/struct_loss= 7.65778 train/feat_loss= 1.17500\n",
      "Epoch: 0298 train_loss= 2.47157 train/struct_loss= 7.65788 train/feat_loss= 1.17500\n",
      "Epoch: 0299 train_loss= 2.47163 train/struct_loss= 7.65814 train/feat_loss= 1.17500\n",
      "Epoch: 0300 train_loss= 2.47178 train/struct_loss= 7.65891 train/feat_loss= 1.17500\n",
      "Epoch: 0300 Auc 0.8138563610403917\n",
      "Epoch: 0301 train_loss= 2.47174 train/struct_loss= 7.65870 train/feat_loss= 1.17500\n",
      "Epoch: 0302 train_loss= 2.47210 train/struct_loss= 7.66052 train/feat_loss= 1.17500\n",
      "Epoch: 0303 train_loss= 2.47120 train/struct_loss= 7.65602 train/feat_loss= 1.17500\n",
      "Epoch: 0304 train_loss= 2.47175 train/struct_loss= 7.65874 train/feat_loss= 1.17500\n",
      "Epoch: 0305 train_loss= 2.47188 train/struct_loss= 7.65941 train/feat_loss= 1.17500\n",
      "Epoch: 0306 train_loss= 2.47183 train/struct_loss= 7.65915 train/feat_loss= 1.17500\n",
      "Epoch: 0307 train_loss= 2.47141 train/struct_loss= 7.65708 train/feat_loss= 1.17500\n",
      "Epoch: 0308 train_loss= 2.47204 train/struct_loss= 7.66020 train/feat_loss= 1.17500\n",
      "Epoch: 0309 train_loss= 2.47184 train/struct_loss= 7.65923 train/feat_loss= 1.17500\n",
      "Epoch: 0310 train_loss= 2.47162 train/struct_loss= 7.65810 train/feat_loss= 1.17500\n",
      "Epoch: 0310 Auc 0.8138344372857296\n",
      "Epoch: 0311 train_loss= 2.47190 train/struct_loss= 7.65950 train/feat_loss= 1.17500\n",
      "Epoch: 0312 train_loss= 2.47118 train/struct_loss= 7.65593 train/feat_loss= 1.17500\n",
      "Epoch: 0313 train_loss= 2.47100 train/struct_loss= 7.65502 train/feat_loss= 1.17500\n",
      "Epoch: 0314 train_loss= 2.47200 train/struct_loss= 7.65999 train/feat_loss= 1.17500\n",
      "Epoch: 0315 train_loss= 2.47167 train/struct_loss= 7.65834 train/feat_loss= 1.17500\n",
      "Epoch: 0316 train_loss= 2.47141 train/struct_loss= 7.65707 train/feat_loss= 1.17500\n",
      "Epoch: 0317 train_loss= 2.47162 train/struct_loss= 7.65811 train/feat_loss= 1.17500\n",
      "Epoch: 0318 train_loss= 2.47156 train/struct_loss= 7.65781 train/feat_loss= 1.17500\n",
      "Epoch: 0319 train_loss= 2.47209 train/struct_loss= 7.66046 train/feat_loss= 1.17500\n",
      "Epoch: 0320 train_loss= 2.47150 train/struct_loss= 7.65749 train/feat_loss= 1.17500\n",
      "Epoch: 0320 Auc 0.8138474545150602\n",
      "Epoch: 0321 train_loss= 2.47183 train/struct_loss= 7.65914 train/feat_loss= 1.17500\n",
      "Epoch: 0322 train_loss= 2.47180 train/struct_loss= 7.65901 train/feat_loss= 1.17500\n",
      "Epoch: 0323 train_loss= 2.47195 train/struct_loss= 7.65976 train/feat_loss= 1.17500\n",
      "Epoch: 0324 train_loss= 2.47198 train/struct_loss= 7.65993 train/feat_loss= 1.17500\n",
      "Epoch: 0325 train_loss= 2.47134 train/struct_loss= 7.65670 train/feat_loss= 1.17500\n",
      "Epoch: 0326 train_loss= 2.47172 train/struct_loss= 7.65859 train/feat_loss= 1.17500\n",
      "Epoch: 0327 train_loss= 2.47157 train/struct_loss= 7.65784 train/feat_loss= 1.17500\n",
      "Epoch: 0328 train_loss= 2.47148 train/struct_loss= 7.65743 train/feat_loss= 1.17500\n",
      "Epoch: 0329 train_loss= 2.47183 train/struct_loss= 7.65914 train/feat_loss= 1.17500\n",
      "Epoch: 0330 train_loss= 2.47158 train/struct_loss= 7.65792 train/feat_loss= 1.17500\n",
      "Epoch: 0330 Auc 0.8138495098670598\n",
      "Epoch: 0331 train_loss= 2.47177 train/struct_loss= 7.65885 train/feat_loss= 1.17500\n",
      "Epoch: 0332 train_loss= 2.47162 train/struct_loss= 7.65811 train/feat_loss= 1.17500\n",
      "Epoch: 0333 train_loss= 2.47160 train/struct_loss= 7.65801 train/feat_loss= 1.17500\n",
      "Epoch: 0334 train_loss= 2.47227 train/struct_loss= 7.66134 train/feat_loss= 1.17500\n",
      "Epoch: 0335 train_loss= 2.47208 train/struct_loss= 7.66043 train/feat_loss= 1.17500\n",
      "Epoch: 0336 train_loss= 2.47171 train/struct_loss= 7.65856 train/feat_loss= 1.17500\n",
      "Epoch: 0337 train_loss= 2.47192 train/struct_loss= 7.65962 train/feat_loss= 1.17500\n",
      "Epoch: 0338 train_loss= 2.47131 train/struct_loss= 7.65657 train/feat_loss= 1.17500\n",
      "Epoch: 0339 train_loss= 2.47190 train/struct_loss= 7.65953 train/feat_loss= 1.17500\n",
      "Epoch: 0340 train_loss= 2.47134 train/struct_loss= 7.65672 train/feat_loss= 1.17500\n",
      "Epoch: 0340 Auc 0.813808402827068\n",
      "Epoch: 0341 train_loss= 2.47209 train/struct_loss= 7.66046 train/feat_loss= 1.17500\n",
      "Epoch: 0342 train_loss= 2.47157 train/struct_loss= 7.65785 train/feat_loss= 1.17500\n",
      "Epoch: 0343 train_loss= 2.47131 train/struct_loss= 7.65657 train/feat_loss= 1.17500\n",
      "Epoch: 0344 train_loss= 2.47212 train/struct_loss= 7.66060 train/feat_loss= 1.17500\n",
      "Epoch: 0345 train_loss= 2.47183 train/struct_loss= 7.65916 train/feat_loss= 1.17500\n",
      "Epoch: 0346 train_loss= 2.47212 train/struct_loss= 7.66059 train/feat_loss= 1.17500\n",
      "Epoch: 0347 train_loss= 2.47162 train/struct_loss= 7.65810 train/feat_loss= 1.17500\n",
      "Epoch: 0348 train_loss= 2.47173 train/struct_loss= 7.65865 train/feat_loss= 1.17500\n",
      "Epoch: 0349 train_loss= 2.47149 train/struct_loss= 7.65747 train/feat_loss= 1.17500\n",
      "Epoch: 0350 train_loss= 2.47226 train/struct_loss= 7.66131 train/feat_loss= 1.17500\n",
      "Epoch: 0350 Auc 0.8138597866270576\n",
      "Epoch: 0351 train_loss= 2.47139 train/struct_loss= 7.65696 train/feat_loss= 1.17500\n",
      "Epoch: 0352 train_loss= 2.47164 train/struct_loss= 7.65819 train/feat_loss= 1.17500\n",
      "Epoch: 0353 train_loss= 2.47193 train/struct_loss= 7.65966 train/feat_loss= 1.17500\n",
      "Epoch: 0354 train_loss= 2.47194 train/struct_loss= 7.65969 train/feat_loss= 1.17500\n",
      "Epoch: 0355 train_loss= 2.47169 train/struct_loss= 7.65848 train/feat_loss= 1.17500\n",
      "Epoch: 0356 train_loss= 2.47147 train/struct_loss= 7.65737 train/feat_loss= 1.17500\n",
      "Epoch: 0357 train_loss= 2.47184 train/struct_loss= 7.65922 train/feat_loss= 1.17500\n",
      "Epoch: 0358 train_loss= 2.47140 train/struct_loss= 7.65701 train/feat_loss= 1.17500\n",
      "Epoch: 0359 train_loss= 2.47142 train/struct_loss= 7.65712 train/feat_loss= 1.17500\n",
      "Epoch: 0360 train_loss= 2.47130 train/struct_loss= 7.65648 train/feat_loss= 1.17500\n",
      "Epoch: 0360 Auc 0.813852935453726\n",
      "Epoch: 0361 train_loss= 2.47154 train/struct_loss= 7.65769 train/feat_loss= 1.17500\n",
      "Epoch: 0362 train_loss= 2.47171 train/struct_loss= 7.65856 train/feat_loss= 1.17500\n",
      "Epoch: 0363 train_loss= 2.47196 train/struct_loss= 7.65982 train/feat_loss= 1.17500\n",
      "Epoch: 0364 train_loss= 2.47165 train/struct_loss= 7.65825 train/feat_loss= 1.17500\n",
      "Epoch: 0365 train_loss= 2.47157 train/struct_loss= 7.65785 train/feat_loss= 1.17500\n",
      "Epoch: 0366 train_loss= 2.47224 train/struct_loss= 7.66121 train/feat_loss= 1.17500\n",
      "Epoch: 0367 train_loss= 2.47142 train/struct_loss= 7.65713 train/feat_loss= 1.17500\n",
      "Epoch: 0368 train_loss= 2.47158 train/struct_loss= 7.65792 train/feat_loss= 1.17500\n",
      "Epoch: 0369 train_loss= 2.47156 train/struct_loss= 7.65783 train/feat_loss= 1.17500\n",
      "Epoch: 0370 train_loss= 2.47210 train/struct_loss= 7.66052 train/feat_loss= 1.17500\n",
      "Epoch: 0370 Auc 0.8138597866270578\n",
      "Epoch: 0371 train_loss= 2.47173 train/struct_loss= 7.65866 train/feat_loss= 1.17500\n",
      "Epoch: 0372 train_loss= 2.47186 train/struct_loss= 7.65930 train/feat_loss= 1.17500\n",
      "Epoch: 0373 train_loss= 2.47159 train/struct_loss= 7.65796 train/feat_loss= 1.17500\n",
      "Epoch: 0374 train_loss= 2.47191 train/struct_loss= 7.65956 train/feat_loss= 1.17500\n",
      "Epoch: 0375 train_loss= 2.47123 train/struct_loss= 7.65615 train/feat_loss= 1.17500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0376 train_loss= 2.47150 train/struct_loss= 7.65752 train/feat_loss= 1.17500\n",
      "Epoch: 0377 train_loss= 2.47175 train/struct_loss= 7.65876 train/feat_loss= 1.17500\n",
      "Epoch: 0378 train_loss= 2.47133 train/struct_loss= 7.65664 train/feat_loss= 1.17500\n",
      "Epoch: 0379 train_loss= 2.47205 train/struct_loss= 7.66025 train/feat_loss= 1.17500\n",
      "Epoch: 0380 train_loss= 2.47189 train/struct_loss= 7.65947 train/feat_loss= 1.17500\n",
      "Epoch: 0380 Auc 0.8138522503363925\n",
      "Epoch: 0381 train_loss= 2.47227 train/struct_loss= 7.66134 train/feat_loss= 1.17500\n",
      "Epoch: 0382 train_loss= 2.47151 train/struct_loss= 7.65757 train/feat_loss= 1.17500\n",
      "Epoch: 0383 train_loss= 2.47147 train/struct_loss= 7.65736 train/feat_loss= 1.17500\n",
      "Epoch: 0384 train_loss= 2.47191 train/struct_loss= 7.65957 train/feat_loss= 1.17500\n",
      "Epoch: 0385 train_loss= 2.47161 train/struct_loss= 7.65805 train/feat_loss= 1.17500\n",
      "Epoch: 0386 train_loss= 2.47198 train/struct_loss= 7.65989 train/feat_loss= 1.17500\n",
      "Epoch: 0387 train_loss= 2.47192 train/struct_loss= 7.65959 train/feat_loss= 1.17500\n",
      "Epoch: 0388 train_loss= 2.47154 train/struct_loss= 7.65772 train/feat_loss= 1.17500\n",
      "Epoch: 0389 train_loss= 2.47149 train/struct_loss= 7.65745 train/feat_loss= 1.17500\n",
      "Epoch: 0390 train_loss= 2.47149 train/struct_loss= 7.65746 train/feat_loss= 1.17500\n",
      "Epoch: 0390 Auc 0.8138447140457274\n",
      "Epoch: 0391 train_loss= 2.47180 train/struct_loss= 7.65901 train/feat_loss= 1.17500\n",
      "Epoch: 0392 train_loss= 2.47159 train/struct_loss= 7.65794 train/feat_loss= 1.17500\n",
      "Epoch: 0393 train_loss= 2.47185 train/struct_loss= 7.65924 train/feat_loss= 1.17500\n",
      "Epoch: 0394 train_loss= 2.47169 train/struct_loss= 7.65847 train/feat_loss= 1.17500\n",
      "Epoch: 0395 train_loss= 2.47225 train/struct_loss= 7.66126 train/feat_loss= 1.17500\n",
      "Epoch: 0396 train_loss= 2.47183 train/struct_loss= 7.65916 train/feat_loss= 1.17500\n",
      "Epoch: 0397 train_loss= 2.47137 train/struct_loss= 7.65686 train/feat_loss= 1.17500\n",
      "Epoch: 0398 train_loss= 2.47155 train/struct_loss= 7.65776 train/feat_loss= 1.17500\n",
      "Epoch: 0399 train_loss= 2.47163 train/struct_loss= 7.65816 train/feat_loss= 1.17500\n",
      "Epoch: 0400 train_loss= 2.47169 train/struct_loss= 7.65844 train/feat_loss= 1.17500\n",
      "Epoch: 0400 Auc 0.8138227902910653\n",
      "Epoch: 0401 train_loss= 2.47211 train/struct_loss= 7.66054 train/feat_loss= 1.17500\n",
      "Epoch: 0402 train_loss= 2.47172 train/struct_loss= 7.65860 train/feat_loss= 1.17500\n",
      "Epoch: 0403 train_loss= 2.47178 train/struct_loss= 7.65889 train/feat_loss= 1.17500\n",
      "Epoch: 0404 train_loss= 2.47182 train/struct_loss= 7.65913 train/feat_loss= 1.17500\n",
      "Epoch: 0405 train_loss= 2.47160 train/struct_loss= 7.65801 train/feat_loss= 1.17500\n",
      "Epoch: 0406 train_loss= 2.47199 train/struct_loss= 7.65994 train/feat_loss= 1.17500\n",
      "Epoch: 0407 train_loss= 2.47189 train/struct_loss= 7.65944 train/feat_loss= 1.17500\n",
      "Epoch: 0408 train_loss= 2.47188 train/struct_loss= 7.65939 train/feat_loss= 1.17500\n",
      "Epoch: 0409 train_loss= 2.47206 train/struct_loss= 7.66032 train/feat_loss= 1.17500\n",
      "Epoch: 0410 train_loss= 2.47141 train/struct_loss= 7.65704 train/feat_loss= 1.17500\n",
      "Epoch: 0410 Auc 0.8138227902910653\n",
      "Epoch: 0411 train_loss= 2.47168 train/struct_loss= 7.65841 train/feat_loss= 1.17500\n",
      "Epoch: 0412 train_loss= 2.47200 train/struct_loss= 7.66003 train/feat_loss= 1.17500\n",
      "Epoch: 0413 train_loss= 2.47193 train/struct_loss= 7.65968 train/feat_loss= 1.17500\n",
      "Epoch: 0414 train_loss= 2.47149 train/struct_loss= 7.65746 train/feat_loss= 1.17500\n",
      "Epoch: 0415 train_loss= 2.47222 train/struct_loss= 7.66112 train/feat_loss= 1.17500\n",
      "Epoch: 0416 train_loss= 2.47181 train/struct_loss= 7.65907 train/feat_loss= 1.17500\n",
      "Epoch: 0417 train_loss= 2.47169 train/struct_loss= 7.65845 train/feat_loss= 1.17500\n",
      "Epoch: 0418 train_loss= 2.47149 train/struct_loss= 7.65747 train/feat_loss= 1.17500\n",
      "Epoch: 0419 train_loss= 2.47181 train/struct_loss= 7.65907 train/feat_loss= 1.17500\n",
      "Epoch: 0420 train_loss= 2.47168 train/struct_loss= 7.65843 train/feat_loss= 1.17500\n",
      "Epoch: 0420 Auc 0.8138015516537362\n",
      "Epoch: 0421 train_loss= 2.47209 train/struct_loss= 7.66046 train/feat_loss= 1.17500\n",
      "Epoch: 0422 train_loss= 2.47148 train/struct_loss= 7.65743 train/feat_loss= 1.17500\n",
      "Epoch: 0423 train_loss= 2.47202 train/struct_loss= 7.66008 train/feat_loss= 1.17500\n",
      "Epoch: 0424 train_loss= 2.47170 train/struct_loss= 7.65850 train/feat_loss= 1.17500\n",
      "Epoch: 0425 train_loss= 2.47201 train/struct_loss= 7.66005 train/feat_loss= 1.17500\n",
      "Epoch: 0426 train_loss= 2.47156 train/struct_loss= 7.65779 train/feat_loss= 1.17500\n",
      "Epoch: 0427 train_loss= 2.47166 train/struct_loss= 7.65832 train/feat_loss= 1.17500\n",
      "Epoch: 0428 train_loss= 2.47188 train/struct_loss= 7.65943 train/feat_loss= 1.17500\n",
      "Epoch: 0429 train_loss= 2.47143 train/struct_loss= 7.65714 train/feat_loss= 1.17500\n",
      "Epoch: 0430 train_loss= 2.47148 train/struct_loss= 7.65742 train/feat_loss= 1.17500\n",
      "Epoch: 0430 Auc 0.8138104581790677\n",
      "Epoch: 0431 train_loss= 2.47187 train/struct_loss= 7.65937 train/feat_loss= 1.17500\n",
      "Epoch: 0432 train_loss= 2.47129 train/struct_loss= 7.65648 train/feat_loss= 1.17500\n",
      "Epoch: 0433 train_loss= 2.47170 train/struct_loss= 7.65852 train/feat_loss= 1.17500\n",
      "Epoch: 0434 train_loss= 2.47187 train/struct_loss= 7.65934 train/feat_loss= 1.17500\n",
      "Epoch: 0435 train_loss= 2.47186 train/struct_loss= 7.65932 train/feat_loss= 1.17500\n",
      "Epoch: 0436 train_loss= 2.47137 train/struct_loss= 7.65688 train/feat_loss= 1.17500\n",
      "Epoch: 0437 train_loss= 2.47219 train/struct_loss= 7.66096 train/feat_loss= 1.17500\n",
      "Epoch: 0438 train_loss= 2.47152 train/struct_loss= 7.65763 train/feat_loss= 1.17500\n",
      "Epoch: 0439 train_loss= 2.47182 train/struct_loss= 7.65913 train/feat_loss= 1.17500\n",
      "Epoch: 0440 train_loss= 2.47205 train/struct_loss= 7.66028 train/feat_loss= 1.17500\n",
      "Epoch: 0440 Auc 0.8138176519110664\n",
      "Epoch: 0441 train_loss= 2.47197 train/struct_loss= 7.65984 train/feat_loss= 1.17500\n",
      "Epoch: 0442 train_loss= 2.47211 train/struct_loss= 7.66054 train/feat_loss= 1.17500\n",
      "Epoch: 0443 train_loss= 2.47147 train/struct_loss= 7.65737 train/feat_loss= 1.17500\n",
      "Epoch: 0444 train_loss= 2.47173 train/struct_loss= 7.65864 train/feat_loss= 1.17500\n",
      "Epoch: 0445 train_loss= 2.47157 train/struct_loss= 7.65783 train/feat_loss= 1.17500\n",
      "Epoch: 0446 train_loss= 2.47160 train/struct_loss= 7.65800 train/feat_loss= 1.17500\n",
      "Epoch: 0447 train_loss= 2.47126 train/struct_loss= 7.65631 train/feat_loss= 1.17500\n",
      "Epoch: 0448 train_loss= 2.47210 train/struct_loss= 7.66052 train/feat_loss= 1.17500\n",
      "Epoch: 0449 train_loss= 2.47173 train/struct_loss= 7.65864 train/feat_loss= 1.17500\n",
      "Epoch: 0450 train_loss= 2.47167 train/struct_loss= 7.65837 train/feat_loss= 1.17500\n",
      "Epoch: 0450 Auc 0.8138597866270578\n",
      "Epoch: 0451 train_loss= 2.47195 train/struct_loss= 7.65975 train/feat_loss= 1.17500\n",
      "Epoch: 0452 train_loss= 2.47174 train/struct_loss= 7.65869 train/feat_loss= 1.17500\n",
      "Epoch: 0453 train_loss= 2.47183 train/struct_loss= 7.65916 train/feat_loss= 1.17500\n",
      "Epoch: 0454 train_loss= 2.47179 train/struct_loss= 7.65896 train/feat_loss= 1.17500\n",
      "Epoch: 0455 train_loss= 2.47226 train/struct_loss= 7.66131 train/feat_loss= 1.17500\n",
      "Epoch: 0456 train_loss= 2.47213 train/struct_loss= 7.66067 train/feat_loss= 1.17500\n",
      "Epoch: 0457 train_loss= 2.47128 train/struct_loss= 7.65640 train/feat_loss= 1.17500\n",
      "Epoch: 0458 train_loss= 2.47180 train/struct_loss= 7.65899 train/feat_loss= 1.17500\n",
      "Epoch: 0459 train_loss= 2.47176 train/struct_loss= 7.65882 train/feat_loss= 1.17500\n",
      "Epoch: 0460 train_loss= 2.47162 train/struct_loss= 7.65811 train/feat_loss= 1.17500\n",
      "Epoch: 0460 Auc 0.8138604717443909\n",
      "Epoch: 0461 train_loss= 2.47188 train/struct_loss= 7.65940 train/feat_loss= 1.17500\n",
      "Epoch: 0462 train_loss= 2.47132 train/struct_loss= 7.65663 train/feat_loss= 1.17500\n",
      "Epoch: 0463 train_loss= 2.47163 train/struct_loss= 7.65817 train/feat_loss= 1.17500\n",
      "Epoch: 0464 train_loss= 2.47146 train/struct_loss= 7.65732 train/feat_loss= 1.17500\n",
      "Epoch: 0465 train_loss= 2.47170 train/struct_loss= 7.65853 train/feat_loss= 1.17500\n",
      "Epoch: 0466 train_loss= 2.47168 train/struct_loss= 7.65842 train/feat_loss= 1.17500\n",
      "Epoch: 0467 train_loss= 2.47170 train/struct_loss= 7.65851 train/feat_loss= 1.17500\n",
      "Epoch: 0468 train_loss= 2.47132 train/struct_loss= 7.65663 train/feat_loss= 1.17500\n",
      "Epoch: 0469 train_loss= 2.47173 train/struct_loss= 7.65866 train/feat_loss= 1.17500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0470 train_loss= 2.47165 train/struct_loss= 7.65825 train/feat_loss= 1.17500\n",
      "Epoch: 0470 Auc 0.81378305348574\n",
      "Epoch: 0471 train_loss= 2.47162 train/struct_loss= 7.65810 train/feat_loss= 1.17500\n",
      "Epoch: 0472 train_loss= 2.47139 train/struct_loss= 7.65695 train/feat_loss= 1.17500\n",
      "Epoch: 0473 train_loss= 2.47203 train/struct_loss= 7.66014 train/feat_loss= 1.17500\n",
      "Epoch: 0474 train_loss= 2.47172 train/struct_loss= 7.65860 train/feat_loss= 1.17500\n",
      "Epoch: 0475 train_loss= 2.47186 train/struct_loss= 7.65932 train/feat_loss= 1.17500\n",
      "Epoch: 0476 train_loss= 2.47227 train/struct_loss= 7.66135 train/feat_loss= 1.17500\n",
      "Epoch: 0477 train_loss= 2.47215 train/struct_loss= 7.66078 train/feat_loss= 1.17500\n",
      "Epoch: 0478 train_loss= 2.47213 train/struct_loss= 7.66068 train/feat_loss= 1.17500\n",
      "Epoch: 0479 train_loss= 2.47181 train/struct_loss= 7.65906 train/feat_loss= 1.17500\n",
      "Epoch: 0480 train_loss= 2.47212 train/struct_loss= 7.66062 train/feat_loss= 1.17500\n",
      "Epoch: 0480 Auc 0.8138214200563989\n",
      "Epoch: 0481 train_loss= 2.47155 train/struct_loss= 7.65777 train/feat_loss= 1.17500\n",
      "Epoch: 0482 train_loss= 2.47186 train/struct_loss= 7.65931 train/feat_loss= 1.17500\n",
      "Epoch: 0483 train_loss= 2.47187 train/struct_loss= 7.65938 train/feat_loss= 1.17500\n",
      "Epoch: 0484 train_loss= 2.47169 train/struct_loss= 7.65846 train/feat_loss= 1.17500\n",
      "Epoch: 0485 train_loss= 2.47201 train/struct_loss= 7.66008 train/feat_loss= 1.17500\n",
      "Epoch: 0486 train_loss= 2.47170 train/struct_loss= 7.65852 train/feat_loss= 1.17500\n",
      "Epoch: 0487 train_loss= 2.47155 train/struct_loss= 7.65775 train/feat_loss= 1.17500\n",
      "Epoch: 0488 train_loss= 2.47189 train/struct_loss= 7.65944 train/feat_loss= 1.17500\n",
      "Epoch: 0489 train_loss= 2.47197 train/struct_loss= 7.65984 train/feat_loss= 1.17500\n",
      "Epoch: 0490 train_loss= 2.47203 train/struct_loss= 7.66015 train/feat_loss= 1.17500\n",
      "Epoch: 0490 Auc 0.8138714336217221\n",
      "Epoch: 0491 train_loss= 2.47195 train/struct_loss= 7.65976 train/feat_loss= 1.17500\n",
      "Epoch: 0492 train_loss= 2.47163 train/struct_loss= 7.65818 train/feat_loss= 1.17500\n",
      "Epoch: 0493 train_loss= 2.47181 train/struct_loss= 7.65908 train/feat_loss= 1.17500\n",
      "Epoch: 0494 train_loss= 2.47145 train/struct_loss= 7.65727 train/feat_loss= 1.17500\n",
      "Epoch: 0495 train_loss= 2.47184 train/struct_loss= 7.65923 train/feat_loss= 1.17500\n",
      "Epoch: 0496 train_loss= 2.47181 train/struct_loss= 7.65904 train/feat_loss= 1.17500\n",
      "Epoch: 0497 train_loss= 2.47152 train/struct_loss= 7.65758 train/feat_loss= 1.17500\n",
      "Epoch: 0498 train_loss= 2.47186 train/struct_loss= 7.65931 train/feat_loss= 1.17500\n",
      "Epoch: 0499 train_loss= 2.47165 train/struct_loss= 7.65827 train/feat_loss= 1.17500\n",
      "Epoch: 0499 Auc 0.8138207349390657\n"
     ]
    }
   ],
   "source": [
    "train_dominant(dataset=\"BlogCatalog\", hidden_dim=64, max_epoch=500, lr=5e-2, dropout=0.3, alpha=0.8, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91f3cce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train_loss= 9.02370 train/struct_loss= 41.35220 train/feat_loss= 0.94157\n",
      "Epoch: 0000 Auc 0.5681543931655508 precision-recall auc 0.04757547300117436\n",
      "Epoch: 0001 train_loss= 2.88270 train/struct_loss= 11.61893 train/feat_loss= 0.69864\n",
      "Epoch: 0002 train_loss= 1.49756 train/struct_loss= 4.71589 train/feat_loss= 0.69297\n",
      "Epoch: 0003 train_loss= 1.29685 train/struct_loss= 3.71253 train/feat_loss= 0.69293\n",
      "Epoch: 0004 train_loss= 1.24913 train/struct_loss= 3.47388 train/feat_loss= 0.69295\n",
      "Epoch: 0005 train_loss= 1.24640 train/struct_loss= 3.46040 train/feat_loss= 0.69290\n",
      "Epoch: 0006 train_loss= 1.24642 train/struct_loss= 3.46050 train/feat_loss= 0.69290\n",
      "Epoch: 0007 train_loss= 1.24660 train/struct_loss= 3.46142 train/feat_loss= 0.69290\n",
      "Epoch: 0008 train_loss= 1.24645 train/struct_loss= 3.46066 train/feat_loss= 0.69290\n",
      "Epoch: 0009 train_loss= 1.24750 train/struct_loss= 3.46587 train/feat_loss= 0.69290\n",
      "Epoch: 0010 train_loss= 1.24642 train/struct_loss= 3.46052 train/feat_loss= 0.69290\n",
      "Epoch: 0010 Auc 0.896743215458337 precision-recall auc 0.17822775403315846\n",
      "Epoch: 0011 train_loss= 1.24647 train/struct_loss= 3.46076 train/feat_loss= 0.69290\n",
      "Epoch: 0012 train_loss= 1.24648 train/struct_loss= 3.46077 train/feat_loss= 0.69290\n",
      "Epoch: 0013 train_loss= 1.24648 train/struct_loss= 3.46081 train/feat_loss= 0.69290\n",
      "Epoch: 0014 train_loss= 1.24649 train/struct_loss= 3.46083 train/feat_loss= 0.69290\n",
      "Epoch: 0015 train_loss= 1.24648 train/struct_loss= 3.46082 train/feat_loss= 0.69290\n",
      "Epoch: 0016 train_loss= 1.24650 train/struct_loss= 3.46090 train/feat_loss= 0.69290\n",
      "Epoch: 0017 train_loss= 1.24648 train/struct_loss= 3.46081 train/feat_loss= 0.69290\n",
      "Epoch: 0018 train_loss= 1.24649 train/struct_loss= 3.46082 train/feat_loss= 0.69290\n",
      "Epoch: 0019 train_loss= 1.24649 train/struct_loss= 3.46087 train/feat_loss= 0.69290\n",
      "Epoch: 0020 train_loss= 1.24650 train/struct_loss= 3.46088 train/feat_loss= 0.69290\n",
      "Epoch: 0020 Auc 0.8967430045888366 precision-recall auc 0.17817968827605246\n",
      "Epoch: 0021 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0022 train_loss= 1.24650 train/struct_loss= 3.46088 train/feat_loss= 0.69290\n",
      "Epoch: 0023 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0024 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0025 train_loss= 1.24650 train/struct_loss= 3.46088 train/feat_loss= 0.69290\n",
      "Epoch: 0026 train_loss= 1.24650 train/struct_loss= 3.46087 train/feat_loss= 0.69290\n",
      "Epoch: 0027 train_loss= 1.24649 train/struct_loss= 3.46085 train/feat_loss= 0.69290\n",
      "Epoch: 0028 train_loss= 1.24648 train/struct_loss= 3.46081 train/feat_loss= 0.69290\n",
      "Epoch: 0029 train_loss= 1.24654 train/struct_loss= 3.46108 train/feat_loss= 0.69290\n",
      "Epoch: 0030 train_loss= 1.24648 train/struct_loss= 3.46081 train/feat_loss= 0.69290\n",
      "Epoch: 0030 Auc 0.8967397361115812 precision-recall auc 0.17819279389789786\n",
      "Epoch: 0031 train_loss= 1.24662 train/struct_loss= 3.46148 train/feat_loss= 0.69290\n",
      "Epoch: 0032 train_loss= 1.24649 train/struct_loss= 3.46087 train/feat_loss= 0.69290\n",
      "Epoch: 0033 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0034 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0035 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0036 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0037 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0038 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0039 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0040 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0040 Auc 0.8967425301324609 precision-recall auc 0.17818011924515348\n",
      "Epoch: 0041 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0042 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0043 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0044 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0045 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0046 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0047 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0048 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0049 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0050 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0050 Auc 0.8967425301324609 precision-recall auc 0.17818011924515348\n",
      "Epoch: 0051 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0052 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0053 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0054 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0055 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0056 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0057 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0058 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0059 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0060 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0060 Auc 0.8967425301324609 precision-recall auc 0.17818011924515348\n",
      "Epoch: 0061 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0062 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0063 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0064 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0065 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0066 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0067 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0068 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0069 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0070 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0070 Auc 0.8967425301324609 precision-recall auc 0.17818011924515348\n",
      "Epoch: 0071 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0072 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0073 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0074 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0075 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0076 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0077 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0078 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0079 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0080 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0080 Auc 0.8967425301324609 precision-recall auc 0.17818011924515348\n",
      "Epoch: 0081 train_loss= 1.24650 train/struct_loss= 3.46088 train/feat_loss= 0.69290\n",
      "Epoch: 0082 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0083 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0084 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0085 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0086 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0087 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0088 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0089 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0090 train_loss= 1.24650 train/struct_loss= 3.46088 train/feat_loss= 0.69290\n",
      "Epoch: 0090 Auc 0.8967450605664651 precision-recall auc 0.17818718820523552\n",
      "Epoch: 0091 train_loss= 1.24649 train/struct_loss= 3.46085 train/feat_loss= 0.69290\n",
      "Epoch: 0092 train_loss= 1.24649 train/struct_loss= 3.46082 train/feat_loss= 0.69290\n",
      "Epoch: 0093 train_loss= 1.24648 train/struct_loss= 3.46080 train/feat_loss= 0.69290\n",
      "Epoch: 0094 train_loss= 1.24649 train/struct_loss= 3.46082 train/feat_loss= 0.69290\n",
      "Epoch: 0095 train_loss= 1.24649 train/struct_loss= 3.46083 train/feat_loss= 0.69290\n",
      "Epoch: 0096 train_loss= 1.24648 train/struct_loss= 3.46081 train/feat_loss= 0.69290\n",
      "Epoch: 0097 train_loss= 1.24650 train/struct_loss= 3.46088 train/feat_loss= 0.69290\n",
      "Epoch: 0098 train_loss= 1.24649 train/struct_loss= 3.46086 train/feat_loss= 0.69290\n",
      "Epoch: 0099 train_loss= 1.24650 train/struct_loss= 3.46089 train/feat_loss= 0.69290\n",
      "Epoch: 0099 Auc 0.8967368893733264 precision-recall auc 0.17816073154107862\n"
     ]
    }
   ],
   "source": [
    "_, _ = train_dominant(dataset=\"ACM\", hidden_dim=64, max_epoch=100, lr=5e-3, dropout=0.3, alpha=0.8, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd49974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train_loss= 4.63550 train/struct_loss= 18.08544 train/feat_loss= 1.27301\n",
      "Score:  [11.331537   8.407093   8.669939  ...  3.9367468  4.41934    5.138904 ] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0000 Auc 0.7683332020108105 precision-recall auc 0.3199356391805774\n",
      "Epoch: 0001 train_loss= 3.51235 train/struct_loss= 12.77481 train/feat_loss= 1.19674\n",
      "Epoch: 0002 train_loss= 2.82027 train/struct_loss= 9.37342 train/feat_loss= 1.18198\n",
      "Epoch: 0003 train_loss= 2.42017 train/struct_loss= 7.37486 train/feat_loss= 1.18150\n",
      "Epoch: 0004 train_loss= 2.27135 train/struct_loss= 6.63073 train/feat_loss= 1.18150\n",
      "Epoch: 0005 train_loss= 2.25135 train/struct_loss= 6.53094 train/feat_loss= 1.18146\n",
      "Epoch: 0006 train_loss= 2.21896 train/struct_loss= 6.36893 train/feat_loss= 1.18146\n",
      "Epoch: 0007 train_loss= 2.20871 train/struct_loss= 6.31779 train/feat_loss= 1.18144\n",
      "Epoch: 0008 train_loss= 2.21536 train/struct_loss= 6.35101 train/feat_loss= 1.18144\n",
      "Epoch: 0009 train_loss= 2.21940 train/struct_loss= 6.37127 train/feat_loss= 1.18143\n",
      "Epoch: 0010 train_loss= 2.21479 train/struct_loss= 6.34827 train/feat_loss= 1.18143\n",
      "Score:  [11.691444   8.112262   8.357562  ...  1.073791   1.3300126  1.557188 ] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0010 Auc 0.7924840443134722 precision-recall auc 0.21928145615933786\n",
      "Epoch: 0011 train_loss= 2.21585 train/struct_loss= 6.35354 train/feat_loss= 1.18143\n",
      "Epoch: 0012 train_loss= 2.21832 train/struct_loss= 6.36587 train/feat_loss= 1.18143\n",
      "Epoch: 0013 train_loss= 2.20876 train/struct_loss= 6.31810 train/feat_loss= 1.18143\n",
      "Epoch: 0014 train_loss= 2.20818 train/struct_loss= 6.31518 train/feat_loss= 1.18143\n",
      "Epoch: 0015 train_loss= 2.21053 train/struct_loss= 6.32694 train/feat_loss= 1.18143\n",
      "Epoch: 0016 train_loss= 2.20834 train/struct_loss= 6.31600 train/feat_loss= 1.18143\n",
      "Epoch: 0017 train_loss= 2.21245 train/struct_loss= 6.33652 train/feat_loss= 1.18143\n",
      "Epoch: 0018 train_loss= 2.21089 train/struct_loss= 6.32874 train/feat_loss= 1.18143\n",
      "Epoch: 0019 train_loss= 2.20873 train/struct_loss= 6.31793 train/feat_loss= 1.18143\n",
      "Epoch: 0020 train_loss= 2.21045 train/struct_loss= 6.32653 train/feat_loss= 1.18143\n",
      "Score:  [11.728529   8.30931    8.401905  ...  1.0738276  1.3300067  1.5575988] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0020 Auc 0.7928871519296532 precision-recall auc 0.22237615131071842\n",
      "Epoch: 0021 train_loss= 2.20830 train/struct_loss= 6.31578 train/feat_loss= 1.18143\n",
      "Epoch: 0022 train_loss= 2.20790 train/struct_loss= 6.31380 train/feat_loss= 1.18143\n",
      "Epoch: 0023 train_loss= 2.20942 train/struct_loss= 6.32139 train/feat_loss= 1.18143\n",
      "Epoch: 0024 train_loss= 2.20805 train/struct_loss= 6.31453 train/feat_loss= 1.18143\n",
      "Epoch: 0025 train_loss= 2.20777 train/struct_loss= 6.31315 train/feat_loss= 1.18143\n",
      "Epoch: 0026 train_loss= 2.20783 train/struct_loss= 6.31346 train/feat_loss= 1.18143\n",
      "Epoch: 0027 train_loss= 2.20685 train/struct_loss= 6.30855 train/feat_loss= 1.18143\n",
      "Epoch: 0028 train_loss= 2.20693 train/struct_loss= 6.30895 train/feat_loss= 1.18143\n",
      "Epoch: 0029 train_loss= 2.20706 train/struct_loss= 6.30959 train/feat_loss= 1.18143\n",
      "Epoch: 0030 train_loss= 2.20632 train/struct_loss= 6.30588 train/feat_loss= 1.18143\n",
      "Score:  [12.101147   7.994265   8.366257  ...  1.0738276  1.3300067  1.5575988] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0030 Auc 0.7934379185905416 precision-recall auc 0.2238616968276115\n",
      "Epoch: 0031 train_loss= 2.20629 train/struct_loss= 6.30573 train/feat_loss= 1.18143\n",
      "Epoch: 0032 train_loss= 2.20673 train/struct_loss= 6.30796 train/feat_loss= 1.18143\n",
      "Epoch: 0033 train_loss= 2.20573 train/struct_loss= 6.30293 train/feat_loss= 1.18143\n",
      "Epoch: 0034 train_loss= 2.20593 train/struct_loss= 6.30395 train/feat_loss= 1.18143\n",
      "Epoch: 0035 train_loss= 2.20502 train/struct_loss= 6.29941 train/feat_loss= 1.18143\n",
      "Epoch: 0036 train_loss= 2.20528 train/struct_loss= 6.30069 train/feat_loss= 1.18143\n",
      "Epoch: 0037 train_loss= 2.20544 train/struct_loss= 6.30150 train/feat_loss= 1.18143\n",
      "Epoch: 0038 train_loss= 2.20586 train/struct_loss= 6.30357 train/feat_loss= 1.18143\n",
      "Epoch: 0039 train_loss= 2.20639 train/struct_loss= 6.30626 train/feat_loss= 1.18143\n",
      "Epoch: 0040 train_loss= 2.20508 train/struct_loss= 6.29971 train/feat_loss= 1.18143\n",
      "Score:  [11.56407    8.205689   8.070801  ...  1.0738276  1.3300067  1.5575988] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0040 Auc 0.7940138991758198 precision-recall auc 0.22413051751863866\n",
      "Epoch: 0041 train_loss= 2.20563 train/struct_loss= 6.30247 train/feat_loss= 1.18143\n",
      "Epoch: 0042 train_loss= 2.20398 train/struct_loss= 6.29419 train/feat_loss= 1.18143\n",
      "Epoch: 0043 train_loss= 2.20403 train/struct_loss= 6.29444 train/feat_loss= 1.18143\n",
      "Epoch: 0044 train_loss= 2.20523 train/struct_loss= 6.30045 train/feat_loss= 1.18143\n",
      "Epoch: 0045 train_loss= 2.20482 train/struct_loss= 6.29840 train/feat_loss= 1.18143\n",
      "Epoch: 0046 train_loss= 2.20460 train/struct_loss= 6.29731 train/feat_loss= 1.18143\n",
      "Epoch: 0047 train_loss= 2.20553 train/struct_loss= 6.30192 train/feat_loss= 1.18143\n",
      "Epoch: 0048 train_loss= 2.20507 train/struct_loss= 6.29967 train/feat_loss= 1.18143\n",
      "Epoch: 0049 train_loss= 2.20455 train/struct_loss= 6.29704 train/feat_loss= 1.18143\n",
      "Epoch: 0050 train_loss= 2.20605 train/struct_loss= 6.30454 train/feat_loss= 1.18143\n",
      "Score:  [12.519575   7.9039826  8.183222  ...  1.0738192  1.329448   1.5570635] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0050 Auc 0.7936448303575651 precision-recall auc 0.2239763232329401\n",
      "Epoch: 0051 train_loss= 2.20577 train/struct_loss= 6.30313 train/feat_loss= 1.18143\n",
      "Epoch: 0052 train_loss= 2.20341 train/struct_loss= 6.29134 train/feat_loss= 1.18143\n",
      "Epoch: 0053 train_loss= 2.20445 train/struct_loss= 6.29656 train/feat_loss= 1.18143\n",
      "Epoch: 0054 train_loss= 2.20445 train/struct_loss= 6.29655 train/feat_loss= 1.18143\n",
      "Epoch: 0055 train_loss= 2.20493 train/struct_loss= 6.29894 train/feat_loss= 1.18143\n",
      "Epoch: 0056 train_loss= 2.20439 train/struct_loss= 6.29624 train/feat_loss= 1.18143\n",
      "Epoch: 0057 train_loss= 2.20545 train/struct_loss= 6.30155 train/feat_loss= 1.18143\n",
      "Epoch: 0058 train_loss= 2.20497 train/struct_loss= 6.29917 train/feat_loss= 1.18143\n",
      "Epoch: 0059 train_loss= 2.20670 train/struct_loss= 6.30778 train/feat_loss= 1.18143\n",
      "Epoch: 0060 train_loss= 2.20409 train/struct_loss= 6.29475 train/feat_loss= 1.18143\n",
      "Score:  [11.474302   7.8921723  8.40244   ...  1.0738276  1.3297278  1.5567659] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0060 Auc 0.7939710355043573 precision-recall auc 0.22563170066940635\n",
      "Epoch: 0061 train_loss= 2.20481 train/struct_loss= 6.29833 train/feat_loss= 1.18143\n",
      "Epoch: 0062 train_loss= 2.20348 train/struct_loss= 6.29171 train/feat_loss= 1.18143\n",
      "Epoch: 0063 train_loss= 2.20362 train/struct_loss= 6.29238 train/feat_loss= 1.18143\n",
      "Epoch: 0064 train_loss= 2.20543 train/struct_loss= 6.30143 train/feat_loss= 1.18143\n",
      "Epoch: 0065 train_loss= 2.20429 train/struct_loss= 6.29576 train/feat_loss= 1.18143\n",
      "Epoch: 0066 train_loss= 2.20276 train/struct_loss= 6.28810 train/feat_loss= 1.18143\n",
      "Epoch: 0067 train_loss= 2.20427 train/struct_loss= 6.29563 train/feat_loss= 1.18143\n",
      "Epoch: 0068 train_loss= 2.20489 train/struct_loss= 6.29873 train/feat_loss= 1.18143\n",
      "Epoch: 0069 train_loss= 2.20361 train/struct_loss= 6.29237 train/feat_loss= 1.18143\n",
      "Epoch: 0070 train_loss= 2.20555 train/struct_loss= 6.30205 train/feat_loss= 1.18143\n",
      "Score:  [11.640598   8.112657   8.15417   ...  1.0738276  1.3297406  1.5575988] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0070 Auc 0.7935389318751279 precision-recall auc 0.2233997854795238\n",
      "Epoch: 0071 train_loss= 2.20523 train/struct_loss= 6.30045 train/feat_loss= 1.18143\n",
      "Epoch: 0072 train_loss= 2.20485 train/struct_loss= 6.29854 train/feat_loss= 1.18143\n",
      "Epoch: 0073 train_loss= 2.20419 train/struct_loss= 6.29523 train/feat_loss= 1.18143\n",
      "Epoch: 0074 train_loss= 2.20516 train/struct_loss= 6.30012 train/feat_loss= 1.18143\n",
      "Epoch: 0075 train_loss= 2.20375 train/struct_loss= 6.29304 train/feat_loss= 1.18143\n",
      "Epoch: 0076 train_loss= 2.20357 train/struct_loss= 6.29213 train/feat_loss= 1.18143\n",
      "Epoch: 0077 train_loss= 2.20522 train/struct_loss= 6.30038 train/feat_loss= 1.18143\n",
      "Epoch: 0078 train_loss= 2.20393 train/struct_loss= 6.29395 train/feat_loss= 1.18143\n",
      "Epoch: 0079 train_loss= 2.20420 train/struct_loss= 6.29531 train/feat_loss= 1.18143\n",
      "Epoch: 0080 train_loss= 2.20499 train/struct_loss= 6.29923 train/feat_loss= 1.18143\n",
      "Score:  [11.65122    7.87007    8.371678  ...  1.0739012  1.3295375  1.5565039] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0080 Auc 0.7937819310714341 precision-recall auc 0.22497114918282266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0081 train_loss= 2.20404 train/struct_loss= 6.29452 train/feat_loss= 1.18143\n",
      "Epoch: 0082 train_loss= 2.20465 train/struct_loss= 6.29754 train/feat_loss= 1.18143\n",
      "Epoch: 0083 train_loss= 2.20406 train/struct_loss= 6.29458 train/feat_loss= 1.18143\n",
      "Epoch: 0084 train_loss= 2.20494 train/struct_loss= 6.29898 train/feat_loss= 1.18143\n",
      "Epoch: 0085 train_loss= 2.20598 train/struct_loss= 6.30419 train/feat_loss= 1.18143\n",
      "Epoch: 0086 train_loss= 2.20527 train/struct_loss= 6.30064 train/feat_loss= 1.18143\n",
      "Epoch: 0087 train_loss= 2.20528 train/struct_loss= 6.30070 train/feat_loss= 1.18143\n",
      "Epoch: 0088 train_loss= 2.20572 train/struct_loss= 6.30288 train/feat_loss= 1.18143\n",
      "Epoch: 0089 train_loss= 2.20461 train/struct_loss= 6.29735 train/feat_loss= 1.18143\n",
      "Epoch: 0090 train_loss= 2.20579 train/struct_loss= 6.30322 train/feat_loss= 1.18143\n",
      "Score:  [11.545618   7.989866   8.279737  ...  1.0738684  1.3293853  1.5575988] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0090 Auc 0.7940422648407583 precision-recall auc 0.2251481637010963\n",
      "Epoch: 0091 train_loss= 2.20440 train/struct_loss= 6.29630 train/feat_loss= 1.18143\n",
      "Epoch: 0092 train_loss= 2.20477 train/struct_loss= 6.29815 train/feat_loss= 1.18143\n",
      "Epoch: 0093 train_loss= 2.20440 train/struct_loss= 6.29631 train/feat_loss= 1.18143\n",
      "Epoch: 0094 train_loss= 2.20575 train/struct_loss= 6.30305 train/feat_loss= 1.18143\n",
      "Epoch: 0095 train_loss= 2.20440 train/struct_loss= 6.29630 train/feat_loss= 1.18143\n",
      "Epoch: 0096 train_loss= 2.20764 train/struct_loss= 6.31248 train/feat_loss= 1.18143\n",
      "Epoch: 0097 train_loss= 2.20643 train/struct_loss= 6.30643 train/feat_loss= 1.18143\n",
      "Epoch: 0098 train_loss= 2.20578 train/struct_loss= 6.30319 train/feat_loss= 1.18143\n",
      "Epoch: 0099 train_loss= 2.20489 train/struct_loss= 6.29872 train/feat_loss= 1.18143\n",
      "Score:  [11.485475   7.91902    8.182805  ...  1.0738319  1.3297378  1.5575988] Label:  [0 0 0 ... 0 0 0]\n",
      "Epoch: 0099 Auc 0.7937179507382952 precision-recall auc 0.22427299859143884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0], dtype=uint8),\n",
       " array([11.485475 ,  7.91902  ,  8.182805 , ...,  1.0738319,  1.3297378,\n",
       "         1.5575988], dtype=float32))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dominant(dataset=\"Flickr\", hidden_dim=64, max_epoch=100, lr=5e-3, dropout=0.3, alpha=0.8, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efe79597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.9199862,\n",
       " 5.6792407,\n",
       " 5.6175327,\n",
       " 5.1550713,\n",
       " 4.9538846,\n",
       " 4.836311,\n",
       " 5.227498,\n",
       " 4.6503024,\n",
       " 4.977279,\n",
       " 5.0427194,\n",
       " 4.943839,\n",
       " 5.131177,\n",
       " 4.764241,\n",
       " 4.520692,\n",
       " 4.49689,\n",
       " 4.58091,\n",
       " 4.67495,\n",
       " 4.3855133,\n",
       " 5.146143,\n",
       " 4.2798777,\n",
       " 4.581525,\n",
       " 4.438023,\n",
       " 4.5066433,\n",
       " 4.1719413,\n",
       " 6.31293,\n",
       " 4.636311,\n",
       " 4.546307,\n",
       " 4.8321476,\n",
       " 4.524132,\n",
       " 4.5016265,\n",
       " 4.614414,\n",
       " 4.545185,\n",
       " 5.433449,\n",
       " 4.325013,\n",
       " 4.534647,\n",
       " 5.422137,\n",
       " 4.414521,\n",
       " 3.8784897,\n",
       " 4.209894,\n",
       " 3.5439591,\n",
       " 5.1668644,\n",
       " 4.247705,\n",
       " 5.4212456,\n",
       " 4.8509264,\n",
       " 4.1380873,\n",
       " 3.7704651,\n",
       " 6.3601513,\n",
       " 4.846906,\n",
       " 4.235647,\n",
       " 4.4057446,\n",
       " 4.2171583,\n",
       " 4.5601144,\n",
       " 3.8973687,\n",
       " 3.7481236,\n",
       " 3.6509554,\n",
       " 4.605194,\n",
       " 4.07028,\n",
       " 3.9284918,\n",
       " 3.6943047,\n",
       " 4.2370086,\n",
       " 4.0126605,\n",
       " 4.655206,\n",
       " 4.3142095,\n",
       " 4.010345,\n",
       " 4.137539,\n",
       " 4.562504,\n",
       " 4.123822,\n",
       " 4.444492,\n",
       " 4.242847,\n",
       " 4.3296175,\n",
       " 3.492065,\n",
       " 3.7454042,\n",
       " 4.0084476,\n",
       " 4.375434,\n",
       " 4.383264,\n",
       " 4.6355844,\n",
       " 4.2125173,\n",
       " 3.4031675,\n",
       " 4.1736345,\n",
       " 3.8646266,\n",
       " 3.6871793,\n",
       " 4.3944035,\n",
       " 4.058595,\n",
       " 4.1514316,\n",
       " 5.416149,\n",
       " 4.251447,\n",
       " 3.6457982,\n",
       " 3.4551609,\n",
       " 3.5127518,\n",
       " 3.595503,\n",
       " 4.349623,\n",
       " 5.2860775,\n",
       " 4.423284,\n",
       " 3.804642,\n",
       " 3.9204862,\n",
       " 3.8610644,\n",
       " 4.526997,\n",
       " 3.9722314,\n",
       " 4.192579,\n",
       " 3.5966885,\n",
       " 3.8046165,\n",
       " 3.6520367,\n",
       " 6.940119,\n",
       " 3.9697113,\n",
       " 3.790911,\n",
       " 3.4992661,\n",
       " 3.7235632,\n",
       " 4.369554,\n",
       " 4.3036647,\n",
       " 3.9759812,\n",
       " 3.7534513,\n",
       " 3.9632912,\n",
       " 3.930053,\n",
       " 3.8073921,\n",
       " 4.013916,\n",
       " 3.7024662,\n",
       " 6.8150864,\n",
       " 3.490943,\n",
       " 3.6205878,\n",
       " 3.4259362,\n",
       " 3.696977,\n",
       " 3.5140831,\n",
       " 3.56273,\n",
       " 3.5508406,\n",
       " 4.157555,\n",
       " 3.7549884,\n",
       " 3.5212734,\n",
       " 3.8499806,\n",
       " 3.445582,\n",
       " 3.1131625,\n",
       " 3.7548041,\n",
       " 3.3379827,\n",
       " 3.6706598,\n",
       " 4.666321,\n",
       " 4.4053116,\n",
       " 3.9495873,\n",
       " 4.147309,\n",
       " 3.6710925,\n",
       " 3.7886152,\n",
       " 3.1876853,\n",
       " 3.5701878,\n",
       " 3.2425747,\n",
       " 3.571456,\n",
       " 3.7382169,\n",
       " 4.1860447,\n",
       " 4.402134,\n",
       " 4.0363474,\n",
       " 3.4638212,\n",
       " 4.0111933,\n",
       " 3.7259886,\n",
       " 3.969383,\n",
       " 3.5525355,\n",
       " 3.998461,\n",
       " 3.2857926,\n",
       " 3.8392045,\n",
       " 3.5713758,\n",
       " 3.7425935,\n",
       " 4.4458876,\n",
       " 3.3148153,\n",
       " 3.6462696,\n",
       " 3.7466543,\n",
       " 4.096266,\n",
       " 3.2602522,\n",
       " 3.7713583,\n",
       " 3.57675,\n",
       " 3.6277795,\n",
       " 5.2312136,\n",
       " 3.4878578,\n",
       " 3.5156012,\n",
       " 3.76685,\n",
       " 3.5323849,\n",
       " 4.0548306,\n",
       " 3.6573353,\n",
       " 3.6530252,\n",
       " 6.8189073,\n",
       " 3.526957,\n",
       " 3.5220957,\n",
       " 3.4910324,\n",
       " 3.213153,\n",
       " 3.238786,\n",
       " 3.786282,\n",
       " 3.590888,\n",
       " 3.1417942,\n",
       " 4.2451034,\n",
       " 4.5411205,\n",
       " 3.3181074,\n",
       " 2.9986327,\n",
       " 6.0272837,\n",
       " 3.616576,\n",
       " 3.8211074,\n",
       " 3.8432412,\n",
       " 3.6199875,\n",
       " 3.211741,\n",
       " 3.72865,\n",
       " 6.1184683,\n",
       " 3.975182,\n",
       " 3.9443803,\n",
       " 3.2377017,\n",
       " 4.0336523,\n",
       " 3.6875513,\n",
       " 3.7838845,\n",
       " 3.0715978,\n",
       " 3.4218822,\n",
       " 3.4763665,\n",
       " 6.2086267,\n",
       " 3.176133,\n",
       " 3.356726,\n",
       " 3.607617,\n",
       " 3.9212413,\n",
       " 3.3708696,\n",
       " 3.2539115,\n",
       " 3.2876139,\n",
       " 3.58421,\n",
       " 3.2771497,\n",
       " 3.574804,\n",
       " 3.6602724,\n",
       " 3.3764362,\n",
       " 3.2641635,\n",
       " 3.849845,\n",
       " 3.4414408,\n",
       " 3.0895987,\n",
       " 3.2640965,\n",
       " 3.5050452,\n",
       " 3.2280662,\n",
       " 3.2990468,\n",
       " 4.4870872,\n",
       " 3.620277,\n",
       " 3.2693954,\n",
       " 3.4777567,\n",
       " 3.536477,\n",
       " 3.7595692,\n",
       " 5.7792945,\n",
       " 3.0549738,\n",
       " 4.0046887,\n",
       " 2.9994094,\n",
       " 3.073314,\n",
       " 6.2307754,\n",
       " 3.4813774,\n",
       " 3.7037463,\n",
       " 3.1878076,\n",
       " 3.2170343,\n",
       " 3.147681,\n",
       " 4.182906,\n",
       " 3.3534894,\n",
       " 3.5491424,\n",
       " 3.4657857,\n",
       " 3.579007,\n",
       " 4.904519,\n",
       " 3.8088565,\n",
       " 3.4901419,\n",
       " 2.9501572,\n",
       " 3.408044,\n",
       " 3.6990523,\n",
       " 6.1005554,\n",
       " 3.3186302,\n",
       " 4.719418,\n",
       " 3.0860162,\n",
       " 3.5397122,\n",
       " 3.564855,\n",
       " 3.5700488,\n",
       " 3.5203269,\n",
       " 3.3332224,\n",
       " 3.6310644,\n",
       " 3.5551221,\n",
       " 3.0278666,\n",
       " 3.5040555,\n",
       " 3.302352,\n",
       " 2.802528,\n",
       " 3.164919,\n",
       " 3.097008,\n",
       " 3.9588313,\n",
       " 3.3195112,\n",
       " 4.3856463,\n",
       " 4.139053,\n",
       " 3.036789,\n",
       " 3.8315496,\n",
       " 3.7541811,\n",
       " 3.378766,\n",
       " 3.2022958,\n",
       " 3.6421175,\n",
       " 3.3155549,\n",
       " 3.0679486,\n",
       " 3.2886038,\n",
       " 3.1910543,\n",
       " 4.302994,\n",
       " 3.5164275,\n",
       " 4.5494585,\n",
       " 3.5704489,\n",
       " 2.7791116,\n",
       " 2.87227,\n",
       " 3.4450035,\n",
       " 3.3204253,\n",
       " 5.7073007,\n",
       " 2.617743,\n",
       " 3.511014,\n",
       " 3.6065402,\n",
       " 3.2380147,\n",
       " 3.5069218,\n",
       " 3.484718,\n",
       " 2.8562427,\n",
       " 3.2185283,\n",
       " 3.4170966,\n",
       " 3.3070545,\n",
       " 3.5993526,\n",
       " 3.2482467,\n",
       " 3.2249699,\n",
       " 2.9675612,\n",
       " 3.4023662,\n",
       " 4.1297107,\n",
       " 3.6981978,\n",
       " 3.434464,\n",
       " 3.7077312,\n",
       " 3.2910273,\n",
       " 3.0908668,\n",
       " 3.6455193,\n",
       " 5.0599737,\n",
       " 4.323501,\n",
       " 3.0894995,\n",
       " 3.2687597,\n",
       " 5.4880095,\n",
       " 3.5455732,\n",
       " 3.3619962,\n",
       " 2.9660654,\n",
       " 3.313945,\n",
       " 3.5904741,\n",
       " 3.46441,\n",
       " 3.349177,\n",
       " 3.4268699,\n",
       " 3.0252218,\n",
       " 3.0906491,\n",
       " 3.2188709,\n",
       " 3.526237,\n",
       " 3.0378911,\n",
       " 3.3832896,\n",
       " 3.4092398,\n",
       " 3.4216669,\n",
       " 3.047364,\n",
       " 7.9354124,\n",
       " 3.1616166,\n",
       " 4.5241656,\n",
       " 2.8433352,\n",
       " 3.0749385,\n",
       " 3.258278,\n",
       " 2.9262533,\n",
       " 2.8765426,\n",
       " 3.5247922,\n",
       " 3.278345,\n",
       " 3.5808682,\n",
       " 3.165092,\n",
       " 3.1042857,\n",
       " 3.1440566,\n",
       " 3.0084858,\n",
       " 3.1451342,\n",
       " 3.0667624,\n",
       " 3.7934146,\n",
       " 3.1636307,\n",
       " 3.455945,\n",
       " 3.3067033,\n",
       " 3.0936406,\n",
       " 4.291288,\n",
       " 4.7942796,\n",
       " 3.3043723,\n",
       " 3.9357352,\n",
       " 6.4602327,\n",
       " 4.1427174,\n",
       " 2.9921846,\n",
       " 2.9810088,\n",
       " 2.8178005,\n",
       " 3.9177608,\n",
       " 3.4735947,\n",
       " 3.6478543,\n",
       " 3.016434,\n",
       " 5.4553337,\n",
       " 3.1546938,\n",
       " 3.0642319,\n",
       " 2.9068058,\n",
       " 3.436493,\n",
       " 3.325291,\n",
       " 3.404313,\n",
       " 3.9433813,\n",
       " 2.9748316,\n",
       " 3.8486462,\n",
       " 6.4381127,\n",
       " 6.57864,\n",
       " 3.171956,\n",
       " 3.151255,\n",
       " 3.6737108,\n",
       " 6.1270256,\n",
       " 2.8474572,\n",
       " 3.7233377,\n",
       " 3.2860606,\n",
       " 3.040515,\n",
       " 3.5249553,\n",
       " 3.6149163,\n",
       " 4.0312386,\n",
       " 4.4995637,\n",
       " 2.9509287,\n",
       " 3.039701,\n",
       " 4.1107335,\n",
       " 3.4436417,\n",
       " 3.113834,\n",
       " 3.7609763,\n",
       " 3.140852,\n",
       " 2.9519289,\n",
       " 3.2427175,\n",
       " 3.2861183,\n",
       " 2.9703777,\n",
       " 3.4069352,\n",
       " 2.9017315,\n",
       " 3.2321196,\n",
       " 3.8654714,\n",
       " 3.1394944,\n",
       " 2.77714,\n",
       " 2.924154,\n",
       " 2.9401007,\n",
       " 3.4352305,\n",
       " 3.821824,\n",
       " 3.7483988,\n",
       " 4.2917557,\n",
       " 2.8360305,\n",
       " 3.6773703,\n",
       " 3.267352,\n",
       " 3.6235902,\n",
       " 3.3595695,\n",
       " 3.6881552,\n",
       " 2.9186144,\n",
       " 3.4136238,\n",
       " 2.8400865,\n",
       " 3.2186992,\n",
       " 2.9359798,\n",
       " 5.0830173,\n",
       " 2.9725566,\n",
       " 3.150896,\n",
       " 3.6477404,\n",
       " 3.037945,\n",
       " 3.3771806,\n",
       " 3.3100462,\n",
       " 3.1256597,\n",
       " 3.1248286,\n",
       " 2.9793174,\n",
       " 3.24113,\n",
       " 3.3643703,\n",
       " 2.9031763,\n",
       " 3.0790331,\n",
       " 4.788172,\n",
       " 3.361425,\n",
       " 2.8864682,\n",
       " 2.9639363,\n",
       " 3.363049,\n",
       " 2.8018367,\n",
       " 3.5482128,\n",
       " 3.1667562,\n",
       " 2.8475466,\n",
       " 3.1009557,\n",
       " 3.179566,\n",
       " 3.3088677,\n",
       " 2.897338,\n",
       " 2.9323516,\n",
       " 2.913164,\n",
       " 3.0475793,\n",
       " 3.906136,\n",
       " 3.3704987,\n",
       " 3.0893016,\n",
       " 3.3706555,\n",
       " 2.9301736,\n",
       " 3.2522378,\n",
       " 3.8687563,\n",
       " 3.5473132,\n",
       " 2.7123923,\n",
       " 3.1034093,\n",
       " 3.4662259,\n",
       " 3.1963863,\n",
       " 2.8444753,\n",
       " 3.3888597,\n",
       " 3.0472672,\n",
       " 3.0649948,\n",
       " 3.2365859,\n",
       " 3.6751695,\n",
       " 3.737236,\n",
       " 3.1756704,\n",
       " 3.023559,\n",
       " 3.287919,\n",
       " 2.88394,\n",
       " 2.918354,\n",
       " 2.5054533,\n",
       " 3.2696264,\n",
       " 3.1289284,\n",
       " 2.6394908,\n",
       " 2.9584675,\n",
       " 2.5887687,\n",
       " 2.968474,\n",
       " 3.2955523,\n",
       " 2.7609644,\n",
       " 3.0436625,\n",
       " 2.770774,\n",
       " 2.9619613,\n",
       " 3.3212588,\n",
       " 11.37511,\n",
       " 3.4481864,\n",
       " 3.1905456,\n",
       " 2.9719431,\n",
       " 4.1428633,\n",
       " 3.0726037,\n",
       " 3.0241265,\n",
       " 3.0875058,\n",
       " 3.031909,\n",
       " 3.0357957,\n",
       " 2.7377462,\n",
       " 3.1440544,\n",
       " 2.8155441,\n",
       " 3.2662325,\n",
       " 2.8042264,\n",
       " 3.1956582,\n",
       " 3.1387749,\n",
       " 2.9194262,\n",
       " 2.9130886,\n",
       " 3.5275168,\n",
       " 3.1559186,\n",
       " 4.3701906,\n",
       " 3.1016512,\n",
       " 2.69945,\n",
       " 3.2200294,\n",
       " 3.0544395,\n",
       " 3.126492,\n",
       " 2.8236737,\n",
       " 3.3128757,\n",
       " 2.9453511,\n",
       " 3.051344,\n",
       " 3.9895177,\n",
       " 3.1205676,\n",
       " 3.110092,\n",
       " 2.9075146,\n",
       " 4.476162,\n",
       " 3.3152876,\n",
       " 3.0415826,\n",
       " 2.86726,\n",
       " 3.2956967,\n",
       " 3.3147984,\n",
       " 2.9540591,\n",
       " 3.2667296,\n",
       " 3.3254867,\n",
       " 2.7511961,\n",
       " 3.45994,\n",
       " 3.0505278,\n",
       " 2.7722208,\n",
       " 3.745654,\n",
       " 3.6831517,\n",
       " 2.706562,\n",
       " 2.838416,\n",
       " 2.8611603,\n",
       " 3.094061,\n",
       " 4.307764,\n",
       " 2.635132,\n",
       " 3.035657,\n",
       " 2.9717357,\n",
       " 3.1576293,\n",
       " 6.5102434,\n",
       " 2.970153,\n",
       " 2.9257572,\n",
       " 3.232318,\n",
       " 3.3155057,\n",
       " 2.7515182,\n",
       " 3.3798804,\n",
       " 3.1433458,\n",
       " 2.7264004,\n",
       " 2.710555,\n",
       " 3.2346678,\n",
       " 2.9236116,\n",
       " 3.289561,\n",
       " 3.203527,\n",
       " 3.2756968,\n",
       " 3.0020063,\n",
       " 3.923488,\n",
       " 3.3588772,\n",
       " 2.8496895,\n",
       " 2.9028604,\n",
       " 2.8402963,\n",
       " 3.2180424,\n",
       " 3.0879917,\n",
       " 3.1061528,\n",
       " 3.433827,\n",
       " 2.611432,\n",
       " 3.1896486,\n",
       " 2.791417,\n",
       " 2.7585976,\n",
       " 3.286997,\n",
       " 2.9895115,\n",
       " 3.3131344,\n",
       " 3.2474735,\n",
       " 2.8017607,\n",
       " 3.2417073,\n",
       " 2.921012,\n",
       " 2.9397297,\n",
       " 3.034955,\n",
       " 3.401495,\n",
       " 2.845708,\n",
       " 3.8731284,\n",
       " 2.739049,\n",
       " 3.3926935,\n",
       " 2.7454834,\n",
       " 3.5206184,\n",
       " 3.3658147,\n",
       " 2.891744,\n",
       " 3.4495974,\n",
       " 2.729809,\n",
       " 4.671728,\n",
       " 3.475248,\n",
       " 3.2133975,\n",
       " 3.3170633,\n",
       " 3.1653476,\n",
       " 3.4266663,\n",
       " 3.2764742,\n",
       " 2.6787128,\n",
       " 3.3601775,\n",
       " 3.0271173,\n",
       " 2.9567254,\n",
       " 6.5470715,\n",
       " 3.1140153,\n",
       " 3.0396821,\n",
       " 2.8123431,\n",
       " 3.111337,\n",
       " 2.7289648,\n",
       " 2.8649364,\n",
       " 3.3568113,\n",
       " 3.4073482,\n",
       " 3.082075,\n",
       " 3.183349,\n",
       " 2.9051557,\n",
       " 2.724218,\n",
       " 3.1667013,\n",
       " 3.1417198,\n",
       " 3.296296,\n",
       " 2.843621,\n",
       " 2.9240813,\n",
       " 2.7749658,\n",
       " 2.7951732,\n",
       " 3.325588,\n",
       " 3.0647893,\n",
       " 3.0542068,\n",
       " 3.3946881,\n",
       " 2.9735548,\n",
       " 3.271999,\n",
       " 3.209577,\n",
       " 3.2579737,\n",
       " 2.5395737,\n",
       " 2.6105428,\n",
       " 3.6198182,\n",
       " 3.1099033,\n",
       " 3.7674584,\n",
       " 3.14251,\n",
       " 2.9597113,\n",
       " 3.4246473,\n",
       " 2.8459857,\n",
       " 2.940436,\n",
       " 2.9117708,\n",
       " 2.542285,\n",
       " 3.1041422,\n",
       " 3.1649778,\n",
       " 2.7811675,\n",
       " 3.2238727,\n",
       " 3.1674676,\n",
       " 2.6691253,\n",
       " 3.7005727,\n",
       " 4.957081,\n",
       " 2.470482,\n",
       " 3.772681,\n",
       " 2.9009323,\n",
       " 3.1459591,\n",
       " 2.5300107,\n",
       " 3.0596075,\n",
       " 3.2090263,\n",
       " 2.846994,\n",
       " 3.0684612,\n",
       " 3.0052624,\n",
       " 3.344992,\n",
       " 3.5852127,\n",
       " 3.1712084,\n",
       " 3.2691255,\n",
       " 2.7330577,\n",
       " 2.9971383,\n",
       " 3.0910473,\n",
       " 3.5034432,\n",
       " 3.6599474,\n",
       " 2.6220584,\n",
       " 3.0955014,\n",
       " 2.9241867,\n",
       " 2.9380417,\n",
       " 3.479333,\n",
       " 3.7985356,\n",
       " 3.6495566,\n",
       " 2.6081028,\n",
       " 5.588464,\n",
       " 3.0295334,\n",
       " 3.5092378,\n",
       " 2.6787722,\n",
       " 2.620673,\n",
       " 2.5601752,\n",
       " 3.6683564,\n",
       " 3.1137662,\n",
       " 3.174525,\n",
       " 2.8247607,\n",
       " 2.483352,\n",
       " 3.4553185,\n",
       " 3.0593128,\n",
       " 2.9835057,\n",
       " 3.2151845,\n",
       " 3.288754,\n",
       " 3.0091085,\n",
       " 3.105233,\n",
       " 2.6950393,\n",
       " 2.58198,\n",
       " 2.9190373,\n",
       " 2.8738544,\n",
       " 3.5496726,\n",
       " 3.149901,\n",
       " 2.780716,\n",
       " 3.1804502,\n",
       " 2.8280134,\n",
       " 3.8759522,\n",
       " 4.0352182,\n",
       " 3.4212894,\n",
       " 2.9773414,\n",
       " 3.1457567,\n",
       " 3.1737318,\n",
       " 2.9623313,\n",
       " 2.5596633,\n",
       " 3.310658,\n",
       " 3.3393078,\n",
       " 2.74638,\n",
       " 3.1335747,\n",
       " 2.6032407,\n",
       " 2.750632,\n",
       " 2.8643713,\n",
       " 3.0738463,\n",
       " 3.1225643,\n",
       " 3.3139787,\n",
       " 3.4807777,\n",
       " 2.8332157,\n",
       " 2.928113,\n",
       " 2.8436732,\n",
       " 3.247736,\n",
       " 3.339725,\n",
       " 2.649975,\n",
       " 3.1424289,\n",
       " 2.9289434,\n",
       " 3.0169487,\n",
       " 3.1915922,\n",
       " 2.647246,\n",
       " 3.1829877,\n",
       " 3.3313768,\n",
       " 2.5731494,\n",
       " 2.9446664,\n",
       " 2.5146022,\n",
       " 5.535692,\n",
       " 2.7301757,\n",
       " 3.405767,\n",
       " 2.4436483,\n",
       " 3.3406458,\n",
       " 4.8973083,\n",
       " 2.7154288,\n",
       " 2.7639391,\n",
       " 2.798319,\n",
       " 3.1847885,\n",
       " 3.0406933,\n",
       " 2.8315675,\n",
       " 3.3565528,\n",
       " 2.7551975,\n",
       " 2.83781,\n",
       " 2.8816857,\n",
       " 2.7939,\n",
       " 2.8881626,\n",
       " 3.4663343,\n",
       " 2.6562736,\n",
       " 2.7161238,\n",
       " 2.8687325,\n",
       " 2.956778,\n",
       " 3.4349995,\n",
       " 3.1613708,\n",
       " 2.8428822,\n",
       " 3.1786327,\n",
       " 2.961257,\n",
       " 2.6458654,\n",
       " 2.7255144,\n",
       " 2.8239195,\n",
       " 3.0988307,\n",
       " 2.9726374,\n",
       " 3.1494882,\n",
       " 3.3338397,\n",
       " 5.649597,\n",
       " 4.2430954,\n",
       " 3.035904,\n",
       " 2.6141253,\n",
       " 3.6087995,\n",
       " 2.6800356,\n",
       " 2.5960667,\n",
       " 2.905808,\n",
       " 2.9242504,\n",
       " 2.8108568,\n",
       " 3.0057971,\n",
       " 3.280508,\n",
       " 2.8576117,\n",
       " 2.7620735,\n",
       " 3.4858608,\n",
       " 2.6435084,\n",
       " 3.3719163,\n",
       " 3.057787,\n",
       " 3.8325524,\n",
       " 3.1800847,\n",
       " 2.8516276,\n",
       " 4.214961,\n",
       " 2.6816008,\n",
       " 3.3506076,\n",
       " 5.508588,\n",
       " 2.9183178,\n",
       " 3.3039591,\n",
       " 5.5660486,\n",
       " 3.01737,\n",
       " 3.1998625,\n",
       " 2.893814,\n",
       " 2.6498199,\n",
       " 2.6643777,\n",
       " 3.592349,\n",
       " 3.553256,\n",
       " 2.7840161,\n",
       " 2.9124298,\n",
       " 2.6361256,\n",
       " 2.4478164,\n",
       " 2.9078412,\n",
       " 2.6582038,\n",
       " 3.1274147,\n",
       " 2.7359934,\n",
       " 2.9001966,\n",
       " 3.327405,\n",
       " 2.3939357,\n",
       " 2.7978096,\n",
       " 2.8045454,\n",
       " 3.0828114,\n",
       " 4.015387,\n",
       " 3.4616132,\n",
       " 2.8642461,\n",
       " 2.6423748,\n",
       " 3.0815217,\n",
       " 2.7527337,\n",
       " 2.7287896,\n",
       " 2.528505,\n",
       " 2.7587492,\n",
       " 2.7213101,\n",
       " 3.2003138,\n",
       " 2.9442878,\n",
       " 2.6355114,\n",
       " 6.040861,\n",
       " 2.8135087,\n",
       " 3.6780272,\n",
       " 4.2474146,\n",
       " 3.3543606,\n",
       " 2.9261203,\n",
       " 2.807555,\n",
       " 2.8445742,\n",
       " 2.6688771,\n",
       " 2.557487,\n",
       " 2.7650175,\n",
       " 2.375485,\n",
       " 2.7351093,\n",
       " 2.7195768,\n",
       " 2.92802,\n",
       " 2.9033546,\n",
       " 5.5051966,\n",
       " 2.978045,\n",
       " 3.0689433,\n",
       " 2.5319626,\n",
       " 4.2766333,\n",
       " 2.984273,\n",
       " 3.6716375,\n",
       " 2.6276197,\n",
       " 2.8662145,\n",
       " 3.4111965,\n",
       " 2.7604692,\n",
       " 2.6903596,\n",
       " 2.6249888,\n",
       " 2.9589975,\n",
       " 2.7611651,\n",
       " 3.2539396,\n",
       " 2.779735,\n",
       " 2.5540032,\n",
       " 2.8956165,\n",
       " 2.6762853,\n",
       " 2.8387375,\n",
       " 2.4684,\n",
       " 3.4572897,\n",
       " 2.735492,\n",
       " 2.940274,\n",
       " 3.0302129,\n",
       " 3.1994438,\n",
       " 2.5955338,\n",
       " 2.6643684,\n",
       " 3.1647487,\n",
       " 3.231408,\n",
       " 2.6398802,\n",
       " 2.945136,\n",
       " 2.824453,\n",
       " 2.7519226,\n",
       " 4.712661,\n",
       " 2.3970618,\n",
       " 2.6897078,\n",
       " 3.2364316,\n",
       " 4.683653,\n",
       " 2.5237787,\n",
       " 2.6544662,\n",
       " 3.3220048,\n",
       " 2.5149775,\n",
       " 2.6574051,\n",
       " 2.6729457,\n",
       " 2.7235477,\n",
       " 2.7759156,\n",
       " 2.8297703,\n",
       " 2.8387108,\n",
       " 3.030853,\n",
       " 3.0402775,\n",
       " 3.450601,\n",
       " 2.5532806,\n",
       " 2.956752,\n",
       " 3.3700795,\n",
       " 2.7975802,\n",
       " 2.953069,\n",
       " 3.0852008,\n",
       " 3.022937,\n",
       " 2.769922,\n",
       " 2.532288,\n",
       " 3.1076508,\n",
       " 2.504352,\n",
       " 3.4208887,\n",
       " 2.4641829,\n",
       " 3.106474,\n",
       " 2.370641,\n",
       " 2.9101098,\n",
       " 2.9043026,\n",
       " 2.774751,\n",
       " 3.031589,\n",
       " 3.4578302,\n",
       " 2.9620292,\n",
       " 2.6440895,\n",
       " 2.9498267,\n",
       " 2.9356692,\n",
       " 3.0100582,\n",
       " 2.9635801,\n",
       " 2.312628,\n",
       " 2.3739989,\n",
       " 2.630936,\n",
       " 2.3118577,\n",
       " 3.848646,\n",
       " 2.841938,\n",
       " 3.1357555,\n",
       " 2.8442283,\n",
       " 2.5369053,\n",
       " 2.7494166,\n",
       " 3.0319436,\n",
       " 2.4661913,\n",
       " 2.9135458,\n",
       " 3.7592258,\n",
       " 2.3072274,\n",
       " 2.5557375,\n",
       " 3.0909333,\n",
       " 2.620366,\n",
       " 2.9683099,\n",
       " 4.5039825,\n",
       " 2.9525084,\n",
       " 2.6298664,\n",
       " 2.8341956,\n",
       " 2.8172045,\n",
       " 2.9629393,\n",
       " 2.6597464,\n",
       " 2.7057955,\n",
       " 3.509941,\n",
       " 2.68025,\n",
       " 2.886496,\n",
       " 2.9706776,\n",
       " 3.0291808,\n",
       " 3.5718832,\n",
       " 2.996343,\n",
       " 2.8467999,\n",
       " 2.9897144,\n",
       " 3.1691608,\n",
       " 4.771923,\n",
       " 3.2767735,\n",
       " 2.7317042,\n",
       " 2.5426269,\n",
       " 3.123794,\n",
       " 3.9450448,\n",
       " 2.6870785,\n",
       " 3.3548002,\n",
       " 2.62824,\n",
       " 2.7754009,\n",
       " 3.3431334,\n",
       " 2.65653,\n",
       " 2.6382365,\n",
       " 3.1607435,\n",
       " 2.5479658,\n",
       " 3.520296,\n",
       " 2.8149283,\n",
       " 2.5009613,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7aeb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'data/ACM.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c5ed40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverageDegree(data):\n",
    "    data_mat = sio.loadmat(data)\n",
    "    adj = data_mat['Network']\n",
    "    truth = data_mat['Label']\n",
    "    _, [normal, anomalous] = np.unique(truth, return_counts=True)\n",
    "    total_nodes = normal + anomalous\n",
    "    print(\"total nodes: \", total_nodes)\n",
    "    print(\"normal nodes: \", normal)\n",
    "    print(\"anomalous nodes: \", anomalous)\n",
    "    # Calculate the total degree by summing the degrees for each node\n",
    "    # For an undirected graph represented in CSC, each non-zero entry in the data array represents an edge\n",
    "    total_degree = adj.data.size\n",
    "\n",
    "    # Calculate the average degree of the nodes\n",
    "    # In an undirected graph, each edge is counted twice (once for each node it connects), so divide by 2\n",
    "    average_degree = total_degree / total_nodes\n",
    "    return average_degree\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "A= [1, 2, 3, 4, 5]\n",
    "label = [1, 0, 0, 0, 1]\n",
    "score = [3, 1, 2, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9200e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5020b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8452d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = data_mat['Network']\n",
    "feat = data_mat['Attributes']\n",
    "truth = data_mat['Label']\n",
    "truth = truth.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c74a7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16484\n"
     ]
    }
   ],
   "source": [
    "_, [normal, anomalous] = np.unique(truth, return_counts=True)\n",
    "print(normal + anomalous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de6a0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.970274205289979"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ad38e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16484"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15887 + 597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8b0ffdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5196x5196 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 350577 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_norm = normalize_adj(adj + sp.eye(adj.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68a84290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0012987 , 0.00156096, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00156096, 0.00187617, 0.00195477, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.00195477, 0.00203666, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.0625    , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.05263158,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.05263158]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_norm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aee509d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_norm = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "adj_norm = adj_norm.toarray()\n",
    "adj = adj + sp.eye(adj.shape[0])\n",
    "adj = adj.toarray()\n",
    "feat = feat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8cb3d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0012987 , 0.00156096, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00156096, 0.00187617, 0.00195477, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.00195477, 0.00203666, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.0625    , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.05263158,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.05263158]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5045946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_norm[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf48adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b570fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5        0.66666667 1.         1.         1.        ]\n",
      "[1.  1.  1.  0.5 0. ]\n",
      "[0.1  0.34 0.4  0.8 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.34, 0.4, 0.8])\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_true, y_scores)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c782ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [0, 0, 0, 1, 1]\n",
    "prob = [3/5, 3/5, 3/5, 2/5, 2/5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ce2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
